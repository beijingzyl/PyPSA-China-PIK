{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the PyPSA-China (PIK version) documentation This is the documentation for the China Python Power System Analysis (PyPSA-China) Model maintained by the Energy Transition Lab at PIK.","title":"Home"},{"location":"#welcome-to-the-pypsa-china-pik-version-documentation","text":"This is the documentation for the China Python Power System Analysis (PyPSA-China) Model maintained by the Energy Transition Lab at PIK.","title":"Welcome to the PyPSA-China (PIK version) documentation"},{"location":"reference/","text":"Reference guide This is the code documentation extracted from docstrings.","title":"Reference"},{"location":"reference/#reference-guide","text":"This is the code documentation extracted from docstrings.","title":"Reference guide"},{"location":"tutorials/","text":"==== UNDER CONSTRUCTION ==== There are currently no tutorials for this project","title":"Tutorials"},{"location":"docs/reference/add_brownfield/","text":"Functions for myopic pathway network generation snakemake rules Add paid-off assets from previous planning horizon to network from next planning horizon add_brownfield(n, n_p, year) Add paid for assets as p_nom to the current network Parameters: n ( Network ) \u2013 next network to prep & optimize in the planning horizon n_p ( Network ) \u2013 previous optimized network year ( int ) \u2013 the planning year Source code in workflow/scripts/add_brownfield.py def add_brownfield(n: pypsa.Network, n_p: pypsa.Network, year: int): \"\"\"Add paid for assets as p_nom to the current network Args: n (pypsa.Network): next network to prep & optimize in the planning horizon n_p (pypsa.Network): previous optimized network year (int): the planning year \"\"\" logger.info(\"Adding brownfield\") # electric transmission grid set optimised capacities of previous as minimum n.lines.s_nom_min = n_p.lines.s_nom_opt # dc_i = n.links[n.links.carrier==\"DC\"].index # n.links.loc[dc_i, \"p_nom_min\"] = n_p.links.loc[dc_i, \"p_nom_opt\"] # update links n.links.loc[(n.links.length > 0) & (n.links.lifetime == np.inf), \"p_nom\"] = n_p.links.loc[ (n_p.links.carrier == \"AC\") & (n_p.links.build_year == 0), \"p_nom_opt\" ] n.links.loc[(n.links.length > 0) & (n.links.lifetime == np.inf), \"p_nom_min\"] = n_p.links.loc[ (n_p.links.carrier == \"AC\") & (n_p.links.build_year == 0), \"p_nom_opt\" ] if year == 2025: add_build_year_to_new_assets(n_p, 2020) for c in n_p.iterate_components([\"Link\", \"Generator\", \"Store\"]): attr = \"e\" if c.name == \"Store\" else \"p\" # first, remove generators, links and stores that track # CO2 or global EU values since these are already in n n_p.mremove(c.name, c.df.index[c.df.lifetime == np.inf]) # remove assets whose build_year + lifetime < year n_p.mremove(c.name, c.df.index[c.df.build_year + c.df.lifetime < year]) # remove assets if their optimized nominal capacity is lower than a threshold # since CHP heat Link is proportional to CHP electric Link, ensure threshold is compatible chp_heat = c.df.index[(c.df[attr + \"_nom_extendable\"] & c.df.index.str.contains(\"CHP\"))] threshold = snakemake.config[\"existing_capacities\"][\"threshold_capacity\"] if not chp_heat.empty: threshold_chp_heat = ( threshold * c.df.loc[chp_heat].efficiency2 / c.df.loc[chp_heat].efficiency ) n_p.mremove( c.name, chp_heat[c.df.loc[chp_heat, attr + \"_nom_opt\"] < threshold_chp_heat] ) n_p.mremove( c.name, c.df.index[ c.df[attr + \"_nom_extendable\"] & ~c.df.index.isin(chp_heat) & (c.df[attr + \"_nom_opt\"] < threshold) ], ) # copy over assets but fix their capacity c.df[attr + \"_nom\"] = c.df[attr + \"_nom_opt\"] c.df[attr + \"_nom_extendable\"] = False c.df[attr + \"_nom_max\"] = np.inf n.import_components_from_dataframe(c.df, c.name) # copy time-dependent selection = n.component_attrs[c.name].type.str.contains(\"series\") & n.component_attrs[ c.name ].status.str.contains(\"Input\") for tattr in n.component_attrs[c.name].index[selection]: n.import_series_from_dataframe(c.pnl[tattr].set_index(n.snapshots), c.name, tattr) for tech in [\"onwind\", \"offwind\", \"solar\"]: ds_tech = xr.open_dataset(snakemake.input[\"profile_\" + tech]) p_nom_max_initial = ds_tech[\"p_nom_max\"].to_pandas() if tech == \"offwind\": for node in OFFSHORE_WIND_NODES: n.generators.loc[ (n.generators.bus == node) & (n.generators.carrier == tech) & (n.generators.build_year == year), \"p_nom_max\", ] = ( p_nom_max_initial[node] - n_p.generators[ (n_p.generators.bus == node) & (n_p.generators.carrier == tech) ].p_nom_opt.sum() ) else: for node in PROV_NAMES: n.generators.loc[ (n.generators.bus == node) & (n.generators.carrier == tech) & (n.generators.build_year == year), \"p_nom_max\", ] = ( p_nom_max_initial[node] - n_p.generators[ (n_p.generators.bus == node) & (n_p.generators.carrier == tech) ].p_nom_opt.sum() ) n.generators.loc[(n.generators.p_nom_max < 0), \"p_nom_max\"] = 0 # retrofit coal power plant with carbon capture n.generators.loc[n.generators.carrier == \"coal power plant\", \"p_nom_extendable\"] = True n.generators.loc[ n.generators.index.str.contains(\"retrofit\") & ~n.generators.index.str.contains(str(year)), \"p_nom_extendable\", ] = False","title":"add_brownfield"},{"location":"docs/reference/add_brownfield/#add_brownfield.add_brownfield","text":"Add paid for assets as p_nom to the current network Parameters: n ( Network ) \u2013 next network to prep & optimize in the planning horizon n_p ( Network ) \u2013 previous optimized network year ( int ) \u2013 the planning year Source code in workflow/scripts/add_brownfield.py def add_brownfield(n: pypsa.Network, n_p: pypsa.Network, year: int): \"\"\"Add paid for assets as p_nom to the current network Args: n (pypsa.Network): next network to prep & optimize in the planning horizon n_p (pypsa.Network): previous optimized network year (int): the planning year \"\"\" logger.info(\"Adding brownfield\") # electric transmission grid set optimised capacities of previous as minimum n.lines.s_nom_min = n_p.lines.s_nom_opt # dc_i = n.links[n.links.carrier==\"DC\"].index # n.links.loc[dc_i, \"p_nom_min\"] = n_p.links.loc[dc_i, \"p_nom_opt\"] # update links n.links.loc[(n.links.length > 0) & (n.links.lifetime == np.inf), \"p_nom\"] = n_p.links.loc[ (n_p.links.carrier == \"AC\") & (n_p.links.build_year == 0), \"p_nom_opt\" ] n.links.loc[(n.links.length > 0) & (n.links.lifetime == np.inf), \"p_nom_min\"] = n_p.links.loc[ (n_p.links.carrier == \"AC\") & (n_p.links.build_year == 0), \"p_nom_opt\" ] if year == 2025: add_build_year_to_new_assets(n_p, 2020) for c in n_p.iterate_components([\"Link\", \"Generator\", \"Store\"]): attr = \"e\" if c.name == \"Store\" else \"p\" # first, remove generators, links and stores that track # CO2 or global EU values since these are already in n n_p.mremove(c.name, c.df.index[c.df.lifetime == np.inf]) # remove assets whose build_year + lifetime < year n_p.mremove(c.name, c.df.index[c.df.build_year + c.df.lifetime < year]) # remove assets if their optimized nominal capacity is lower than a threshold # since CHP heat Link is proportional to CHP electric Link, ensure threshold is compatible chp_heat = c.df.index[(c.df[attr + \"_nom_extendable\"] & c.df.index.str.contains(\"CHP\"))] threshold = snakemake.config[\"existing_capacities\"][\"threshold_capacity\"] if not chp_heat.empty: threshold_chp_heat = ( threshold * c.df.loc[chp_heat].efficiency2 / c.df.loc[chp_heat].efficiency ) n_p.mremove( c.name, chp_heat[c.df.loc[chp_heat, attr + \"_nom_opt\"] < threshold_chp_heat] ) n_p.mremove( c.name, c.df.index[ c.df[attr + \"_nom_extendable\"] & ~c.df.index.isin(chp_heat) & (c.df[attr + \"_nom_opt\"] < threshold) ], ) # copy over assets but fix their capacity c.df[attr + \"_nom\"] = c.df[attr + \"_nom_opt\"] c.df[attr + \"_nom_extendable\"] = False c.df[attr + \"_nom_max\"] = np.inf n.import_components_from_dataframe(c.df, c.name) # copy time-dependent selection = n.component_attrs[c.name].type.str.contains(\"series\") & n.component_attrs[ c.name ].status.str.contains(\"Input\") for tattr in n.component_attrs[c.name].index[selection]: n.import_series_from_dataframe(c.pnl[tattr].set_index(n.snapshots), c.name, tattr) for tech in [\"onwind\", \"offwind\", \"solar\"]: ds_tech = xr.open_dataset(snakemake.input[\"profile_\" + tech]) p_nom_max_initial = ds_tech[\"p_nom_max\"].to_pandas() if tech == \"offwind\": for node in OFFSHORE_WIND_NODES: n.generators.loc[ (n.generators.bus == node) & (n.generators.carrier == tech) & (n.generators.build_year == year), \"p_nom_max\", ] = ( p_nom_max_initial[node] - n_p.generators[ (n_p.generators.bus == node) & (n_p.generators.carrier == tech) ].p_nom_opt.sum() ) else: for node in PROV_NAMES: n.generators.loc[ (n.generators.bus == node) & (n.generators.carrier == tech) & (n.generators.build_year == year), \"p_nom_max\", ] = ( p_nom_max_initial[node] - n_p.generators[ (n_p.generators.bus == node) & (n_p.generators.carrier == tech) ].p_nom_opt.sum() ) n.generators.loc[(n.generators.p_nom_max < 0), \"p_nom_max\"] = 0 # retrofit coal power plant with carbon capture n.generators.loc[n.generators.carrier == \"coal power plant\", \"p_nom_extendable\"] = True n.generators.loc[ n.generators.index.str.contains(\"retrofit\") & ~n.generators.index.str.contains(str(year)), \"p_nom_extendable\", ] = False","title":"add_brownfield"},{"location":"docs/reference/add_electricity/","text":"Misc collection of functions supporting network prep still to be cleaned up add_missing_carriers(n, carriers) Function to add missing carriers to the network without raising errors. Parameters: n ( Network ) \u2013 the pypsa network object carriers ( list | set ) \u2013 a list of carriers that should be included Source code in workflow/scripts/add_electricity.py def add_missing_carriers(n: pypsa.Network, carriers: list | set) -> None: \"\"\"Function to add missing carriers to the network without raising errors. Args: n (pypsa.Network): the pypsa network object carriers (list | set): a list of carriers that should be included \"\"\" missing_carriers = set(carriers) - set(n.carriers.index) if len(missing_carriers) > 0: n.add(\"Carrier\", missing_carriers) calculate_annuity(lifetime, discount_rate) Calculate the annuity factor for an asset with lifetime n years and discount rate of r, e.g. annuity(20, 0.05) * 20 = 1.6 Parameters: lifetime ( int ) \u2013 ecomic asset lifetime for discounting/NPV calc discount_rate ( float ) \u2013 the WACC Returns: float ( float ) \u2013 the annuity factor Source code in workflow/scripts/add_electricity.py def calculate_annuity(lifetime: int, discount_rate: float) -> float: \"\"\"Calculate the annuity factor for an asset with lifetime n years and discount rate of r, e.g. annuity(20, 0.05) * 20 = 1.6 Args: lifetime (int): ecomic asset lifetime for discounting/NPV calc discount_rate (float): the WACC Returns: float: the annuity factor \"\"\" r = discount_rate n = lifetime if isinstance(r, pd.Series): if r.any() < 0: raise ValueError(\"Discount rate must be positive\") if r.any() < 0: raise ValueError(\"Discount rate must be positive\") return pd.Series(1 / n, index=r.index).where(r == 0, r / (1.0 - 1.0 / (1.0 + r) ** n)) elif r < 0: raise ValueError(\"Discount rate must be positive\") elif r < 0: raise ValueError(\"Discount rate must be positive\") elif r > 0: return r / (1.0 - 1.0 / (1.0 + r) ** n) else: return 1 / n load_costs(tech_costs, cost_config, elec_config, cost_year, n_years) Calculate the anualised capex costs and OM costs for the technologies based on the input data Parameters: tech_costs ( PathLike ) \u2013 the csv containing the costs cost_config ( dict ) \u2013 the snakemake pypsa-china cost config elec_config ( dict ) \u2013 the snakemake pypsa-china electricity config cost_year ( int ) \u2013 the year for which the costs are retrived n_years ( int ) \u2013 the # of years over which the investment is annuitised Returns: DataFrame \u2013 pd.DataFrame: costs dataframe in [CURRENCY] per MW_ ... or per MWh_ ... Source code in workflow/scripts/add_electricity.py def load_costs( tech_costs: PathLike, cost_config: dict, elec_config: dict, cost_year: int, n_years: int ) -> pd.DataFrame: \"\"\"Calculate the anualised capex costs and OM costs for the technologies based on the input data Args: tech_costs (PathLike): the csv containing the costs cost_config (dict): the snakemake pypsa-china cost config elec_config (dict): the snakemake pypsa-china electricity config cost_year (int): the year for which the costs are retrived n_years (int): the # of years over which the investment is annuitised Returns: pd.DataFrame: costs dataframe in [CURRENCY] per MW_ ... or per MWh_ ... \"\"\" # set all asset costs and other parameters costs = pd.read_csv(tech_costs, index_col=list(range(3))).sort_index() # correct units to MW and EUR costs.loc[costs.unit.str.contains(\"/kW\"), \"value\"] *= 1e3 costs.loc[costs.unit.str.contains(\"USD\"), \"value\"] *= cost_config[\"USD2013_to_EUR2013\"] costs.loc[costs.unit.str.contains(\"USD\"), \"value\"] *= cost_config[\"USD2013_to_EUR2013\"] cost_year = float(cost_year) costs = ( costs.loc[idx[:, cost_year, :], \"value\"] .unstack(level=2) .groupby(\"technology\") .sum(min_count=1) ) # TODO set default lifetime as option costs = costs.fillna( { \"CO2 intensity\": 0, \"FOM\": 0, \"VOM\": 0, \"discount rate\": cost_config[\"discountrate\"], \"discount rate\": cost_config[\"discountrate\"], \"efficiency\": 1, \"fuel\": 0, \"investment\": 0, \"lifetime\": 25, } ) costs[\"capital_cost\"] = ( (calculate_annuity(costs[\"lifetime\"], costs[\"discount rate\"]) + costs[\"FOM\"] / 100.0) * costs[\"investment\"] * n_years ) costs.at[\"OCGT\", \"fuel\"] = costs.at[\"gas\", \"fuel\"] costs.at[\"CCGT\", \"fuel\"] = costs.at[\"gas\", \"fuel\"] costs[\"marginal_cost\"] = costs[\"VOM\"] + costs[\"fuel\"] / costs[\"efficiency\"] costs = costs.rename(columns={\"CO2 intensity\": \"co2_emissions\"}) costs.at[\"OCGT\", \"co2_emissions\"] = costs.at[\"gas\", \"co2_emissions\"] costs.at[\"CCGT\", \"co2_emissions\"] = costs.at[\"gas\", \"co2_emissions\"] if not 0 <= cost_config[\"pv_utility_fraction\"] <= 1: raise ValueError(\"pv_utility_fraction must be between 0 and 1 in cost config\") f_util = cost_config[\"pv_utility_fraction\"] costs.at[\"solar\", \"capital_cost\"] = ( f_util * costs.at[\"solar-utility\", \"capital_cost\"] + (1 - f_util) * costs.at[\"solar-rooftop\", \"capital_cost\"] ) def costs_for_storage(store, link1, link2=None, max_hours=1.0): capital_cost = link1[\"capital_cost\"] + max_hours * store[\"capital_cost\"] if link2 is not None: capital_cost += link2[\"capital_cost\"] return pd.Series(dict(capital_cost=capital_cost, marginal_cost=0.0, co2_emissions=0.0)) max_hours = elec_config[\"max_hours\"] costs.loc[\"battery\"] = costs_for_storage( costs.loc[\"battery storage\"], costs.loc[\"battery inverter\"], max_hours=max_hours[\"battery\"] ) costs.loc[\"H2\"] = costs_for_storage( costs.loc[\"hydrogen storage tank type 1\"], costs.loc[\"fuel cell\"], costs.loc[\"electrolysis\"], max_hours=max_hours[\"H2\"], ) for attr in (\"marginal_cost\", \"capital_cost\"): overwrites = cost_config.get(attr) overwrites = cost_config.get(attr) if overwrites is not None: overwrites = pd.Series(overwrites) costs.loc[overwrites.index, attr] = overwrites return costs sanitize_carriers(n, config) Sanitize the carrier information in a PyPSA Network object. The function ensures that all unique carrier names are present in the network's carriers attribute, and adds nice names and colors for each carrier according to the provided configuration dictionary. Parameters: n ( Network ) \u2013 PyPSA Network object representing the electrical power system. config ( dict ) \u2013 A dictionary containing configuration information, specifically the \"plotting\" key with \"nice_names\" and \"tech_colors\" keys for carriers. Source code in workflow/scripts/add_electricity.py def sanitize_carriers(n: pypsa.Network, config: dict) -> None: \"\"\"Sanitize the carrier information in a PyPSA Network object. The function ensures that all unique carrier names are present in the network's carriers attribute, and adds nice names and colors for each carrier according to the provided configuration dictionary. Args: n (pypsa.Network): PyPSA Network object representing the electrical power system. config (dict): A dictionary containing configuration information, specifically the \"plotting\" key with \"nice_names\" and \"tech_colors\" keys for carriers. \"\"\" # update default nice names w user settings nice_names = NICE_NAMES_DEFAULT.update(config[\"plotting\"].get(\"nice_names\", {})) for c in n.iterate_components(): if \"carrier\" in c.df: add_missing_carriers(n, c.df.carrier) # sort the nice names to match carriers and fill missing with \"ugly\" names carrier_i = n.carriers.index nice_names = pd.Series(nice_names).reindex(carrier_i).fillna(carrier_i.to_series()) # replace empty nice names with nice names n.carriers.nice_name.where(n.carriers.nice_name != \"\", nice_names, inplace=True) # TODO make less messy, avoid using map tech_colors = config[\"plotting\"][\"tech_colors\"] colors = pd.Series(tech_colors).reindex(carrier_i) # try to fill missing colors with tech_colors after renaming missing_colors_i = colors[colors.isna()].index colors[missing_colors_i] = missing_colors_i.map(lambda x: rename_techs(x, nice_names)).map( tech_colors ) if colors.isna().any(): missing_i = list(colors.index[colors.isna()]) logger.warning(f\"tech_colors for carriers {missing_i} not defined in config.\") n.carriers[\"color\"] = n.carriers.color.where(n.carriers.color != \"\", colors)","title":"add_electricity"},{"location":"docs/reference/add_electricity/#add_electricity.add_missing_carriers","text":"Function to add missing carriers to the network without raising errors. Parameters: n ( Network ) \u2013 the pypsa network object carriers ( list | set ) \u2013 a list of carriers that should be included Source code in workflow/scripts/add_electricity.py def add_missing_carriers(n: pypsa.Network, carriers: list | set) -> None: \"\"\"Function to add missing carriers to the network without raising errors. Args: n (pypsa.Network): the pypsa network object carriers (list | set): a list of carriers that should be included \"\"\" missing_carriers = set(carriers) - set(n.carriers.index) if len(missing_carriers) > 0: n.add(\"Carrier\", missing_carriers)","title":"add_missing_carriers"},{"location":"docs/reference/add_electricity/#add_electricity.calculate_annuity","text":"Calculate the annuity factor for an asset with lifetime n years and discount rate of r, e.g. annuity(20, 0.05) * 20 = 1.6 Parameters: lifetime ( int ) \u2013 ecomic asset lifetime for discounting/NPV calc discount_rate ( float ) \u2013 the WACC Returns: float ( float ) \u2013 the annuity factor Source code in workflow/scripts/add_electricity.py def calculate_annuity(lifetime: int, discount_rate: float) -> float: \"\"\"Calculate the annuity factor for an asset with lifetime n years and discount rate of r, e.g. annuity(20, 0.05) * 20 = 1.6 Args: lifetime (int): ecomic asset lifetime for discounting/NPV calc discount_rate (float): the WACC Returns: float: the annuity factor \"\"\" r = discount_rate n = lifetime if isinstance(r, pd.Series): if r.any() < 0: raise ValueError(\"Discount rate must be positive\") if r.any() < 0: raise ValueError(\"Discount rate must be positive\") return pd.Series(1 / n, index=r.index).where(r == 0, r / (1.0 - 1.0 / (1.0 + r) ** n)) elif r < 0: raise ValueError(\"Discount rate must be positive\") elif r < 0: raise ValueError(\"Discount rate must be positive\") elif r > 0: return r / (1.0 - 1.0 / (1.0 + r) ** n) else: return 1 / n","title":"calculate_annuity"},{"location":"docs/reference/add_electricity/#add_electricity.load_costs","text":"Calculate the anualised capex costs and OM costs for the technologies based on the input data Parameters: tech_costs ( PathLike ) \u2013 the csv containing the costs cost_config ( dict ) \u2013 the snakemake pypsa-china cost config elec_config ( dict ) \u2013 the snakemake pypsa-china electricity config cost_year ( int ) \u2013 the year for which the costs are retrived n_years ( int ) \u2013 the # of years over which the investment is annuitised Returns: DataFrame \u2013 pd.DataFrame: costs dataframe in [CURRENCY] per MW_ ... or per MWh_ ... Source code in workflow/scripts/add_electricity.py def load_costs( tech_costs: PathLike, cost_config: dict, elec_config: dict, cost_year: int, n_years: int ) -> pd.DataFrame: \"\"\"Calculate the anualised capex costs and OM costs for the technologies based on the input data Args: tech_costs (PathLike): the csv containing the costs cost_config (dict): the snakemake pypsa-china cost config elec_config (dict): the snakemake pypsa-china electricity config cost_year (int): the year for which the costs are retrived n_years (int): the # of years over which the investment is annuitised Returns: pd.DataFrame: costs dataframe in [CURRENCY] per MW_ ... or per MWh_ ... \"\"\" # set all asset costs and other parameters costs = pd.read_csv(tech_costs, index_col=list(range(3))).sort_index() # correct units to MW and EUR costs.loc[costs.unit.str.contains(\"/kW\"), \"value\"] *= 1e3 costs.loc[costs.unit.str.contains(\"USD\"), \"value\"] *= cost_config[\"USD2013_to_EUR2013\"] costs.loc[costs.unit.str.contains(\"USD\"), \"value\"] *= cost_config[\"USD2013_to_EUR2013\"] cost_year = float(cost_year) costs = ( costs.loc[idx[:, cost_year, :], \"value\"] .unstack(level=2) .groupby(\"technology\") .sum(min_count=1) ) # TODO set default lifetime as option costs = costs.fillna( { \"CO2 intensity\": 0, \"FOM\": 0, \"VOM\": 0, \"discount rate\": cost_config[\"discountrate\"], \"discount rate\": cost_config[\"discountrate\"], \"efficiency\": 1, \"fuel\": 0, \"investment\": 0, \"lifetime\": 25, } ) costs[\"capital_cost\"] = ( (calculate_annuity(costs[\"lifetime\"], costs[\"discount rate\"]) + costs[\"FOM\"] / 100.0) * costs[\"investment\"] * n_years ) costs.at[\"OCGT\", \"fuel\"] = costs.at[\"gas\", \"fuel\"] costs.at[\"CCGT\", \"fuel\"] = costs.at[\"gas\", \"fuel\"] costs[\"marginal_cost\"] = costs[\"VOM\"] + costs[\"fuel\"] / costs[\"efficiency\"] costs = costs.rename(columns={\"CO2 intensity\": \"co2_emissions\"}) costs.at[\"OCGT\", \"co2_emissions\"] = costs.at[\"gas\", \"co2_emissions\"] costs.at[\"CCGT\", \"co2_emissions\"] = costs.at[\"gas\", \"co2_emissions\"] if not 0 <= cost_config[\"pv_utility_fraction\"] <= 1: raise ValueError(\"pv_utility_fraction must be between 0 and 1 in cost config\") f_util = cost_config[\"pv_utility_fraction\"] costs.at[\"solar\", \"capital_cost\"] = ( f_util * costs.at[\"solar-utility\", \"capital_cost\"] + (1 - f_util) * costs.at[\"solar-rooftop\", \"capital_cost\"] ) def costs_for_storage(store, link1, link2=None, max_hours=1.0): capital_cost = link1[\"capital_cost\"] + max_hours * store[\"capital_cost\"] if link2 is not None: capital_cost += link2[\"capital_cost\"] return pd.Series(dict(capital_cost=capital_cost, marginal_cost=0.0, co2_emissions=0.0)) max_hours = elec_config[\"max_hours\"] costs.loc[\"battery\"] = costs_for_storage( costs.loc[\"battery storage\"], costs.loc[\"battery inverter\"], max_hours=max_hours[\"battery\"] ) costs.loc[\"H2\"] = costs_for_storage( costs.loc[\"hydrogen storage tank type 1\"], costs.loc[\"fuel cell\"], costs.loc[\"electrolysis\"], max_hours=max_hours[\"H2\"], ) for attr in (\"marginal_cost\", \"capital_cost\"): overwrites = cost_config.get(attr) overwrites = cost_config.get(attr) if overwrites is not None: overwrites = pd.Series(overwrites) costs.loc[overwrites.index, attr] = overwrites return costs","title":"load_costs"},{"location":"docs/reference/add_electricity/#add_electricity.sanitize_carriers","text":"Sanitize the carrier information in a PyPSA Network object. The function ensures that all unique carrier names are present in the network's carriers attribute, and adds nice names and colors for each carrier according to the provided configuration dictionary. Parameters: n ( Network ) \u2013 PyPSA Network object representing the electrical power system. config ( dict ) \u2013 A dictionary containing configuration information, specifically the \"plotting\" key with \"nice_names\" and \"tech_colors\" keys for carriers. Source code in workflow/scripts/add_electricity.py def sanitize_carriers(n: pypsa.Network, config: dict) -> None: \"\"\"Sanitize the carrier information in a PyPSA Network object. The function ensures that all unique carrier names are present in the network's carriers attribute, and adds nice names and colors for each carrier according to the provided configuration dictionary. Args: n (pypsa.Network): PyPSA Network object representing the electrical power system. config (dict): A dictionary containing configuration information, specifically the \"plotting\" key with \"nice_names\" and \"tech_colors\" keys for carriers. \"\"\" # update default nice names w user settings nice_names = NICE_NAMES_DEFAULT.update(config[\"plotting\"].get(\"nice_names\", {})) for c in n.iterate_components(): if \"carrier\" in c.df: add_missing_carriers(n, c.df.carrier) # sort the nice names to match carriers and fill missing with \"ugly\" names carrier_i = n.carriers.index nice_names = pd.Series(nice_names).reindex(carrier_i).fillna(carrier_i.to_series()) # replace empty nice names with nice names n.carriers.nice_name.where(n.carriers.nice_name != \"\", nice_names, inplace=True) # TODO make less messy, avoid using map tech_colors = config[\"plotting\"][\"tech_colors\"] colors = pd.Series(tech_colors).reindex(carrier_i) # try to fill missing colors with tech_colors after renaming missing_colors_i = colors[colors.isna()].index colors[missing_colors_i] = missing_colors_i.map(lambda x: rename_techs(x, nice_names)).map( tech_colors ) if colors.isna().any(): missing_i = list(colors.index[colors.isna()]) logger.warning(f\"tech_colors for carriers {missing_i} not defined in config.\") n.carriers[\"color\"] = n.carriers.color.where(n.carriers.color != \"\", colors)","title":"sanitize_carriers"},{"location":"docs/reference/add_existing_baseyear/","text":"Functions supporting myopic pathway network creation add_build_year_to_new_assets(n, baseyear) add a build year to new assets Parameters: n ( Network ) \u2013 the network baseyear ( int ) \u2013 year in which optimized assets are built Source code in workflow/scripts/add_existing_baseyear.py def add_build_year_to_new_assets(n: pypsa.Network, baseyear: int): \"\"\"add a build year to new assets Args: n (pypsa.Network): the network baseyear (int): year in which optimized assets are built \"\"\" # Give assets with lifetimes and no build year the build year baseyear for c in n.iterate_components([\"Link\", \"Generator\", \"Store\"]): attr = \"e\" if c.name == \"Store\" else \"p\" assets = c.df.index[(c.df.lifetime != np.inf) & (c.df[attr + \"_nom_extendable\"] is True)] # add -baseyear to name rename = pd.Series(c.df.index, c.df.index) rename[assets] += \"-\" + str(baseyear) c.df.rename(index=rename, inplace=True) assets = c.df.index[ (c.df.lifetime != np.inf) & (c.df[attr + \"_nom_extendable\"] is True) & (c.df.build_year == 0) ] c.df.loc[assets, \"build_year\"] = baseyear # rename time-dependent selection = n.component_attrs[c.name].type.str.contains(\"series\") & n.component_attrs[ c.name ].status.str.contains(\"Input\") for attr in n.component_attrs[c.name].index[selection]: c.pnl[attr].rename(columns=rename, inplace=True) add_power_capacities_installed_before_baseyear(n, grouping_years, costs, baseyear, config) Parameters n : pypsa.Network grouping_years : intervals to group existing capacities costs : to read lifetime to estimate YearDecomissioning baseyear : int Source code in workflow/scripts/add_existing_baseyear.py def add_power_capacities_installed_before_baseyear( n: pypsa.Network, grouping_years, costs, baseyear, config ): \"\"\" Parameters ---------- n : pypsa.Network grouping_years : intervals to group existing capacities costs : to read lifetime to estimate YearDecomissioning baseyear : int \"\"\" logger.info(\"adding power capacities installed before baseyear\") df_agg = pd.DataFrame() # include renewables in df_agg add_existing_capacities(df_agg) df_agg[\"grouping_year\"] = np.take( grouping_years, np.digitize(df_agg.DateIn, grouping_years, right=True) ) df = df_agg.pivot_table( index=[\"grouping_year\", \"Tech\"], columns=\"cluster_bus\", values=\"Capacity\", aggfunc=\"sum\" ) df.fillna(0) carrier = { \"coal\": \"coal power plant\", \"CHP coal\": \"CHP coal\", \"CHP gas\": \"CHP gas\", \"OCGT\": \"OCGT gas\", \"solar\": \"solar\", \"solar thermal\": \"solar thermal\", \"onwind\": \"onwind\", \"offwind\": \"offwind\", \"coal boiler\": \"coal boiler\", \"ground heat pump\": \"heat pump\", \"nuclear\": \"nuclear\", } for grouping_year, generator in df.index: # capacity is the capacity in MW at each node for this capacity = df.loc[grouping_year, generator] capacity = capacity[~capacity.isna()] capacity = capacity[capacity > config[\"existing_capacities\"][\"threshold_capacity\"]] if generator in [\"solar\", \"onwind\", \"offwind\"]: p_max_pu = n.generators_t.p_max_pu[capacity.index + \" \" + generator] n.add( \"Generator\", capacity.index, suffix=\" \" + generator + \"-\" + str(grouping_year), bus=capacity.index, carrier=carrier[generator], p_nom=capacity, p_nom_min=capacity, p_nom_extendable=False, marginal_cost=costs.at[generator, \"marginal_cost\"], capital_cost=costs.at[generator, \"capital_cost\"], efficiency=costs.at[generator, \"efficiency\"], p_max_pu=p_max_pu.rename(columns=n.generators.bus), build_year=grouping_year, lifetime=costs.at[generator, \"lifetime\"], ) if generator == \"coal\": n.add( \"Generator\", capacity.index, suffix=\" \" + generator + \"-\" + str(grouping_year), bus=capacity.index, carrier=carrier[generator], p_nom=capacity, p_nom_extendable=False, marginal_cost=costs.at[generator, \"marginal_cost\"], capital_cost=costs.at[generator, \"capital_cost\"], efficiency=costs.at[generator, \"efficiency\"], build_year=grouping_year, lifetime=costs.at[generator, \"lifetime\"], ) if generator == \"nuclear\": n.add( \"Generator\", capacity.index, suffix=\" \" + generator + \"-\" + str(grouping_year), bus=capacity.index, carrier=carrier[generator], p_nom=capacity, p_nom_min=capacity, p_nom_extendable=False, p_min_pu=0.7, marginal_cost=costs.at[generator, \"marginal_cost\"], capital_cost=costs.at[generator, \"capital_cost\"], efficiency=costs.at[generator, \"efficiency\"], build_year=grouping_year, lifetime=costs.at[generator, \"lifetime\"], ) if generator == \"solar thermal\": p_max_pu = n.generators_t.p_max_pu[capacity.index + \" central \" + generator] p_max_pu.columns = capacity.index n.add( \"Generator\", capacity.index, suffix=\" central \" + generator + \"-\" + str(grouping_year), bus=capacity.index + \" central heat\", carrier=carrier[generator], p_nom=capacity, p_nom_min=capacity, p_nom_extendable=False, marginal_cost=costs.at[\"central \" + generator, \"marginal_cost\"], capital_cost=costs.at[\"central \" + generator, \"capital_cost\"], p_max_pu=p_max_pu, build_year=grouping_year, lifetime=costs.at[\"central \" + generator, \"lifetime\"], ) if generator == \"CHP coal\": bus0 = capacity.index + \" coal\" n.add( \"Link\", capacity.index, suffix=\" \" + generator + \" generator\" + \"-\" + str(grouping_year), bus0=bus0, bus1=capacity.index, carrier=carrier[generator], marginal_cost=0.37 * costs.at[\"central coal CHP\", \"VOM\"], # NB: VOM is per MWel capital_cost=0.37 * costs.at[\"central coal CHP\", \"capital_cost\"], # NB: fixed cost is per MWel, p_nom=capacity / 0.37, p_nom_min=capacity / 0.37, p_nom_extendable=False, efficiency=0.37, p_nom_ratio=1.0, c_b=0.75, build_year=grouping_year, lifetime=costs.at[\"central coal CHP\", \"lifetime\"], ) n.add( \"Link\", capacity.index, suffix=\" \" + generator + \" boiler\" + \"-\" + str(grouping_year), bus0=bus0, bus1=capacity.index + \" central heat\", carrier=carrier[generator], marginal_cost=0.37 * costs.at[\"central coal CHP\", \"VOM\"], # NB: VOM is per MWel p_nom=capacity / 0.37 * 0.15, p_nom_min=capacity / 0.37 * 0.15, p_nom_extendable=False, efficiency=0.37 / 0.15, build_year=grouping_year, lifetime=costs.at[\"central coal CHP\", \"lifetime\"], ) if generator == \"CHP gas\": bus0 = capacity.index + \" gas\" n.add( \"Link\", capacity.index, suffix=\" \" + generator + \" generator\" + \"-\" + str(grouping_year), bus0=bus0, bus1=capacity.index, carrier=carrier[generator], marginal_cost=costs.at[\"central gas CHP\", \"efficiency\"] * costs.at[\"central gas CHP\", \"VOM\"], # NB: VOM is per MWel capital_cost=costs.at[\"central gas CHP\", \"efficiency\"] * costs.at[\"central gas CHP\", \"capital_cost\"], # NB: fixed cost is per MWel, p_nom=capacity / costs.at[\"central gas CHP\", \"efficiency\"], p_nom_min=capacity / costs.at[\"central gas CHP\", \"efficiency\"], p_nom_extendable=False, efficiency=costs.at[\"central gas CHP\", \"efficiency\"], p_nom_ratio=1.0, c_b=costs.at[\"central gas CHP\", \"c_b\"], build_year=grouping_year, lifetime=costs.at[\"central gas CHP\", \"lifetime\"], ) n.add( \"Link\", capacity.index, suffix=\" \" + generator + \" boiler\" + \"-\" + str(grouping_year), bus0=bus0, bus1=capacity.index + \" central heat\", carrier=carrier[generator], marginal_cost=costs.at[\"central gas CHP\", \"efficiency\"] * costs.at[\"central gas CHP\", \"VOM\"], # NB: VOM is per MWel p_nom=capacity / costs.at[\"central gas CHP\", \"efficiency\"] * costs.at[\"central gas CHP\", \"c_v\"], p_nom_min=capacity / costs.at[\"central gas CHP\", \"efficiency\"] * costs.at[\"central gas CHP\", \"c_v\"], p_nom_extendable=False, efficiency=costs.at[\"central gas CHP\", \"efficiency\"] / costs.at[\"central gas CHP\", \"c_v\"], build_year=grouping_year, lifetime=costs.at[\"central gas CHP\", \"lifetime\"], ) if generator == \"OCGT\": bus0 = capacity.index + \" gas\" n.add( \"Link\", capacity.index, suffix=\" \" + generator + \"-\" + str(grouping_year), bus0=bus0, bus1=capacity.index, carrier=carrier[generator], marginal_cost=costs.at[generator, \"efficiency\"] * costs.at[generator, \"VOM\"], # NB: VOM is per MWel capital_cost=costs.at[generator, \"efficiency\"] * costs.at[generator, \"capital_cost\"], # NB: fixed cost is per MWel p_nom=capacity / costs.at[generator, \"efficiency\"], p_nom_min=capacity / costs.at[generator, \"efficiency\"], p_nom_extendable=False, efficiency=costs.at[generator, \"efficiency\"], build_year=grouping_year, lifetime=costs.at[generator, \"lifetime\"], ) if generator == \"coal boiler\": bus0 = capacity.index + \" coal\" for cat in [\" central \"]: n.add( \"Link\", capacity.index, suffix=\"\" + cat + generator + \"-\" + str(grouping_year), bus0=bus0, bus1=capacity.index + cat + \"heat\", carrier=carrier[generator], marginal_cost=costs.at[cat.lstrip() + generator, \"efficiency\"] * costs.at[cat.lstrip() + generator, \"VOM\"], capital_cost=costs.at[cat.lstrip() + generator, \"efficiency\"] * costs.at[cat.lstrip() + generator, \"capital_cost\"], p_nom=capacity / costs.at[cat.lstrip() + generator, \"efficiency\"], p_nom_min=capacity / costs.at[cat.lstrip() + generator, \"efficiency\"], p_nom_extendable=False, efficiency=costs.at[cat.lstrip() + generator, \"efficiency\"], build_year=grouping_year, lifetime=costs.at[cat.lstrip() + generator, \"lifetime\"], ) # TODO fix centralise if generator == \"ground heat pump\": date_range = pd.date_range( \"2025-01-01 00:00\", \"2025-12-31 23:00\", freq=config[\"snapshots\"][\"freq\"], tz=\"Asia/shanghai\", ) date_range = date_range.map(lambda t: t.replace(year=2020)) with pd.HDFStore(snakemake.input.cop_name, mode=\"r\") as store: gshp_cop = store[\"gshp_cop_profiles\"] gshp_cop.index = gshp_cop.index.tz_localize(\"Asia/shanghai\") gshp_cop = gshp_cop.loc[date_range].set_index(n.snapshots) n.add( \"Link\", capacity.index, suffix=\" \" + generator + \"-\" + str(grouping_year), bus0=capacity.index, bus1=capacity.index + \" central heat\", carrier=\"heat pump\", efficiency=( gshp_cop[capacity.index] if config[\"time_dep_hp_cop\"] else costs.at[\"decentral ground-sourced heat pump\", \"efficiency\"] ), capital_cost=costs.at[\"decentral ground-sourced heat pump\", \"efficiency\"] * costs.at[\"decentral ground-sourced heat pump\", \"capital_cost\"], marginal_cost=costs.at[\"decentral ground-sourced heat pump\", \"efficiency\"] * costs.at[\"decentral ground-sourced heat pump\", \"marginal_cost\"], p_nom=capacity / costs.at[\"decentral ground-sourced heat pump\", \"efficiency\"], p_nom_min=capacity / costs.at[\"decentral ground-sourced heat pump\", \"efficiency\"], p_nom_extendable=False, build_year=grouping_year, lifetime=costs.at[\"decentral ground-sourced heat pump\", \"lifetime\"], )","title":"add_existing_baseyear"},{"location":"docs/reference/add_existing_baseyear/#add_existing_baseyear.add_build_year_to_new_assets","text":"add a build year to new assets Parameters: n ( Network ) \u2013 the network baseyear ( int ) \u2013 year in which optimized assets are built Source code in workflow/scripts/add_existing_baseyear.py def add_build_year_to_new_assets(n: pypsa.Network, baseyear: int): \"\"\"add a build year to new assets Args: n (pypsa.Network): the network baseyear (int): year in which optimized assets are built \"\"\" # Give assets with lifetimes and no build year the build year baseyear for c in n.iterate_components([\"Link\", \"Generator\", \"Store\"]): attr = \"e\" if c.name == \"Store\" else \"p\" assets = c.df.index[(c.df.lifetime != np.inf) & (c.df[attr + \"_nom_extendable\"] is True)] # add -baseyear to name rename = pd.Series(c.df.index, c.df.index) rename[assets] += \"-\" + str(baseyear) c.df.rename(index=rename, inplace=True) assets = c.df.index[ (c.df.lifetime != np.inf) & (c.df[attr + \"_nom_extendable\"] is True) & (c.df.build_year == 0) ] c.df.loc[assets, \"build_year\"] = baseyear # rename time-dependent selection = n.component_attrs[c.name].type.str.contains(\"series\") & n.component_attrs[ c.name ].status.str.contains(\"Input\") for attr in n.component_attrs[c.name].index[selection]: c.pnl[attr].rename(columns=rename, inplace=True)","title":"add_build_year_to_new_assets"},{"location":"docs/reference/add_existing_baseyear/#add_existing_baseyear.add_power_capacities_installed_before_baseyear","text":"","title":"add_power_capacities_installed_before_baseyear"},{"location":"docs/reference/add_existing_baseyear/#add_existing_baseyear.add_power_capacities_installed_before_baseyear--parameters","text":"n : pypsa.Network grouping_years : intervals to group existing capacities costs : to read lifetime to estimate YearDecomissioning baseyear : int Source code in workflow/scripts/add_existing_baseyear.py def add_power_capacities_installed_before_baseyear( n: pypsa.Network, grouping_years, costs, baseyear, config ): \"\"\" Parameters ---------- n : pypsa.Network grouping_years : intervals to group existing capacities costs : to read lifetime to estimate YearDecomissioning baseyear : int \"\"\" logger.info(\"adding power capacities installed before baseyear\") df_agg = pd.DataFrame() # include renewables in df_agg add_existing_capacities(df_agg) df_agg[\"grouping_year\"] = np.take( grouping_years, np.digitize(df_agg.DateIn, grouping_years, right=True) ) df = df_agg.pivot_table( index=[\"grouping_year\", \"Tech\"], columns=\"cluster_bus\", values=\"Capacity\", aggfunc=\"sum\" ) df.fillna(0) carrier = { \"coal\": \"coal power plant\", \"CHP coal\": \"CHP coal\", \"CHP gas\": \"CHP gas\", \"OCGT\": \"OCGT gas\", \"solar\": \"solar\", \"solar thermal\": \"solar thermal\", \"onwind\": \"onwind\", \"offwind\": \"offwind\", \"coal boiler\": \"coal boiler\", \"ground heat pump\": \"heat pump\", \"nuclear\": \"nuclear\", } for grouping_year, generator in df.index: # capacity is the capacity in MW at each node for this capacity = df.loc[grouping_year, generator] capacity = capacity[~capacity.isna()] capacity = capacity[capacity > config[\"existing_capacities\"][\"threshold_capacity\"]] if generator in [\"solar\", \"onwind\", \"offwind\"]: p_max_pu = n.generators_t.p_max_pu[capacity.index + \" \" + generator] n.add( \"Generator\", capacity.index, suffix=\" \" + generator + \"-\" + str(grouping_year), bus=capacity.index, carrier=carrier[generator], p_nom=capacity, p_nom_min=capacity, p_nom_extendable=False, marginal_cost=costs.at[generator, \"marginal_cost\"], capital_cost=costs.at[generator, \"capital_cost\"], efficiency=costs.at[generator, \"efficiency\"], p_max_pu=p_max_pu.rename(columns=n.generators.bus), build_year=grouping_year, lifetime=costs.at[generator, \"lifetime\"], ) if generator == \"coal\": n.add( \"Generator\", capacity.index, suffix=\" \" + generator + \"-\" + str(grouping_year), bus=capacity.index, carrier=carrier[generator], p_nom=capacity, p_nom_extendable=False, marginal_cost=costs.at[generator, \"marginal_cost\"], capital_cost=costs.at[generator, \"capital_cost\"], efficiency=costs.at[generator, \"efficiency\"], build_year=grouping_year, lifetime=costs.at[generator, \"lifetime\"], ) if generator == \"nuclear\": n.add( \"Generator\", capacity.index, suffix=\" \" + generator + \"-\" + str(grouping_year), bus=capacity.index, carrier=carrier[generator], p_nom=capacity, p_nom_min=capacity, p_nom_extendable=False, p_min_pu=0.7, marginal_cost=costs.at[generator, \"marginal_cost\"], capital_cost=costs.at[generator, \"capital_cost\"], efficiency=costs.at[generator, \"efficiency\"], build_year=grouping_year, lifetime=costs.at[generator, \"lifetime\"], ) if generator == \"solar thermal\": p_max_pu = n.generators_t.p_max_pu[capacity.index + \" central \" + generator] p_max_pu.columns = capacity.index n.add( \"Generator\", capacity.index, suffix=\" central \" + generator + \"-\" + str(grouping_year), bus=capacity.index + \" central heat\", carrier=carrier[generator], p_nom=capacity, p_nom_min=capacity, p_nom_extendable=False, marginal_cost=costs.at[\"central \" + generator, \"marginal_cost\"], capital_cost=costs.at[\"central \" + generator, \"capital_cost\"], p_max_pu=p_max_pu, build_year=grouping_year, lifetime=costs.at[\"central \" + generator, \"lifetime\"], ) if generator == \"CHP coal\": bus0 = capacity.index + \" coal\" n.add( \"Link\", capacity.index, suffix=\" \" + generator + \" generator\" + \"-\" + str(grouping_year), bus0=bus0, bus1=capacity.index, carrier=carrier[generator], marginal_cost=0.37 * costs.at[\"central coal CHP\", \"VOM\"], # NB: VOM is per MWel capital_cost=0.37 * costs.at[\"central coal CHP\", \"capital_cost\"], # NB: fixed cost is per MWel, p_nom=capacity / 0.37, p_nom_min=capacity / 0.37, p_nom_extendable=False, efficiency=0.37, p_nom_ratio=1.0, c_b=0.75, build_year=grouping_year, lifetime=costs.at[\"central coal CHP\", \"lifetime\"], ) n.add( \"Link\", capacity.index, suffix=\" \" + generator + \" boiler\" + \"-\" + str(grouping_year), bus0=bus0, bus1=capacity.index + \" central heat\", carrier=carrier[generator], marginal_cost=0.37 * costs.at[\"central coal CHP\", \"VOM\"], # NB: VOM is per MWel p_nom=capacity / 0.37 * 0.15, p_nom_min=capacity / 0.37 * 0.15, p_nom_extendable=False, efficiency=0.37 / 0.15, build_year=grouping_year, lifetime=costs.at[\"central coal CHP\", \"lifetime\"], ) if generator == \"CHP gas\": bus0 = capacity.index + \" gas\" n.add( \"Link\", capacity.index, suffix=\" \" + generator + \" generator\" + \"-\" + str(grouping_year), bus0=bus0, bus1=capacity.index, carrier=carrier[generator], marginal_cost=costs.at[\"central gas CHP\", \"efficiency\"] * costs.at[\"central gas CHP\", \"VOM\"], # NB: VOM is per MWel capital_cost=costs.at[\"central gas CHP\", \"efficiency\"] * costs.at[\"central gas CHP\", \"capital_cost\"], # NB: fixed cost is per MWel, p_nom=capacity / costs.at[\"central gas CHP\", \"efficiency\"], p_nom_min=capacity / costs.at[\"central gas CHP\", \"efficiency\"], p_nom_extendable=False, efficiency=costs.at[\"central gas CHP\", \"efficiency\"], p_nom_ratio=1.0, c_b=costs.at[\"central gas CHP\", \"c_b\"], build_year=grouping_year, lifetime=costs.at[\"central gas CHP\", \"lifetime\"], ) n.add( \"Link\", capacity.index, suffix=\" \" + generator + \" boiler\" + \"-\" + str(grouping_year), bus0=bus0, bus1=capacity.index + \" central heat\", carrier=carrier[generator], marginal_cost=costs.at[\"central gas CHP\", \"efficiency\"] * costs.at[\"central gas CHP\", \"VOM\"], # NB: VOM is per MWel p_nom=capacity / costs.at[\"central gas CHP\", \"efficiency\"] * costs.at[\"central gas CHP\", \"c_v\"], p_nom_min=capacity / costs.at[\"central gas CHP\", \"efficiency\"] * costs.at[\"central gas CHP\", \"c_v\"], p_nom_extendable=False, efficiency=costs.at[\"central gas CHP\", \"efficiency\"] / costs.at[\"central gas CHP\", \"c_v\"], build_year=grouping_year, lifetime=costs.at[\"central gas CHP\", \"lifetime\"], ) if generator == \"OCGT\": bus0 = capacity.index + \" gas\" n.add( \"Link\", capacity.index, suffix=\" \" + generator + \"-\" + str(grouping_year), bus0=bus0, bus1=capacity.index, carrier=carrier[generator], marginal_cost=costs.at[generator, \"efficiency\"] * costs.at[generator, \"VOM\"], # NB: VOM is per MWel capital_cost=costs.at[generator, \"efficiency\"] * costs.at[generator, \"capital_cost\"], # NB: fixed cost is per MWel p_nom=capacity / costs.at[generator, \"efficiency\"], p_nom_min=capacity / costs.at[generator, \"efficiency\"], p_nom_extendable=False, efficiency=costs.at[generator, \"efficiency\"], build_year=grouping_year, lifetime=costs.at[generator, \"lifetime\"], ) if generator == \"coal boiler\": bus0 = capacity.index + \" coal\" for cat in [\" central \"]: n.add( \"Link\", capacity.index, suffix=\"\" + cat + generator + \"-\" + str(grouping_year), bus0=bus0, bus1=capacity.index + cat + \"heat\", carrier=carrier[generator], marginal_cost=costs.at[cat.lstrip() + generator, \"efficiency\"] * costs.at[cat.lstrip() + generator, \"VOM\"], capital_cost=costs.at[cat.lstrip() + generator, \"efficiency\"] * costs.at[cat.lstrip() + generator, \"capital_cost\"], p_nom=capacity / costs.at[cat.lstrip() + generator, \"efficiency\"], p_nom_min=capacity / costs.at[cat.lstrip() + generator, \"efficiency\"], p_nom_extendable=False, efficiency=costs.at[cat.lstrip() + generator, \"efficiency\"], build_year=grouping_year, lifetime=costs.at[cat.lstrip() + generator, \"lifetime\"], ) # TODO fix centralise if generator == \"ground heat pump\": date_range = pd.date_range( \"2025-01-01 00:00\", \"2025-12-31 23:00\", freq=config[\"snapshots\"][\"freq\"], tz=\"Asia/shanghai\", ) date_range = date_range.map(lambda t: t.replace(year=2020)) with pd.HDFStore(snakemake.input.cop_name, mode=\"r\") as store: gshp_cop = store[\"gshp_cop_profiles\"] gshp_cop.index = gshp_cop.index.tz_localize(\"Asia/shanghai\") gshp_cop = gshp_cop.loc[date_range].set_index(n.snapshots) n.add( \"Link\", capacity.index, suffix=\" \" + generator + \"-\" + str(grouping_year), bus0=capacity.index, bus1=capacity.index + \" central heat\", carrier=\"heat pump\", efficiency=( gshp_cop[capacity.index] if config[\"time_dep_hp_cop\"] else costs.at[\"decentral ground-sourced heat pump\", \"efficiency\"] ), capital_cost=costs.at[\"decentral ground-sourced heat pump\", \"efficiency\"] * costs.at[\"decentral ground-sourced heat pump\", \"capital_cost\"], marginal_cost=costs.at[\"decentral ground-sourced heat pump\", \"efficiency\"] * costs.at[\"decentral ground-sourced heat pump\", \"marginal_cost\"], p_nom=capacity / costs.at[\"decentral ground-sourced heat pump\", \"efficiency\"], p_nom_min=capacity / costs.at[\"decentral ground-sourced heat pump\", \"efficiency\"], p_nom_extendable=False, build_year=grouping_year, lifetime=costs.at[\"decentral ground-sourced heat pump\", \"lifetime\"], )","title":"Parameters"},{"location":"docs/reference/build_biomass_potential/","text":"build_biomass_potential(biomass_potentials_path) summary Parameters: biomass_potentials_path ( PathLike ) \u2013 description . Source code in workflow/scripts/build_biomass_potential.py def build_biomass_potential(biomass_potentials_path: PathLike): \"\"\"_summary_ Args: biomass_potentials_path (PathLike, optional): _description_. \"\"\" df = pd.read_excel(biomass_potentials_path, sheet_name=\"supplementary data 1\") df = df.groupby(\"Province name\").sum() df = df[df.columns[df.columns.str.contains(\"Agricultural residues burnt as waste\")]].sum(axis=1) # convert t biomass yr-1 to TWh heat_content = 19 # GJ/t df = df * heat_content * 2.7777 * 1e-1 # convert GJ to MWh ##rename df = df.rename(index={\"Inner-Monglia\": \"InnerMongolia\", \"Anhui \": \"Anhui\"}) df = df.add_suffix(\" biomass\") df.to_hdf(snakemake.output.biomass_potential, key=\"biomass\")","title":"build_biomass_potential"},{"location":"docs/reference/build_biomass_potential/#build_biomass_potential.build_biomass_potential","text":"summary Parameters: biomass_potentials_path ( PathLike ) \u2013 description . Source code in workflow/scripts/build_biomass_potential.py def build_biomass_potential(biomass_potentials_path: PathLike): \"\"\"_summary_ Args: biomass_potentials_path (PathLike, optional): _description_. \"\"\" df = pd.read_excel(biomass_potentials_path, sheet_name=\"supplementary data 1\") df = df.groupby(\"Province name\").sum() df = df[df.columns[df.columns.str.contains(\"Agricultural residues burnt as waste\")]].sum(axis=1) # convert t biomass yr-1 to TWh heat_content = 19 # GJ/t df = df * heat_content * 2.7777 * 1e-1 # convert GJ to MWh ##rename df = df.rename(index={\"Inner-Monglia\": \"InnerMongolia\", \"Anhui \": \"Anhui\"}) df = df.add_suffix(\" biomass\") df.to_hdf(snakemake.output.biomass_potential, key=\"biomass\")","title":"build_biomass_potential"},{"location":"docs/reference/build_cop_profiles/","text":"Snakemake rule script to calculate the heat pump coefficient of performance with atlite build_cop_profiles() Build COP time profiles with atlite Write outputs to snakemake.output.cop as hf5 Source code in workflow/scripts/build_cop_profiles.py def build_cop_profiles(): \"\"\"Build COP time profiles with atlite Write outputs to snakemake.output.cop as hf5 \"\"\" with pd.HDFStore(snakemake.input.population_map, mode=\"r\") as store: pop_map = store[\"population_gridcell_map\"] # this one includes soil temperature cutout = atlite.Cutout(snakemake.input.cutout) pop_matrix = sp.sparse.csr_matrix(pop_map.T) index = pop_map.columns index.name = \"provinces\" soil_temp = cutout.soil_temperature(matrix=pop_matrix, index=index) soil_temp[\"time\"] = ( pd.DatetimeIndex(soil_temp[\"time\"].values, tz=\"UTC\") .tz_convert(TIMEZONE) .tz_localize(None) .values ) with pd.HDFStore(snakemake.input.temp, mode=\"r\") as store: temp = store[\"temperature\"] source_T = temp source_soil_T = soil_temp.to_pandas().divide(pop_map.sum()) # quadratic regression based on Staffell et al. (2012) # https://doi.org/10.1039/C2EE22653G sink_T = 55.0 # Based on DTU / large area radiators delta_T = sink_T - source_T # For ASHP def ashp_cop(d): return 6.81 - 0.121 * d + 0.000630 * d**2 cop = ashp_cop(delta_T) delta_soil_T = sink_T - source_soil_T # For GSHP def gshp_cop(d): return 8.77 - 0.150 * d + 0.000734 * d**2 cop_soil = gshp_cop(delta_soil_T) with pd.HDFStore(snakemake.output.cop, mode=\"w\", complevel=4) as store: store[\"ashp_cop_profiles\"] = cop store[\"gshp_cop_profiles\"] = cop_soil","title":"build_cop_profiles"},{"location":"docs/reference/build_cop_profiles/#build_cop_profiles.build_cop_profiles","text":"Build COP time profiles with atlite Write outputs to snakemake.output.cop as hf5 Source code in workflow/scripts/build_cop_profiles.py def build_cop_profiles(): \"\"\"Build COP time profiles with atlite Write outputs to snakemake.output.cop as hf5 \"\"\" with pd.HDFStore(snakemake.input.population_map, mode=\"r\") as store: pop_map = store[\"population_gridcell_map\"] # this one includes soil temperature cutout = atlite.Cutout(snakemake.input.cutout) pop_matrix = sp.sparse.csr_matrix(pop_map.T) index = pop_map.columns index.name = \"provinces\" soil_temp = cutout.soil_temperature(matrix=pop_matrix, index=index) soil_temp[\"time\"] = ( pd.DatetimeIndex(soil_temp[\"time\"].values, tz=\"UTC\") .tz_convert(TIMEZONE) .tz_localize(None) .values ) with pd.HDFStore(snakemake.input.temp, mode=\"r\") as store: temp = store[\"temperature\"] source_T = temp source_soil_T = soil_temp.to_pandas().divide(pop_map.sum()) # quadratic regression based on Staffell et al. (2012) # https://doi.org/10.1039/C2EE22653G sink_T = 55.0 # Based on DTU / large area radiators delta_T = sink_T - source_T # For ASHP def ashp_cop(d): return 6.81 - 0.121 * d + 0.000630 * d**2 cop = ashp_cop(delta_T) delta_soil_T = sink_T - source_soil_T # For GSHP def gshp_cop(d): return 8.77 - 0.150 * d + 0.000734 * d**2 cop_soil = gshp_cop(delta_soil_T) with pd.HDFStore(snakemake.output.cop, mode=\"w\", complevel=4) as store: store[\"ashp_cop_profiles\"] = cop store[\"gshp_cop_profiles\"] = cop_soil","title":"build_cop_profiles"},{"location":"docs/reference/build_cutout/","text":"Functions to download ERA5/SARAH data and build the atlite cutout for the atlite. These functions linked to the build_cutout rule. cutout_timespan(config, weather_year) build the cutout timespan. Note that the coutout requests are in UTC (TBC) Parameters: config ( dict ) \u2013 the snakemake config weather_year ( dict ) \u2013 the coutout weather year Returns: tuple ( list ) \u2013 end and start of the cutout timespan Source code in workflow/scripts/build_cutout.py def cutout_timespan(config: dict, weather_year: int) -> list: \"\"\"build the cutout timespan. Note that the coutout requests are in UTC (TBC) Args: config (dict): the snakemake config weather_year (dict): the coutout weather year Returns: tuple: end and start of the cutout timespan \"\"\" snapshot_cfg = config[\"snapshots\"] # make snapshots for TZ and then convert to naive UTC for atlite snapshots = ( make_periodic_snapshots( year=weather_year, freq=snapshot_cfg[\"freq\"], start_day_hour=snapshot_cfg[\"start\"], end_day_hour=snapshot_cfg[\"end\"], bounds=snapshot_cfg[\"bounds\"], # here we need to convert UTC to local tz=TIMEZONE, end_year=(None if not snapshot_cfg[\"end_year_plus1\"] else weather_year + 1), ) .tz_convert(\"UTC\") .tz_localize(None) ) return [snapshots[0], snapshots[-1]]","title":"build_cutout"},{"location":"docs/reference/build_cutout/#build_cutout.cutout_timespan","text":"build the cutout timespan. Note that the coutout requests are in UTC (TBC) Parameters: config ( dict ) \u2013 the snakemake config weather_year ( dict ) \u2013 the coutout weather year Returns: tuple ( list ) \u2013 end and start of the cutout timespan Source code in workflow/scripts/build_cutout.py def cutout_timespan(config: dict, weather_year: int) -> list: \"\"\"build the cutout timespan. Note that the coutout requests are in UTC (TBC) Args: config (dict): the snakemake config weather_year (dict): the coutout weather year Returns: tuple: end and start of the cutout timespan \"\"\" snapshot_cfg = config[\"snapshots\"] # make snapshots for TZ and then convert to naive UTC for atlite snapshots = ( make_periodic_snapshots( year=weather_year, freq=snapshot_cfg[\"freq\"], start_day_hour=snapshot_cfg[\"start\"], end_day_hour=snapshot_cfg[\"end\"], bounds=snapshot_cfg[\"bounds\"], # here we need to convert UTC to local tz=TIMEZONE, end_year=(None if not snapshot_cfg[\"end_year_plus1\"] else weather_year + 1), ) .tz_convert(\"UTC\") .tz_localize(None) ) return [snapshots[0], snapshots[-1]]","title":"cutout_timespan"},{"location":"docs/reference/build_load_profiles/","text":"Functions for the rules to build the hourly heat and load demand profiles - electricity load profiles are based on scaling an hourly base year profile to yearly future projections - daily heating demand is based on the degree day approx (from atlite) & upscaled hourly based on an intraday profile (for Denmark by default, see snakefile) build_daily_heat_demand_profiles(heat_demand_config, atlite_heating_hr_shift, switch_month_day=True) build the heat demand profile according to forecast demans Parameters: heat_demand_config ( dict ) \u2013 the heat demand configuration atlite_heating_hr_shift ( int ) \u2013 the hour shift for heating demand, needed due to imperfect timezone handling in atlite switch_month_day ( bool , default: True ) \u2013 whether to switch month & day from heat_demand_config. Defaults to True. Returns: pd.DataFrame: regional daily heating demand with April to Sept forced to 0 Source code in workflow/scripts/build_load_profiles.py def build_daily_heat_demand_profiles( heat_demand_config: dict, atlite_heating_hr_shift: int, switch_month_day: bool = True ) -> pd.DataFrame: \"\"\"build the heat demand profile according to forecast demans Args: heat_demand_config (dict): the heat demand configuration atlite_heating_hr_shift (int): the hour shift for heating demand, needed due to imperfect timezone handling in atlite switch_month_day (bool, optional): whether to switch month & day from heat_demand_config. Defaults to True. Returns: pd.DataFrame: regional daily heating demand with April to Sept forced to 0 \"\"\" with pd.HDFStore(snakemake.input.population_map, mode=\"r\") as store: pop_map = store[\"population_gridcell_map\"] cutout = atlite.Cutout(snakemake.input.cutout) atlite_year = get_cutout_params(snakemake.config)[\"weather_year\"] pop_matrix = sp.sparse.csr_matrix(pop_map.T) index = pop_map.columns index.name = \"provinces\" # TODO clarify a bit here, maybe the po_matrix should be normalised earlier? # unclear whether it's per cap or not total_hd = cutout.heat_demand( matrix=pop_matrix, index=index, threshold=heat_demand_config[\"heating_start_temp\"], a=heat_demand_config[\"heating_lin_slope\"], constant=heat_demand_config[\"heating_offet\"], # hack to bring it back to local from UTC hour_shift=atlite_heating_hr_shift, ) regonal_daily_hd = total_hd.to_pandas().divide(pop_map.sum()) # input given as dd-mm but loc as yyyy-mm-dd if switch_month_day: start_day = \"{}-{}\".format(*heat_demand_config[\"start_day\"].split(\"-\")[::-1]) end_day = \"{}-{}\".format(*heat_demand_config[\"end_day\"].split(\"-\")[::-1]) else: start_day = heat_demand_config[\"start_day\"] end_day = heat_demand_config[\"end_day\"] regonal_daily_hd.loc[f\"{atlite_year}-{start_day}\":f\"{atlite_year}-{end_day}\"] = 0 return regonal_daily_hd build_heat_demand_profile(daily_hd, hot_water_per_day, snapshots, planning_horizons) Downscale the daily heat demand to hourly heat demand using pre-defined intraday profiles THIS FUNCTION ALSO MAKES PROJECTIONS FOR HEATING DEMAND - WHICH IS NOT THE CORRECT PLACE Parameters: daily_hd ( DataFrame ) \u2013 the day resolved heat demand for each region (atlite time axis) hot_water_per_day ( DataFrame ) \u2013 the day resolved hot water demand for each region snapshots ( date_range ) \u2013 the snapshots for the planning year planning_horizons ( int | str ) \u2013 the planning year Returns: tuple [ DataFrame , DataFrame , object , DataFrame ] \u2013 tuple[pd.DataFrame, pd.DataFrame, object, pd.DataFrame]: heat, space_heat, space_heating_per_hdd, water_heat demands Source code in workflow/scripts/build_load_profiles.py def build_heat_demand_profile( daily_hd: pd.DataFrame, hot_water_per_day: pd.DataFrame, snapshots: pd.date_range, planning_horizons: int | str, ) -> tuple[pd.DataFrame, pd.DataFrame, object, pd.DataFrame]: \"\"\"Downscale the daily heat demand to hourly heat demand using pre-defined intraday profiles THIS FUNCTION ALSO MAKES PROJECTIONS FOR HEATING DEMAND - WHICH IS NOT THE CORRECT PLACE Args: daily_hd (DataFrame): the day resolved heat demand for each region (atlite time axis) hot_water_per_day (DataFrame): the day resolved hot water demand for each region snapshots (pd.date_range): the snapshots for the planning year planning_horizons (int | str): the planning year Returns: tuple[pd.DataFrame, pd.DataFrame, object, pd.DataFrame]: heat, space_heat, space_heating_per_hdd, water_heat demands \"\"\" # TODO - very strange, why would this be needed unless atlite is buggy daily_hd_uniq = daily_hd[~daily_hd.index.duplicated(keep=\"first\")] # hourly resolution regional demand (but wrong data, it's just ffill) heat_demand_hourly = shift_profile_to_planning_year( daily_hd_uniq, planning_yr=planning_horizons ).reindex(index=snapshots, method=\"ffill\") # ===== downscale to hourly ======= intraday_profiles = pd.read_csv(snakemake.input.intraday_profiles, index_col=0) # TODO, does this work with variable frequency? intraday_year_profiles = downscale_time_data( dt_index=heat_demand_hourly.index, weekly_profile=( list(intraday_profiles[\"weekday\"]) * 5 + list(intraday_profiles[\"weekend\"]) * 2 ), regional_tzs=pd.Series(index=PROV_NAMES, data=list(REGIONAL_GEO_TIMEZONES.values())), ) # TWh -> MWh space_heat_demand_total = pd.read_csv(snakemake.input.space_heat_demand, index_col=0) * 1e6 space_heat_demand_total = space_heat_demand_total.squeeze() # ==== SCALE TO FUTURE DEMAND ====== # TODO soft-code ref/base year or find a better variable name # TODO remind coupling: fix this kind of stuff or make separate fn # Belongs outside of this function really and in main factor = make_heat_demand_projections( planning_horizons, snakemake.wildcards[\"heating_demand\"], ref_year=REF_YEAR ) # WOULD BE NICER TO SUM THAN TO WEIGH OR TO directly build the profile with the freq # TODO, does this work with variable frequency? space_heating_per_hdd = (space_heat_demand_total * factor) / ( heat_demand_hourly.sum() * snakemake.config[\"snapshots\"][\"frequency\"] ) space_heat_demand = intraday_year_profiles.mul(heat_demand_hourly).mul(space_heating_per_hdd) water_heat_demand = intraday_year_profiles.mul(hot_water_per_day / 24.0) heat_demand = space_heat_demand + water_heat_demand return heat_demand, space_heat_demand, space_heating_per_hdd, water_heat_demand build_hot_water_per_day(planning_horizons) Make projections for the hot water demand increase and scale the ref year value NB: the ref year value is for 2008 not 2020 -> incorrect Parameters: planning_horizons ( int | str ) \u2013 the planning year to which demand will be projected Raises: ValueError \u2013 if the config projection type is not supported Returns: Series \u2013 pd.Series: regional hot water demand per day Source code in workflow/scripts/build_load_profiles.py def build_hot_water_per_day(planning_horizons: int | str) -> pd.Series: \"\"\"Make projections for the hot water demand increase and scale the ref year value NB: the ref year value is for 2008 not 2020 -> incorrect Args: planning_horizons (int | str): the planning year to which demand will be projected Raises: ValueError: if the config projection type is not supported Returns: pd.Series: regional hot water demand per day \"\"\" with pd.HDFStore(snakemake.input.population, mode=\"r\") as store: population_count = store[\"population\"] unit_hot_water_start_yr = UNIT_HOT_WATER_START_YEAR unit_hot_water_end_yr = UNIT_HOT_WATER_END_YEAR if snakemake.wildcards[\"heating_demand\"] == \"positive\": def func(x, a, b): return a * x + b x = np.array([START_YEAR, END_YEAR]) y = np.array([unit_hot_water_start_yr, unit_hot_water_end_yr]) popt, pcov = curve_fit(func, x, y) unit_hot_water = func(int(planning_horizons), *popt) elif snakemake.wildcards[\"heating_demand\"] == \"constant\": unit_hot_water = unit_hot_water_start_yr elif snakemake.wildcards[\"heating_demand\"] == \"mean\": def lin_func(x: np.array, a: float, b: float) -> np.array: return a * x + b x = np.array([START_YEAR, END_YEAR]) y = np.array([unit_hot_water_start_yr, unit_hot_water_end_yr]) popt, pcov = curve_fit(lin_func, x, y) popt, pcov = curve_fit(lin_func, x, y) unit_hot_water = (lin_func(int(planning_horizons), *popt) + UNIT_HOT_WATER_START_YEAR) / 2 else: raise ValueError(f\"Invalid heating demand type {snakemake.wildcards['heating_demand']}\") # MWh per day per region hot_water_per_day = unit_hot_water * population_count / 365.0 return hot_water_per_day downscale_time_data(dt_index, weekly_profile, regional_tzs) Make hourly resolved data profiles based on exogenous weekdays and weekend profiles. This fn takes into account that the profiles are in local time and that regions may have different timezones. Parameters: dt_index ( DatetimeIndex ) \u2013 the snapshots (in network local naive time) but hourly res. weekly_profile ( Iterable ) \u2013 the weekly profile as a list of 7*24 entries. regional_tzs ( Series ) \u2013 regional geographical timezones for profiles. Defaults to pd.Series(index=PROV_NAMES, data=list(REGIONAL_GEO_TIMEZONES.values())). Returns: DataFrame \u2013 pd.DataFrame: Regionally resolved profiles for each snapshot hour rperesented by dt_index Source code in workflow/scripts/build_load_profiles.py def downscale_time_data( dt_index: pd.DatetimeIndex, weekly_profile: Iterable, regional_tzs: pd.Series, ) -> pd.DataFrame: \"\"\"Make hourly resolved data profiles based on exogenous weekdays and weekend profiles. This fn takes into account that the profiles are in local time and that regions may have different timezones. Args: dt_index (DatetimeIndex): the snapshots (in network local naive time) but hourly res. weekly_profile (Iterable): the weekly profile as a list of 7*24 entries. regional_tzs (pd.Series, optional): regional geographical timezones for profiles. Defaults to pd.Series(index=PROV_NAMES, data=list(REGIONAL_GEO_TIMEZONES.values())). Returns: pd.DataFrame: Regionally resolved profiles for each snapshot hour rperesented by dt_index \"\"\" weekly_profile = pd.Series(weekly_profile, range(24 * 7)) # make a dataframe with timestamps localized to the network TIMEZONE timestamps all_times = pd.DataFrame( dict(zip(PROV_NAMES, [dt_index.tz_localize(TIMEZONE)] * len(PROV_NAMES))), index=dt_index.tz_localize(TIMEZONE), columns=PROV_NAMES, ) # then localize to regional time. _dt ensures index is not changed week_hours = all_times.apply( lambda col: col.dt.tz_convert(regional_tzs[col.name]).tz_localize(None) ) # then convert into week hour & map to the intraday heat demand profile (based on local time) return week_hours.apply(lambda col: col.dt.weekday * 24 + col.dt.hour).apply( lambda col: col.map(weekly_profile) ) make_heat_demand_projections(planning_year, projection_name, ref_year=REF_YEAR) Make projections for heating demand Parameters: projection_name ( str ) \u2013 name of projection planning_year ( int ) \u2013 year to project to ref_year ( int , default: REF_YEAR ) \u2013 reference year. Defaults to REF_YEAR. Returns: float ( float ) \u2013 scaling factor relative to base year for heating demand Source code in workflow/scripts/build_load_profiles.py def make_heat_demand_projections( planning_year: int, projection_name: str, ref_year=REF_YEAR ) -> float: \"\"\"Make projections for heating demand Args: projection_name (str): name of projection planning_year (int): year to project to ref_year (int, optional): reference year. Defaults to REF_YEAR. Returns: float: scaling factor relative to base year for heating demand \"\"\" years = np.array([1985, 1990, 1995, 2000, 2005, 2010, 2015, 2020, 2060]) if projection_name == \"positive\": def func(x, a, b, c, d): \"\"\"Cubic polynomial fit to proj\"\"\" return a * x**3 + b * x**2 + c * x + d # TODO soft code # heated area projection in China # 2060: 6.02 * 36.52 * 1e4 north population * floor_space_per_capita in city heated_area = np.array( [2742, 21263, 64645, 110766, 252056, 435668, 672205, 988209, 2198504] ) # 10000 m2 # Perform curve fitting popt, pcov = curve_fit(func, years, heated_area) factor = func(int(planning_year), *popt) / func(REF_YEAR, *popt) elif projection_name == \"constant\": factor = 1.0 else: raise ValueError(f\"Invalid heating demand projection {projection_name}\") return factor prepare_hourly_load_data(hourly_load_p, prov_codes_p) Read the hourly demand data and prepare it for use in the model Parameters: hourly_load_p ( PathLike ) \u2013 raw elec data from zenodo, see readme in data. prov_codes_p ( PathLike ) \u2013 province mapping for data. Returns: DataFrame \u2013 pd.DataFrame: the hourly demand data with the right province names, in TWh/hr Source code in workflow/scripts/build_load_profiles.py def prepare_hourly_load_data( hourly_load_p: os.PathLike, prov_codes_p: os.PathLike, ) -> pd.DataFrame: \"\"\"Read the hourly demand data and prepare it for use in the model Args: hourly_load_p (os.PathLike, optional): raw elec data from zenodo, see readme in data. prov_codes_p (os.PathLike, optional): province mapping for data. Returns: pd.DataFrame: the hourly demand data with the right province names, in TWh/hr \"\"\" TO_TWh = 1e-6 hourly = pd.read_csv(hourly_load_p) hourly_TWh = hourly.drop(columns=[\"Time Series\"]) * TO_TWh prov_codes = pd.read_csv(prov_codes_p) prov_codes.set_index(\"Code\", inplace=True) hourly_TWh.columns = hourly_TWh.columns.map(prov_codes[\"Full name\"]) return hourly_TWh project_elec_demand(hourly_demand_base_yr_MWh, yearly_projections_TWh, year=2020) project the hourly demand to the future years Parameters: hourly_demand_base_yr_MWh ( DataFrame ) \u2013 the hourly demand in the base year yearly_projections_TWh ( DataFrame ) \u2013 the yearly projections Returns: \u2013 pd.DataFrame: the projected hourly demand Source code in workflow/scripts/build_load_profiles.py def project_elec_demand( hourly_demand_base_yr_MWh: pd.DataFrame, yearly_projections_TWh: pd.DataFrame, year=2020 ): \"\"\"project the hourly demand to the future years Args: hourly_demand_base_yr_MWh (pd.DataFrame): the hourly demand in the base year yearly_projections_TWh (pd.DataFrame): the yearly projections Returns: pd.DataFrame: the projected hourly demand \"\"\" hourly_load_TWH_hr = hourly_demand_base_yr_MWh.loc[:, PROV_NAMES] # normalise the hourly load hourly_load_TWH_hr /= hourly_load_TWH_hr.sum(axis=0) yearly_projections_TWh = yearly_projections_TWh.T.loc[int(year), PROV_NAMES] hourly_load_projected = yearly_projections_TWh.multiply(hourly_load_TWH_hr) if len(hourly_load_projected) == 8784: # rm feb 29th hourly_load_projected.drop(hourly_load_projected.index[1416:1440], inplace=True) elif len(hourly_load_projected) != 8760: raise ValueError(\"The length of the hourly load is not 8760 or 8784 (leap year, dropped)\") snapshots = make_periodic_snapshots( year=year, freq=\"1h\", start_day_hour=\"01-01 00:00:00\", end_day_hour=\"12-31 23:00\", bounds=\"both\", ) hourly_load_projected.index = snapshots return hourly_load_projected read_yearly_projections(yearly_projections_p='resources/data/load/Province_Load_2020_2060.csv') prepare projections for model use Parameters: yearly_projections_p ( PathLike , default: 'resources/data/load/Province_Load_2020_2060.csv' ) \u2013 the data path. Defaults to \"resources/data/load/Province_Load_2020_2060.csv\". Returns: DataFrame \u2013 pd.DataFrame: the formatted data Source code in workflow/scripts/build_load_profiles.py def read_yearly_projections( yearly_projections_p: os.PathLike = \"resources/data/load/Province_Load_2020_2060.csv\", ) -> pd.DataFrame: \"\"\"prepare projections for model use Args: yearly_projections_p (os.PathLike, optional): the data path. Defaults to \"resources/data/load/Province_Load_2020_2060.csv\". Returns: pd.DataFrame: the formatted data \"\"\" TO_TWh = 1 / 10 yearly_proj_TWh = pd.read_csv(yearly_projections_p) yearly_proj_TWh.rename(columns={\"Unnamed: 0\": \"province\"}, inplace=True) yearly_proj_TWh.set_index(\"province\", inplace=True) yearly_proj_TWh.rename(columns={c: int(c) for c in yearly_proj_TWh.columns}, inplace=True) return yearly_proj_TWh * TO_TWh","title":"build_load_profiles"},{"location":"docs/reference/build_load_profiles/#build_load_profiles.build_daily_heat_demand_profiles","text":"build the heat demand profile according to forecast demans Parameters: heat_demand_config ( dict ) \u2013 the heat demand configuration atlite_heating_hr_shift ( int ) \u2013 the hour shift for heating demand, needed due to imperfect timezone handling in atlite switch_month_day ( bool , default: True ) \u2013 whether to switch month & day from heat_demand_config. Defaults to True. Returns: pd.DataFrame: regional daily heating demand with April to Sept forced to 0 Source code in workflow/scripts/build_load_profiles.py def build_daily_heat_demand_profiles( heat_demand_config: dict, atlite_heating_hr_shift: int, switch_month_day: bool = True ) -> pd.DataFrame: \"\"\"build the heat demand profile according to forecast demans Args: heat_demand_config (dict): the heat demand configuration atlite_heating_hr_shift (int): the hour shift for heating demand, needed due to imperfect timezone handling in atlite switch_month_day (bool, optional): whether to switch month & day from heat_demand_config. Defaults to True. Returns: pd.DataFrame: regional daily heating demand with April to Sept forced to 0 \"\"\" with pd.HDFStore(snakemake.input.population_map, mode=\"r\") as store: pop_map = store[\"population_gridcell_map\"] cutout = atlite.Cutout(snakemake.input.cutout) atlite_year = get_cutout_params(snakemake.config)[\"weather_year\"] pop_matrix = sp.sparse.csr_matrix(pop_map.T) index = pop_map.columns index.name = \"provinces\" # TODO clarify a bit here, maybe the po_matrix should be normalised earlier? # unclear whether it's per cap or not total_hd = cutout.heat_demand( matrix=pop_matrix, index=index, threshold=heat_demand_config[\"heating_start_temp\"], a=heat_demand_config[\"heating_lin_slope\"], constant=heat_demand_config[\"heating_offet\"], # hack to bring it back to local from UTC hour_shift=atlite_heating_hr_shift, ) regonal_daily_hd = total_hd.to_pandas().divide(pop_map.sum()) # input given as dd-mm but loc as yyyy-mm-dd if switch_month_day: start_day = \"{}-{}\".format(*heat_demand_config[\"start_day\"].split(\"-\")[::-1]) end_day = \"{}-{}\".format(*heat_demand_config[\"end_day\"].split(\"-\")[::-1]) else: start_day = heat_demand_config[\"start_day\"] end_day = heat_demand_config[\"end_day\"] regonal_daily_hd.loc[f\"{atlite_year}-{start_day}\":f\"{atlite_year}-{end_day}\"] = 0 return regonal_daily_hd","title":"build_daily_heat_demand_profiles"},{"location":"docs/reference/build_load_profiles/#build_load_profiles.build_heat_demand_profile","text":"Downscale the daily heat demand to hourly heat demand using pre-defined intraday profiles THIS FUNCTION ALSO MAKES PROJECTIONS FOR HEATING DEMAND - WHICH IS NOT THE CORRECT PLACE Parameters: daily_hd ( DataFrame ) \u2013 the day resolved heat demand for each region (atlite time axis) hot_water_per_day ( DataFrame ) \u2013 the day resolved hot water demand for each region snapshots ( date_range ) \u2013 the snapshots for the planning year planning_horizons ( int | str ) \u2013 the planning year Returns: tuple [ DataFrame , DataFrame , object , DataFrame ] \u2013 tuple[pd.DataFrame, pd.DataFrame, object, pd.DataFrame]: heat, space_heat, space_heating_per_hdd, water_heat demands Source code in workflow/scripts/build_load_profiles.py def build_heat_demand_profile( daily_hd: pd.DataFrame, hot_water_per_day: pd.DataFrame, snapshots: pd.date_range, planning_horizons: int | str, ) -> tuple[pd.DataFrame, pd.DataFrame, object, pd.DataFrame]: \"\"\"Downscale the daily heat demand to hourly heat demand using pre-defined intraday profiles THIS FUNCTION ALSO MAKES PROJECTIONS FOR HEATING DEMAND - WHICH IS NOT THE CORRECT PLACE Args: daily_hd (DataFrame): the day resolved heat demand for each region (atlite time axis) hot_water_per_day (DataFrame): the day resolved hot water demand for each region snapshots (pd.date_range): the snapshots for the planning year planning_horizons (int | str): the planning year Returns: tuple[pd.DataFrame, pd.DataFrame, object, pd.DataFrame]: heat, space_heat, space_heating_per_hdd, water_heat demands \"\"\" # TODO - very strange, why would this be needed unless atlite is buggy daily_hd_uniq = daily_hd[~daily_hd.index.duplicated(keep=\"first\")] # hourly resolution regional demand (but wrong data, it's just ffill) heat_demand_hourly = shift_profile_to_planning_year( daily_hd_uniq, planning_yr=planning_horizons ).reindex(index=snapshots, method=\"ffill\") # ===== downscale to hourly ======= intraday_profiles = pd.read_csv(snakemake.input.intraday_profiles, index_col=0) # TODO, does this work with variable frequency? intraday_year_profiles = downscale_time_data( dt_index=heat_demand_hourly.index, weekly_profile=( list(intraday_profiles[\"weekday\"]) * 5 + list(intraday_profiles[\"weekend\"]) * 2 ), regional_tzs=pd.Series(index=PROV_NAMES, data=list(REGIONAL_GEO_TIMEZONES.values())), ) # TWh -> MWh space_heat_demand_total = pd.read_csv(snakemake.input.space_heat_demand, index_col=0) * 1e6 space_heat_demand_total = space_heat_demand_total.squeeze() # ==== SCALE TO FUTURE DEMAND ====== # TODO soft-code ref/base year or find a better variable name # TODO remind coupling: fix this kind of stuff or make separate fn # Belongs outside of this function really and in main factor = make_heat_demand_projections( planning_horizons, snakemake.wildcards[\"heating_demand\"], ref_year=REF_YEAR ) # WOULD BE NICER TO SUM THAN TO WEIGH OR TO directly build the profile with the freq # TODO, does this work with variable frequency? space_heating_per_hdd = (space_heat_demand_total * factor) / ( heat_demand_hourly.sum() * snakemake.config[\"snapshots\"][\"frequency\"] ) space_heat_demand = intraday_year_profiles.mul(heat_demand_hourly).mul(space_heating_per_hdd) water_heat_demand = intraday_year_profiles.mul(hot_water_per_day / 24.0) heat_demand = space_heat_demand + water_heat_demand return heat_demand, space_heat_demand, space_heating_per_hdd, water_heat_demand","title":"build_heat_demand_profile"},{"location":"docs/reference/build_load_profiles/#build_load_profiles.build_hot_water_per_day","text":"Make projections for the hot water demand increase and scale the ref year value NB: the ref year value is for 2008 not 2020 -> incorrect Parameters: planning_horizons ( int | str ) \u2013 the planning year to which demand will be projected Raises: ValueError \u2013 if the config projection type is not supported Returns: Series \u2013 pd.Series: regional hot water demand per day Source code in workflow/scripts/build_load_profiles.py def build_hot_water_per_day(planning_horizons: int | str) -> pd.Series: \"\"\"Make projections for the hot water demand increase and scale the ref year value NB: the ref year value is for 2008 not 2020 -> incorrect Args: planning_horizons (int | str): the planning year to which demand will be projected Raises: ValueError: if the config projection type is not supported Returns: pd.Series: regional hot water demand per day \"\"\" with pd.HDFStore(snakemake.input.population, mode=\"r\") as store: population_count = store[\"population\"] unit_hot_water_start_yr = UNIT_HOT_WATER_START_YEAR unit_hot_water_end_yr = UNIT_HOT_WATER_END_YEAR if snakemake.wildcards[\"heating_demand\"] == \"positive\": def func(x, a, b): return a * x + b x = np.array([START_YEAR, END_YEAR]) y = np.array([unit_hot_water_start_yr, unit_hot_water_end_yr]) popt, pcov = curve_fit(func, x, y) unit_hot_water = func(int(planning_horizons), *popt) elif snakemake.wildcards[\"heating_demand\"] == \"constant\": unit_hot_water = unit_hot_water_start_yr elif snakemake.wildcards[\"heating_demand\"] == \"mean\": def lin_func(x: np.array, a: float, b: float) -> np.array: return a * x + b x = np.array([START_YEAR, END_YEAR]) y = np.array([unit_hot_water_start_yr, unit_hot_water_end_yr]) popt, pcov = curve_fit(lin_func, x, y) popt, pcov = curve_fit(lin_func, x, y) unit_hot_water = (lin_func(int(planning_horizons), *popt) + UNIT_HOT_WATER_START_YEAR) / 2 else: raise ValueError(f\"Invalid heating demand type {snakemake.wildcards['heating_demand']}\") # MWh per day per region hot_water_per_day = unit_hot_water * population_count / 365.0 return hot_water_per_day","title":"build_hot_water_per_day"},{"location":"docs/reference/build_load_profiles/#build_load_profiles.downscale_time_data","text":"Make hourly resolved data profiles based on exogenous weekdays and weekend profiles. This fn takes into account that the profiles are in local time and that regions may have different timezones. Parameters: dt_index ( DatetimeIndex ) \u2013 the snapshots (in network local naive time) but hourly res. weekly_profile ( Iterable ) \u2013 the weekly profile as a list of 7*24 entries. regional_tzs ( Series ) \u2013 regional geographical timezones for profiles. Defaults to pd.Series(index=PROV_NAMES, data=list(REGIONAL_GEO_TIMEZONES.values())). Returns: DataFrame \u2013 pd.DataFrame: Regionally resolved profiles for each snapshot hour rperesented by dt_index Source code in workflow/scripts/build_load_profiles.py def downscale_time_data( dt_index: pd.DatetimeIndex, weekly_profile: Iterable, regional_tzs: pd.Series, ) -> pd.DataFrame: \"\"\"Make hourly resolved data profiles based on exogenous weekdays and weekend profiles. This fn takes into account that the profiles are in local time and that regions may have different timezones. Args: dt_index (DatetimeIndex): the snapshots (in network local naive time) but hourly res. weekly_profile (Iterable): the weekly profile as a list of 7*24 entries. regional_tzs (pd.Series, optional): regional geographical timezones for profiles. Defaults to pd.Series(index=PROV_NAMES, data=list(REGIONAL_GEO_TIMEZONES.values())). Returns: pd.DataFrame: Regionally resolved profiles for each snapshot hour rperesented by dt_index \"\"\" weekly_profile = pd.Series(weekly_profile, range(24 * 7)) # make a dataframe with timestamps localized to the network TIMEZONE timestamps all_times = pd.DataFrame( dict(zip(PROV_NAMES, [dt_index.tz_localize(TIMEZONE)] * len(PROV_NAMES))), index=dt_index.tz_localize(TIMEZONE), columns=PROV_NAMES, ) # then localize to regional time. _dt ensures index is not changed week_hours = all_times.apply( lambda col: col.dt.tz_convert(regional_tzs[col.name]).tz_localize(None) ) # then convert into week hour & map to the intraday heat demand profile (based on local time) return week_hours.apply(lambda col: col.dt.weekday * 24 + col.dt.hour).apply( lambda col: col.map(weekly_profile) )","title":"downscale_time_data"},{"location":"docs/reference/build_load_profiles/#build_load_profiles.make_heat_demand_projections","text":"Make projections for heating demand Parameters: projection_name ( str ) \u2013 name of projection planning_year ( int ) \u2013 year to project to ref_year ( int , default: REF_YEAR ) \u2013 reference year. Defaults to REF_YEAR. Returns: float ( float ) \u2013 scaling factor relative to base year for heating demand Source code in workflow/scripts/build_load_profiles.py def make_heat_demand_projections( planning_year: int, projection_name: str, ref_year=REF_YEAR ) -> float: \"\"\"Make projections for heating demand Args: projection_name (str): name of projection planning_year (int): year to project to ref_year (int, optional): reference year. Defaults to REF_YEAR. Returns: float: scaling factor relative to base year for heating demand \"\"\" years = np.array([1985, 1990, 1995, 2000, 2005, 2010, 2015, 2020, 2060]) if projection_name == \"positive\": def func(x, a, b, c, d): \"\"\"Cubic polynomial fit to proj\"\"\" return a * x**3 + b * x**2 + c * x + d # TODO soft code # heated area projection in China # 2060: 6.02 * 36.52 * 1e4 north population * floor_space_per_capita in city heated_area = np.array( [2742, 21263, 64645, 110766, 252056, 435668, 672205, 988209, 2198504] ) # 10000 m2 # Perform curve fitting popt, pcov = curve_fit(func, years, heated_area) factor = func(int(planning_year), *popt) / func(REF_YEAR, *popt) elif projection_name == \"constant\": factor = 1.0 else: raise ValueError(f\"Invalid heating demand projection {projection_name}\") return factor","title":"make_heat_demand_projections"},{"location":"docs/reference/build_load_profiles/#build_load_profiles.prepare_hourly_load_data","text":"Read the hourly demand data and prepare it for use in the model Parameters: hourly_load_p ( PathLike ) \u2013 raw elec data from zenodo, see readme in data. prov_codes_p ( PathLike ) \u2013 province mapping for data. Returns: DataFrame \u2013 pd.DataFrame: the hourly demand data with the right province names, in TWh/hr Source code in workflow/scripts/build_load_profiles.py def prepare_hourly_load_data( hourly_load_p: os.PathLike, prov_codes_p: os.PathLike, ) -> pd.DataFrame: \"\"\"Read the hourly demand data and prepare it for use in the model Args: hourly_load_p (os.PathLike, optional): raw elec data from zenodo, see readme in data. prov_codes_p (os.PathLike, optional): province mapping for data. Returns: pd.DataFrame: the hourly demand data with the right province names, in TWh/hr \"\"\" TO_TWh = 1e-6 hourly = pd.read_csv(hourly_load_p) hourly_TWh = hourly.drop(columns=[\"Time Series\"]) * TO_TWh prov_codes = pd.read_csv(prov_codes_p) prov_codes.set_index(\"Code\", inplace=True) hourly_TWh.columns = hourly_TWh.columns.map(prov_codes[\"Full name\"]) return hourly_TWh","title":"prepare_hourly_load_data"},{"location":"docs/reference/build_load_profiles/#build_load_profiles.project_elec_demand","text":"project the hourly demand to the future years Parameters: hourly_demand_base_yr_MWh ( DataFrame ) \u2013 the hourly demand in the base year yearly_projections_TWh ( DataFrame ) \u2013 the yearly projections Returns: \u2013 pd.DataFrame: the projected hourly demand Source code in workflow/scripts/build_load_profiles.py def project_elec_demand( hourly_demand_base_yr_MWh: pd.DataFrame, yearly_projections_TWh: pd.DataFrame, year=2020 ): \"\"\"project the hourly demand to the future years Args: hourly_demand_base_yr_MWh (pd.DataFrame): the hourly demand in the base year yearly_projections_TWh (pd.DataFrame): the yearly projections Returns: pd.DataFrame: the projected hourly demand \"\"\" hourly_load_TWH_hr = hourly_demand_base_yr_MWh.loc[:, PROV_NAMES] # normalise the hourly load hourly_load_TWH_hr /= hourly_load_TWH_hr.sum(axis=0) yearly_projections_TWh = yearly_projections_TWh.T.loc[int(year), PROV_NAMES] hourly_load_projected = yearly_projections_TWh.multiply(hourly_load_TWH_hr) if len(hourly_load_projected) == 8784: # rm feb 29th hourly_load_projected.drop(hourly_load_projected.index[1416:1440], inplace=True) elif len(hourly_load_projected) != 8760: raise ValueError(\"The length of the hourly load is not 8760 or 8784 (leap year, dropped)\") snapshots = make_periodic_snapshots( year=year, freq=\"1h\", start_day_hour=\"01-01 00:00:00\", end_day_hour=\"12-31 23:00\", bounds=\"both\", ) hourly_load_projected.index = snapshots return hourly_load_projected","title":"project_elec_demand"},{"location":"docs/reference/build_load_profiles/#build_load_profiles.read_yearly_projections","text":"prepare projections for model use Parameters: yearly_projections_p ( PathLike , default: 'resources/data/load/Province_Load_2020_2060.csv' ) \u2013 the data path. Defaults to \"resources/data/load/Province_Load_2020_2060.csv\". Returns: DataFrame \u2013 pd.DataFrame: the formatted data Source code in workflow/scripts/build_load_profiles.py def read_yearly_projections( yearly_projections_p: os.PathLike = \"resources/data/load/Province_Load_2020_2060.csv\", ) -> pd.DataFrame: \"\"\"prepare projections for model use Args: yearly_projections_p (os.PathLike, optional): the data path. Defaults to \"resources/data/load/Province_Load_2020_2060.csv\". Returns: pd.DataFrame: the formatted data \"\"\" TO_TWh = 1 / 10 yearly_proj_TWh = pd.read_csv(yearly_projections_p) yearly_proj_TWh.rename(columns={\"Unnamed: 0\": \"province\"}, inplace=True) yearly_proj_TWh.set_index(\"province\", inplace=True) yearly_proj_TWh.rename(columns={c: int(c) for c in yearly_proj_TWh.columns}, inplace=True) return yearly_proj_TWh * TO_TWh","title":"read_yearly_projections"},{"location":"docs/reference/build_population/","text":"Rules for building the population data by region build_population(data_path=None) Build the population data by region Parameters: data_path ( PathLike , default: None ) \u2013 the path to the pop csv. Defaults to None. Source code in workflow/scripts/build_population.py def build_population(data_path: os.PathLike = None): \"\"\"Build the population data by region Args: data_path (os.PathLike, optional): the path to the pop csv. Defaults to None. \"\"\" if data_path is None: data_path = snakemake.input.population population = YEARBOOK_DATA2POP * load_pop_csv(csv_path=data_path) population.name = \"population\" population.to_hdf(snakemake.output.population, key=population.name) load_pop_csv(csv_path) Load the national bureau of statistics of China population (Yearbook - Population, table 2.5 pop at year end by Region) Parameters: csv_path ( Pathlike ) \u2013 the csv path Returns: DataFrame \u2013 pd.DataFrame: the population for constants.POP_YEAR by province Raises: ValueError: if the province names are not as expected Source code in workflow/scripts/build_population.py def load_pop_csv(csv_path: os.PathLike) -> pd.DataFrame: \"\"\"Load the national bureau of statistics of China population (Yearbook - Population, table 2.5 pop at year end by Region) Args: csv_path (os.Pathlike): the csv path Returns: pd.DataFrame: the population for constants.POP_YEAR by province Raises: ValueError: if the province names are not as expected \"\"\" df = pd.read_csv(csv_path, index_col=0, header=0) df = df.apply(pd.to_numeric) df = df[POP_YEAR][df.index.isin(PROV_NAMES)] if not sorted(df.index.to_list()) == sorted(PROV_NAMES): raise ValueError( f\"Province names do not match {sorted(df.index.to_list())} != {sorted(PROV_NAMES)}\" ) return df","title":"build_population"},{"location":"docs/reference/build_population/#build_population.build_population","text":"Build the population data by region Parameters: data_path ( PathLike , default: None ) \u2013 the path to the pop csv. Defaults to None. Source code in workflow/scripts/build_population.py def build_population(data_path: os.PathLike = None): \"\"\"Build the population data by region Args: data_path (os.PathLike, optional): the path to the pop csv. Defaults to None. \"\"\" if data_path is None: data_path = snakemake.input.population population = YEARBOOK_DATA2POP * load_pop_csv(csv_path=data_path) population.name = \"population\" population.to_hdf(snakemake.output.population, key=population.name)","title":"build_population"},{"location":"docs/reference/build_population/#build_population.load_pop_csv","text":"Load the national bureau of statistics of China population (Yearbook - Population, table 2.5 pop at year end by Region) Parameters: csv_path ( Pathlike ) \u2013 the csv path Returns: DataFrame \u2013 pd.DataFrame: the population for constants.POP_YEAR by province Raises: ValueError: if the province names are not as expected Source code in workflow/scripts/build_population.py def load_pop_csv(csv_path: os.PathLike) -> pd.DataFrame: \"\"\"Load the national bureau of statistics of China population (Yearbook - Population, table 2.5 pop at year end by Region) Args: csv_path (os.Pathlike): the csv path Returns: pd.DataFrame: the population for constants.POP_YEAR by province Raises: ValueError: if the province names are not as expected \"\"\" df = pd.read_csv(csv_path, index_col=0, header=0) df = df.apply(pd.to_numeric) df = df[POP_YEAR][df.index.isin(PROV_NAMES)] if not sorted(df.index.to_list()) == sorted(PROV_NAMES): raise ValueError( f\"Province names do not match {sorted(df.index.to_list())} != {sorted(PROV_NAMES)}\" ) return df","title":"load_pop_csv"},{"location":"docs/reference/build_population_gridcell_map/","text":"build_gridded_population(prov_pop_path, pop_density_raster_path, cutout_path, province_shape_path, gridded_pop_out) Build a gridded population DataFrame by matching population density to the cutout grid cells. This DataFrame is a sparse matrix of the population and shape BusesxCutout_gridcells where buses are the provinces Parameters: prov_pop_path ( PathLike ) \u2013 Path to the province population count file (hdf5). pop_density_raster_path ( PathLike ) \u2013 Path to the population density raster file. cutout_path ( PathLike ) \u2013 Path to the cutout file containing the grid. province_shape_path ( PathLike ) \u2013 Path to the province shape file. gridded_pop_out ( PathLike ) \u2013 output file path. Source code in workflow/scripts/build_population_gridcell_map.py def build_gridded_population( prov_pop_path: PathLike, pop_density_raster_path: PathLike, cutout_path: PathLike, province_shape_path: PathLike, gridded_pop_out: PathLike, ): \"\"\"Build a gridded population DataFrame by matching population density to the cutout grid cells. This DataFrame is a sparse matrix of the population and shape BusesxCutout_gridcells where buses are the provinces Args: prov_pop_path (PathLike): Path to the province population count file (hdf5). pop_density_raster_path (PathLike): Path to the population density raster file. cutout_path (PathLike): Path to the cutout file containing the grid. province_shape_path (PathLike): Path to the province shape file. gridded_pop_out (PathLike): output file path. \"\"\" with pd.HDFStore(prov_pop_path, mode=\"r\") as store: pop_province = store[\"population\"] prov_poly = read_province_shapes(province_shape_path) pop_density = read_pop_density(pop_density_raster_path, prov_poly, crs=CRS) cutout = atlite.Cutout(cutout_path) grid_points = cutout.grid # this is in polygons but need points for sjoin with pop dnesity to work grid_points.to_crs(3857, inplace=True) grid_points[\"geometry\"] = grid_points.centroid grid_points.to_crs(CRS, inplace=True) # match cutout grid to province # cutout_pts_in_prov = gpd.tools.sjoin(grid_points, prov_poly, # how=\"left\", predicate=\"intersects\") # TODO: do you want to dropna here? cutout_pts_in_prov = gpd.tools.sjoin( grid_points, prov_poly, how=\"left\", predicate=\"intersects\" ) # .dropna() cutout_pts_in_prov.rename( columns={\"index_right\": \"province_index\", \"province\": \"province_name\"}, inplace=True ) # match cutout grid to province cutout_pts_in_prov = gpd.tools.sjoin(grid_points, prov_poly, how=\"left\", predicate=\"intersects\") cutout_pts_in_prov.rename( columns={\"index_right\": \"province_index\", \"province\": \"province_name\"}, inplace=True ) # cutout_pts_in_prov.dropna(inplace=True) # TODO CRS, think about whether this makes sense or need grid interp merged = gpd.tools.sjoin_nearest( cutout_pts_in_prov.to_crs(3857), pop_density.to_crs(3857), how=\"inner\" ) merged = merged.to_crs(CRS) # points outside china are NaN, need to rename to keep the index cutout after agg # otherwise the spare matrix will not match the cutoutpoints # (smarter would be to change the cutout) merged.fillna({\"province_name\": \"OutsideChina\"}, inplace=True) points_in_provinces = pd.DataFrame(index=cutout_pts_in_prov.index) # normalise pop per province and make a loc_id/province table points_in_provinces = ( merged.groupby(\"province_name\")[\"pop_density\"] .apply(lambda x: x / x.sum()) .unstack(fill_value=0.0) .T ) # now get rid of the outside china \"province\" points_in_provinces.drop(columns=\"OutsideChina\", inplace=True) points_in_provinces.index.name = \"\" points_in_provinces.fillna(0.0, inplace=True) points_in_provinces *= pop_province with pd.HDFStore(gridded_pop_out, mode=\"w\", complevel=4) as store: store[\"population_gridcell_map\"] = points_in_provinces build_population_map(prov_pop_path, pop_density_raster_path, cutout_path, province_shape_path, gridded_pop_out) Build a gridded population DataFrame by matching population density to the cutout grid cells. This DataFrame is a sparse matrix of the population and shape BusesxCutout_gridcells where buses are the provinces Parameters: prov_pop_path ( PathLike ) \u2013 Path to the province population count file (hdf5). pop_density_raster_path ( PathLike ) \u2013 Path to the population density raster file. cutout_path ( PathLike ) \u2013 Path to the cutout file containing the grid. province_shape_path ( PathLike ) \u2013 Path to the province shape file. gridded_pop_out ( PathLike ) \u2013 output file path. Source code in workflow/scripts/build_population_gridcell_map.py def build_population_map( prov_pop_path: PathLike, pop_density_raster_path: PathLike, cutout_path: PathLike, province_shape_path: PathLike, gridded_pop_out: PathLike, ): \"\"\"Build a gridded population DataFrame by matching population density to the cutout grid cells. This DataFrame is a sparse matrix of the population and shape BusesxCutout_gridcells where buses are the provinces Args: prov_pop_path (PathLike): Path to the province population count file (hdf5). pop_density_raster_path (PathLike): Path to the population density raster file. cutout_path (PathLike): Path to the cutout file containing the grid. province_shape_path (PathLike): Path to the province shape file. gridded_pop_out (PathLike): output file path. \"\"\" # =============== load data =================== with pd.HDFStore(prov_pop_path, mode=\"r\") as store: pop_province_count = store[\"population\"] # CFSR points and Provinces pop_ww = load_cfrs_data(pop_density_raster_path) prov_poly = gpd.read_file(province_shape_path)[[\"province\", \"geometry\"]] prov_poly.set_index(\"province\", inplace=True) prov_poly = prov_poly.reindex(PROV_NAMES) prov_poly.reset_index(inplace=True) # load renewable profiles & grid & extract gridpoints cutout = atlite.Cutout(cutout_path) grid_points = cutout.grid grid_points.to_crs(3857, inplace=True) grid_points[\"geometry\"] = grid_points.centroid grid_points.to_crs(CRS, inplace=True) # match cutout grid to province cutout_pts_in_prov = gpd.tools.sjoin(grid_points, prov_poly, how=\"left\", predicate=\"intersects\") cutout_pts_in_prov.rename( columns={\"index_right\": \"province_index\", \"province\": \"province_name\"}, inplace=True ) # Province masks merged with population density # TODO: THIS REQUIRES EXPLANATION - can't just use random crs :|| cutout_pts_in_prov = cutout_pts_in_prov.to_crs(3857) pop_ww = pop_ww.to_crs(3857) merged = gpd.tools.sjoin_nearest(cutout_pts_in_prov, pop_ww, how=\"inner\") merged = merged.to_crs(CRS) # normalised pop distribution per province # need an extra province for points not in the province, otherwise lose cutout grid index merged.fillna({\"province_name\": \"OutsideChina\"}, inplace=True) points_in_provinces = pd.DataFrame(index=cutout_pts_in_prov.index) points_in_provinces = ( merged.groupby(\"province_name\")[\"pop_density\"] .apply(lambda x: x / x.sum()) .unstack(fill_value=0.0) .T ) # Cleanup the matrix: get rid of the outside china \"province\" et points_in_provinces.drop(columns=\"OutsideChina\", inplace=True) points_in_provinces.index.name = \"\" points_in_provinces.fillna(0.0, inplace=True) # go from normalised distribution to head count points_in_provinces *= pop_province_count with pd.HDFStore(gridded_pop_out, mode=\"w\", complevel=4) as store: store[\"population_gridcell_map\"] = points_in_provinces load_cfrs_data(target) load CFRS_grid.nc type files into a geodatafram Parameters: target ( PathLike ) \u2013 the abs path Returns: GeoDataFrame \u2013 gpd.GeoDataFrame: the data in gdf Source code in workflow/scripts/build_population_gridcell_map.py def load_cfrs_data(target: PathLike) -> gpd.GeoDataFrame: \"\"\"load CFRS_grid.nc type files into a geodatafram Args: target (PathLike): the abs path Returns: gpd.GeoDataFrame: the data in gdf \"\"\" pop_density = xr.open_dataarray(target).to_dataset(name=\"pop_density\") pop_ww = xarr_to_gdf(pop_density, var_name=\"pop_density\") # TODO is the CRS correct? return pop_ww xarr_to_gdf(xarr, var_name, x_var='x', y_var='y', crs=CRS) convert an xarray to GDF Parameters: xarr ( DataArray ) \u2013 the input array var_name ( str ) \u2013 the array variable to be converted. x_var ( str , default: 'x' ) \u2013 the x dimension. Defaults to \"x\". y_var ( str , default: 'y' ) \u2013 the y dimension. Defaults to \"y\". crs ( _type_ , default: CRS ) \u2013 the crs. Defaults to CRS. Returns: GeoDataFrame \u2013 gpd.GeoDataFrame: geodata frame in chosen CRS Source code in workflow/scripts/build_population_gridcell_map.py def xarr_to_gdf( xarr: xr.DataArray, var_name: str, x_var=\"x\", y_var=\"y\", crs=CRS ) -> gpd.GeoDataFrame: \"\"\"convert an xarray to GDF Args: xarr (xr.DataArray): the input array var_name (str): the array variable to be converted. x_var (str, optional): the x dimension. Defaults to \"x\". y_var (str, optional): the y dimension. Defaults to \"y\". crs (_type_, optional): the crs. Defaults to CRS. Returns: gpd.GeoDataFrame: geodata frame in chosen CRS \"\"\" df = xarr.to_dataframe() df.reset_index(inplace=True) return gpd.GeoDataFrame( df[var_name], geometry=gpd.points_from_xy(df[x_var], df[y_var]), crs=crs )","title":"build_population_gridcell_map"},{"location":"docs/reference/build_population_gridcell_map/#build_population_gridcell_map.build_gridded_population","text":"Build a gridded population DataFrame by matching population density to the cutout grid cells. This DataFrame is a sparse matrix of the population and shape BusesxCutout_gridcells where buses are the provinces Parameters: prov_pop_path ( PathLike ) \u2013 Path to the province population count file (hdf5). pop_density_raster_path ( PathLike ) \u2013 Path to the population density raster file. cutout_path ( PathLike ) \u2013 Path to the cutout file containing the grid. province_shape_path ( PathLike ) \u2013 Path to the province shape file. gridded_pop_out ( PathLike ) \u2013 output file path. Source code in workflow/scripts/build_population_gridcell_map.py def build_gridded_population( prov_pop_path: PathLike, pop_density_raster_path: PathLike, cutout_path: PathLike, province_shape_path: PathLike, gridded_pop_out: PathLike, ): \"\"\"Build a gridded population DataFrame by matching population density to the cutout grid cells. This DataFrame is a sparse matrix of the population and shape BusesxCutout_gridcells where buses are the provinces Args: prov_pop_path (PathLike): Path to the province population count file (hdf5). pop_density_raster_path (PathLike): Path to the population density raster file. cutout_path (PathLike): Path to the cutout file containing the grid. province_shape_path (PathLike): Path to the province shape file. gridded_pop_out (PathLike): output file path. \"\"\" with pd.HDFStore(prov_pop_path, mode=\"r\") as store: pop_province = store[\"population\"] prov_poly = read_province_shapes(province_shape_path) pop_density = read_pop_density(pop_density_raster_path, prov_poly, crs=CRS) cutout = atlite.Cutout(cutout_path) grid_points = cutout.grid # this is in polygons but need points for sjoin with pop dnesity to work grid_points.to_crs(3857, inplace=True) grid_points[\"geometry\"] = grid_points.centroid grid_points.to_crs(CRS, inplace=True) # match cutout grid to province # cutout_pts_in_prov = gpd.tools.sjoin(grid_points, prov_poly, # how=\"left\", predicate=\"intersects\") # TODO: do you want to dropna here? cutout_pts_in_prov = gpd.tools.sjoin( grid_points, prov_poly, how=\"left\", predicate=\"intersects\" ) # .dropna() cutout_pts_in_prov.rename( columns={\"index_right\": \"province_index\", \"province\": \"province_name\"}, inplace=True ) # match cutout grid to province cutout_pts_in_prov = gpd.tools.sjoin(grid_points, prov_poly, how=\"left\", predicate=\"intersects\") cutout_pts_in_prov.rename( columns={\"index_right\": \"province_index\", \"province\": \"province_name\"}, inplace=True ) # cutout_pts_in_prov.dropna(inplace=True) # TODO CRS, think about whether this makes sense or need grid interp merged = gpd.tools.sjoin_nearest( cutout_pts_in_prov.to_crs(3857), pop_density.to_crs(3857), how=\"inner\" ) merged = merged.to_crs(CRS) # points outside china are NaN, need to rename to keep the index cutout after agg # otherwise the spare matrix will not match the cutoutpoints # (smarter would be to change the cutout) merged.fillna({\"province_name\": \"OutsideChina\"}, inplace=True) points_in_provinces = pd.DataFrame(index=cutout_pts_in_prov.index) # normalise pop per province and make a loc_id/province table points_in_provinces = ( merged.groupby(\"province_name\")[\"pop_density\"] .apply(lambda x: x / x.sum()) .unstack(fill_value=0.0) .T ) # now get rid of the outside china \"province\" points_in_provinces.drop(columns=\"OutsideChina\", inplace=True) points_in_provinces.index.name = \"\" points_in_provinces.fillna(0.0, inplace=True) points_in_provinces *= pop_province with pd.HDFStore(gridded_pop_out, mode=\"w\", complevel=4) as store: store[\"population_gridcell_map\"] = points_in_provinces","title":"build_gridded_population"},{"location":"docs/reference/build_population_gridcell_map/#build_population_gridcell_map.build_population_map","text":"Build a gridded population DataFrame by matching population density to the cutout grid cells. This DataFrame is a sparse matrix of the population and shape BusesxCutout_gridcells where buses are the provinces Parameters: prov_pop_path ( PathLike ) \u2013 Path to the province population count file (hdf5). pop_density_raster_path ( PathLike ) \u2013 Path to the population density raster file. cutout_path ( PathLike ) \u2013 Path to the cutout file containing the grid. province_shape_path ( PathLike ) \u2013 Path to the province shape file. gridded_pop_out ( PathLike ) \u2013 output file path. Source code in workflow/scripts/build_population_gridcell_map.py def build_population_map( prov_pop_path: PathLike, pop_density_raster_path: PathLike, cutout_path: PathLike, province_shape_path: PathLike, gridded_pop_out: PathLike, ): \"\"\"Build a gridded population DataFrame by matching population density to the cutout grid cells. This DataFrame is a sparse matrix of the population and shape BusesxCutout_gridcells where buses are the provinces Args: prov_pop_path (PathLike): Path to the province population count file (hdf5). pop_density_raster_path (PathLike): Path to the population density raster file. cutout_path (PathLike): Path to the cutout file containing the grid. province_shape_path (PathLike): Path to the province shape file. gridded_pop_out (PathLike): output file path. \"\"\" # =============== load data =================== with pd.HDFStore(prov_pop_path, mode=\"r\") as store: pop_province_count = store[\"population\"] # CFSR points and Provinces pop_ww = load_cfrs_data(pop_density_raster_path) prov_poly = gpd.read_file(province_shape_path)[[\"province\", \"geometry\"]] prov_poly.set_index(\"province\", inplace=True) prov_poly = prov_poly.reindex(PROV_NAMES) prov_poly.reset_index(inplace=True) # load renewable profiles & grid & extract gridpoints cutout = atlite.Cutout(cutout_path) grid_points = cutout.grid grid_points.to_crs(3857, inplace=True) grid_points[\"geometry\"] = grid_points.centroid grid_points.to_crs(CRS, inplace=True) # match cutout grid to province cutout_pts_in_prov = gpd.tools.sjoin(grid_points, prov_poly, how=\"left\", predicate=\"intersects\") cutout_pts_in_prov.rename( columns={\"index_right\": \"province_index\", \"province\": \"province_name\"}, inplace=True ) # Province masks merged with population density # TODO: THIS REQUIRES EXPLANATION - can't just use random crs :|| cutout_pts_in_prov = cutout_pts_in_prov.to_crs(3857) pop_ww = pop_ww.to_crs(3857) merged = gpd.tools.sjoin_nearest(cutout_pts_in_prov, pop_ww, how=\"inner\") merged = merged.to_crs(CRS) # normalised pop distribution per province # need an extra province for points not in the province, otherwise lose cutout grid index merged.fillna({\"province_name\": \"OutsideChina\"}, inplace=True) points_in_provinces = pd.DataFrame(index=cutout_pts_in_prov.index) points_in_provinces = ( merged.groupby(\"province_name\")[\"pop_density\"] .apply(lambda x: x / x.sum()) .unstack(fill_value=0.0) .T ) # Cleanup the matrix: get rid of the outside china \"province\" et points_in_provinces.drop(columns=\"OutsideChina\", inplace=True) points_in_provinces.index.name = \"\" points_in_provinces.fillna(0.0, inplace=True) # go from normalised distribution to head count points_in_provinces *= pop_province_count with pd.HDFStore(gridded_pop_out, mode=\"w\", complevel=4) as store: store[\"population_gridcell_map\"] = points_in_provinces","title":"build_population_map"},{"location":"docs/reference/build_population_gridcell_map/#build_population_gridcell_map.load_cfrs_data","text":"load CFRS_grid.nc type files into a geodatafram Parameters: target ( PathLike ) \u2013 the abs path Returns: GeoDataFrame \u2013 gpd.GeoDataFrame: the data in gdf Source code in workflow/scripts/build_population_gridcell_map.py def load_cfrs_data(target: PathLike) -> gpd.GeoDataFrame: \"\"\"load CFRS_grid.nc type files into a geodatafram Args: target (PathLike): the abs path Returns: gpd.GeoDataFrame: the data in gdf \"\"\" pop_density = xr.open_dataarray(target).to_dataset(name=\"pop_density\") pop_ww = xarr_to_gdf(pop_density, var_name=\"pop_density\") # TODO is the CRS correct? return pop_ww","title":"load_cfrs_data"},{"location":"docs/reference/build_population_gridcell_map/#build_population_gridcell_map.xarr_to_gdf","text":"convert an xarray to GDF Parameters: xarr ( DataArray ) \u2013 the input array var_name ( str ) \u2013 the array variable to be converted. x_var ( str , default: 'x' ) \u2013 the x dimension. Defaults to \"x\". y_var ( str , default: 'y' ) \u2013 the y dimension. Defaults to \"y\". crs ( _type_ , default: CRS ) \u2013 the crs. Defaults to CRS. Returns: GeoDataFrame \u2013 gpd.GeoDataFrame: geodata frame in chosen CRS Source code in workflow/scripts/build_population_gridcell_map.py def xarr_to_gdf( xarr: xr.DataArray, var_name: str, x_var=\"x\", y_var=\"y\", crs=CRS ) -> gpd.GeoDataFrame: \"\"\"convert an xarray to GDF Args: xarr (xr.DataArray): the input array var_name (str): the array variable to be converted. x_var (str, optional): the x dimension. Defaults to \"x\". y_var (str, optional): the y dimension. Defaults to \"y\". crs (_type_, optional): the crs. Defaults to CRS. Returns: gpd.GeoDataFrame: geodata frame in chosen CRS \"\"\" df = xarr.to_dataframe() df.reset_index(inplace=True) return gpd.GeoDataFrame( df[var_name], geometry=gpd.points_from_xy(df[x_var], df[y_var]), crs=crs )","title":"xarr_to_gdf"},{"location":"docs/reference/build_province_shapes/","text":"Functions to get the province shapes. fetch_natural_earth_records(country_iso2_code='CN') fetch the province/state level (1st admin level) from the NATURAL_EARTH data store and make a file Parameters: country_iso2_code ( str , default: 'CN' ) \u2013 the country code (iso_a2) for which provincial records will be extracted. None will not filter (untestetd) Defaults to 'CN' Returns: Records: the natural earth records Source code in workflow/scripts/build_province_shapes.py def fetch_natural_earth_records(country_iso2_code=\"CN\") -> object: \"\"\"fetch the province/state level (1st admin level) from the NATURAL_EARTH data store and make a file Args: country_iso2_code (str, optional): the country code (iso_a2) for which provincial records will be extracted. None will not filter (untestetd) Defaults to 'CN' Returns: Records: the natural earth records \"\"\" shpfilename = shpreader.natural_earth( resolution=NATURAL_EARTH_RESOLUTION, category=\"cultural\", name=NATURAL_EARTH_DATA_SET ) reader = shpreader.Reader(shpfilename) logger.info(\"Succesfully downloaded natural earth shapefiles\") provinces_states = reader.records() def filter_country_code(records: object, target_iso_a2_code=\"CN\") -> list: \"\"\"filter provincial/state (admin level 1) records for one country Args: records (shpreader.Reader.records): the records object from cartopy shpreader for natural earth dataset target_iso_a2_code (str, optional): the country code (iso_a2) for which provincial records will be extracted. Defaults to 'CN'. Returns: list: records list \"\"\" results = [] for rec in records: if rec.attributes[\"iso_a2\"] == target_iso_a2_code: results.append(rec) return results # TODO test with none if country_iso2_code is not None: provinces_states = filter_country_code( provinces_states, target_iso_a2_code=country_iso2_code ) return provinces_states records_to_data_frame(records) dump irrelevant info and make records into a GeoDataFrame that matches the PROV_NAMES Parameters: records ( object ) \u2013 the cartopy shpread records from natural earth Returns: GeoDataFrame \u2013 gpd.GeoDataFrame: the cleaned up & sorted data in a format that can be saved Source code in workflow/scripts/build_province_shapes.py def records_to_data_frame(records: object) -> gpd.GeoDataFrame: \"\"\"dump irrelevant info and make records into a GeoDataFrame that matches the PROV_NAMES Args: records (object): the cartopy shpread records from natural earth Returns: gpd.GeoDataFrame: the cleaned up & sorted data in a format that can be saved \"\"\" records[0].attributes[\"name\"] d = {\"province\": [r.attributes[\"name_en\"] for r in records]} geo = [r.geometry for r in records] gdf = gpd.GeoDataFrame(d, geometry=geo) gdf.sort_values(by=\"province\", inplace=True) # remove white spaces gdf[\"province\"] = gdf.province.str.replace(\" \", \"\") filtered = gdf[gdf.province.isin(PROV_NAMES)] if not filtered.province.to_list() == sorted(PROV_NAMES): raise ValueError( \"Built cut-out does not have the right provinces\" + \"- do your province lists have white spaces?\" ) return filtered save_province_data(provinces_gdf, crs=CRS, output_file=DEFAULT_SHAPE_OUTPATH) save to file Parameters: provinces_gdf ( GeoDataFrame ) \u2013 the cleaned up province records crs ( int , default: CRS ) \u2013 the crs in epsg format. Defaults to CRS. output_file ( pathlike , default: DEFAULT_SHAPE_OUTPATH ) \u2013 the output path. defaults to DEFAULT_SHAPE_OUTPATH Source code in workflow/scripts/build_province_shapes.py def save_province_data( provinces_gdf: gpd.GeoDataFrame, crs: int = CRS, output_file: os.PathLike = DEFAULT_SHAPE_OUTPATH, ): \"\"\"save to file Args: provinces_gdf (GeoDataFrame): the cleaned up province records crs (int, optional): the crs in epsg format. Defaults to CRS. output_file (os.pathlike): the output path. defaults to DEFAULT_SHAPE_OUTPATH \"\"\" provinces_gdf.set_crs(epsg=crs, inplace=True) # WGS84 provinces_gdf.to_file(os.path.abspath(output_file))","title":"build_province_shapes"},{"location":"docs/reference/build_province_shapes/#build_province_shapes.fetch_natural_earth_records","text":"fetch the province/state level (1st admin level) from the NATURAL_EARTH data store and make a file Parameters: country_iso2_code ( str , default: 'CN' ) \u2013 the country code (iso_a2) for which provincial records will be extracted. None will not filter (untestetd) Defaults to 'CN' Returns: Records: the natural earth records Source code in workflow/scripts/build_province_shapes.py def fetch_natural_earth_records(country_iso2_code=\"CN\") -> object: \"\"\"fetch the province/state level (1st admin level) from the NATURAL_EARTH data store and make a file Args: country_iso2_code (str, optional): the country code (iso_a2) for which provincial records will be extracted. None will not filter (untestetd) Defaults to 'CN' Returns: Records: the natural earth records \"\"\" shpfilename = shpreader.natural_earth( resolution=NATURAL_EARTH_RESOLUTION, category=\"cultural\", name=NATURAL_EARTH_DATA_SET ) reader = shpreader.Reader(shpfilename) logger.info(\"Succesfully downloaded natural earth shapefiles\") provinces_states = reader.records() def filter_country_code(records: object, target_iso_a2_code=\"CN\") -> list: \"\"\"filter provincial/state (admin level 1) records for one country Args: records (shpreader.Reader.records): the records object from cartopy shpreader for natural earth dataset target_iso_a2_code (str, optional): the country code (iso_a2) for which provincial records will be extracted. Defaults to 'CN'. Returns: list: records list \"\"\" results = [] for rec in records: if rec.attributes[\"iso_a2\"] == target_iso_a2_code: results.append(rec) return results # TODO test with none if country_iso2_code is not None: provinces_states = filter_country_code( provinces_states, target_iso_a2_code=country_iso2_code ) return provinces_states","title":"fetch_natural_earth_records"},{"location":"docs/reference/build_province_shapes/#build_province_shapes.records_to_data_frame","text":"dump irrelevant info and make records into a GeoDataFrame that matches the PROV_NAMES Parameters: records ( object ) \u2013 the cartopy shpread records from natural earth Returns: GeoDataFrame \u2013 gpd.GeoDataFrame: the cleaned up & sorted data in a format that can be saved Source code in workflow/scripts/build_province_shapes.py def records_to_data_frame(records: object) -> gpd.GeoDataFrame: \"\"\"dump irrelevant info and make records into a GeoDataFrame that matches the PROV_NAMES Args: records (object): the cartopy shpread records from natural earth Returns: gpd.GeoDataFrame: the cleaned up & sorted data in a format that can be saved \"\"\" records[0].attributes[\"name\"] d = {\"province\": [r.attributes[\"name_en\"] for r in records]} geo = [r.geometry for r in records] gdf = gpd.GeoDataFrame(d, geometry=geo) gdf.sort_values(by=\"province\", inplace=True) # remove white spaces gdf[\"province\"] = gdf.province.str.replace(\" \", \"\") filtered = gdf[gdf.province.isin(PROV_NAMES)] if not filtered.province.to_list() == sorted(PROV_NAMES): raise ValueError( \"Built cut-out does not have the right provinces\" + \"- do your province lists have white spaces?\" ) return filtered","title":"records_to_data_frame"},{"location":"docs/reference/build_province_shapes/#build_province_shapes.save_province_data","text":"save to file Parameters: provinces_gdf ( GeoDataFrame ) \u2013 the cleaned up province records crs ( int , default: CRS ) \u2013 the crs in epsg format. Defaults to CRS. output_file ( pathlike , default: DEFAULT_SHAPE_OUTPATH ) \u2013 the output path. defaults to DEFAULT_SHAPE_OUTPATH Source code in workflow/scripts/build_province_shapes.py def save_province_data( provinces_gdf: gpd.GeoDataFrame, crs: int = CRS, output_file: os.PathLike = DEFAULT_SHAPE_OUTPATH, ): \"\"\"save to file Args: provinces_gdf (GeoDataFrame): the cleaned up province records crs (int, optional): the crs in epsg format. Defaults to CRS. output_file (os.pathlike): the output path. defaults to DEFAULT_SHAPE_OUTPATH \"\"\" provinces_gdf.set_crs(epsg=crs, inplace=True) # WGS84 provinces_gdf.to_file(os.path.abspath(output_file))","title":"save_province_data"},{"location":"docs/reference/build_renewable_potential/","text":"Functions associated with the build_renewable_potential rule. - Temporal Profiles are built based on the atlite cutout - Potentials are built based on the atlite cutout and raster data (land availability) make_offshore_wind_profile(offwind_config, cutout, outp_path) Make the offwind geographical potentials and per unit availability time series for each raster cell ! Somewhat compute intensive ! Parameters: offwind_config ( dict ) \u2013 the configuration for the offshore wind cutout ( Cutout ) \u2013 the atlite cutout outp_path ( PathLike ) \u2013 the output path for the raster date Source code in workflow/scripts/build_renewable_potential.py def make_offshore_wind_profile(offwind_config: dict, cutout: atlite.Cutout, outp_path: PathLike): \"\"\"Make the offwind geographical potentials and per unit availability time series for each raster cell ! Somewhat compute intensive ! Args: offwind_config (dict): the configuration for the offshore wind cutout (atlite.Cutout): the atlite cutout outp_path (PathLike): the output path for the raster date \"\"\" offwind_resource = offwind_config[\"resource\"] offwind_correction_factor = offwind_config.get( \"correction_factor\", DEFAULT_OFFSHORE_WIND_CORR_FACTOR ) offwind_capacity_per_sqkm = offwind_config[\"capacity_per_sqkm\"] if offwind_correction_factor != 1.0: logger.info(f\"offwind_correction_factor is set as {offwind_correction_factor}\") offwind_provinces = OFFSHORE_WIND_NODES EEZ_province_shp = gpd.read_file(snakemake.input[\"offshore_province_shapes\"]).set_index( \"province\" ) EEZ_province_shp = EEZ_province_shp.reindex(offwind_provinces).rename_axis(\"bus\") EEZ_country = gpd.GeoDataFrame( geometry=[EEZ_province_shp.unary_union], crs=EEZ_province_shp.crs, index=[\"country\"] ) excluder_offwind = ExclusionContainer(crs=3035, res=500) if \"max_depth\" in offwind_config: func = functools.partial(np.greater, -offwind_config[\"max_depth\"]) excluder_offwind.add_raster(snakemake.input.gebco, codes=func, crs=CRS, nodata=-1000) if offwind_config[\"natura\"]: protected_shp = gpd.read_file(snakemake.input[\"natura1\"]) protected_shp1 = gpd.read_file(snakemake.input[\"natura2\"]) protected_shp2 = gpd.read_file(snakemake.input[\"natura3\"]) protected_shp = pd.concat([protected_shp, protected_shp1], ignore_index=True) protected_shp = pd.concat([protected_shp, protected_shp2], ignore_index=True) protected_shp = protected_shp.geometry protected_shp = gpd.GeoDataFrame(protected_shp) protected_Marine_shp = gpd.tools.overlay(protected_shp, EEZ_country, how=\"intersection\") # this is to avoid atlite complaining about parallelisation if not os.path.isdir(os.path.dirname(TMP)): mkdir(os.path.dirname(TMP)) protected_Marine_shp.to_file(TMP) excluder_offwind.add_geometry(TMP) kwargs = dict(nprocesses=nprocesses, disable_progressbar=noprogress) if noprogress: logger.info(\"Calculate offwind landuse availabilities...\") start = time.time() offwind_matrix = cutout.availabilitymatrix(EEZ_province_shp, excluder_offwind, **kwargs) duration = time.time() - start logger.info(f\"Completed offwind availability calculation ({duration:2.2f}s)\") else: offwind_matrix = cutout.availabilitymatrix(EEZ_province_shp, excluder_offwind, **kwargs) offwind_potential = offwind_capacity_per_sqkm * offwind_matrix.sum(\"bus\") * area offwind_func = getattr(cutout, offwind_resource.pop(\"method\")) offwind_resource[\"dask_kwargs\"] = {\"num_workers\": nprocesses} # ? offwind_capacity_factor = offwind_correction_factor * offwind_func( capacity_factor=True, **offwind_resource ) offwind_layout = offwind_capacity_factor * area * offwind_capacity_per_sqkm offwind_profile, offwind_capacities = offwind_func( matrix=offwind_matrix.stack(spatial=[\"y\", \"x\"]), layout=offwind_layout, index=EEZ_province_shp.index, per_unit=True, return_capacity=True, **offwind_resource, ) logger.info(\"Calculating offwind maximal capacity per bus (method 'simple')\") offwind_p_nom_max = offwind_capacity_per_sqkm * offwind_matrix @ area offwind_ds = xr.merge( [ (offwind_correction_factor * offwind_profile).rename(\"profile\"), offwind_capacities.rename(\"weight\"), offwind_p_nom_max.rename(\"p_nom_max\"), offwind_potential.rename(\"potential\"), ] ) offwind_ds = offwind_ds.sel( bus=( (offwind_ds[\"profile\"].mean(\"time\") > offwind_config.get(\"min_p_max_pu\", 0.0)) & (offwind_ds[\"p_nom_max\"] > offwind_config.get(\"min_p_nom_max\", 0.0)) ) ) if \"clip_p_max_pu\" in offwind_config: min_p_max_pu = offwind_config[\"clip_p_max_pu\"] offwind_ds[\"profile\"] = offwind_ds[\"profile\"].where( offwind_ds[\"profile\"] >= min_p_max_pu, 0 ) # shift back from UTC to network time offwind_ds[\"time\"] = ( pd.DatetimeIndex(offwind_ds[\"time\"], tz=\"UTC\").tz_convert(TIMEZONE).tz_localize(None).values ) offwind_ds.to_netcdf(outp_path) make_onshore_wind_profile(onwind_config, cutout, outp_path) Make the onwind geographical potentials and per unit availability time series for each raster cell ! Somewhat compute intensive ! Parameters: onwind_config ( dict ) \u2013 the onshore wind config (from the yaml config read by snakemake) cutout ( Cutout ) \u2013 the atlite cutout outp_path ( PathLike ) \u2013 the output path for the raster data Source code in workflow/scripts/build_renewable_potential.py def make_onshore_wind_profile(onwind_config: dict, cutout: atlite.Cutout, outp_path: PathLike): \"\"\"Make the onwind geographical potentials and per unit availability time series for each raster cell ! Somewhat compute intensive ! Args: onwind_config (dict): the onshore wind config (from the yaml config read by snakemake) cutout (atlite.Cutout): the atlite cutout outp_path (PathLike): the output path for the raster data \"\"\" logger.info(\"Making onshore wind profile \") onwind_resource = onwind_config[\"resource\"] onwind_correction_factor = onwind_config.get(\"correction_factor\", 1.0) onwind_capacity_per_sqkm = onwind_config[\"capacity_per_sqkm\"] if onwind_correction_factor != 1.0: logger.info(f\"onwind_correction_factor is set as {onwind_correction_factor}\") excluder_onwind = ExclusionContainer(crs=3035, res=500) excluder_onwind.add_raster(grass, invert=True, crs=4326) excluder_onwind.add_raster(bare, invert=True, crs=4326) excluder_onwind.add_raster(shrubland, invert=True, crs=4326) kwargs = dict(nprocesses=nprocesses, disable_progressbar=noprogress) if noprogress: logger.info(\"Calculate onwind landuse availabilities...\") start = time.time() onwind_matrix = cutout.availabilitymatrix(provinces_shp, excluder_onwind, **kwargs) duration = time.time() - start logger.info(f\"Completed onwind availability calculation ({duration:2.2f}s)\") else: onwind_matrix = cutout.availabilitymatrix(provinces_shp, excluder_onwind, **kwargs) onwind_potential = onwind_capacity_per_sqkm * onwind_matrix.sum(\"bus\") * area onwind_func = getattr(cutout, onwind_resource.pop(\"method\")) onwind_resource[\"dask_kwargs\"] = {\"num_workers\": nprocesses} # ? onwind_capacity_factor = onwind_correction_factor * onwind_func( capacity_factor=True, **onwind_resource ) onwind_layout = onwind_capacity_factor * area * onwind_capacity_per_sqkm onwind_profile, onwind_capacities = onwind_func( matrix=onwind_matrix.stack(spatial=[\"y\", \"x\"]), layout=onwind_layout, index=buses, per_unit=True, return_capacity=True, **onwind_resource, ) logger.info(\"Calculating onwind maximal capacity per bus (method 'simple')\") onwind_p_nom_max = onwind_capacity_per_sqkm * onwind_matrix @ area onwind_ds = xr.merge( [ (onwind_correction_factor * onwind_profile).rename(\"profile\"), onwind_capacities.rename(\"weight\"), onwind_p_nom_max.rename(\"p_nom_max\"), onwind_potential.rename(\"potential\"), ] ) onwind_ds = onwind_ds.sel( bus=( (onwind_ds[\"profile\"].mean(\"time\") > onwind_config.get(\"min_p_max_pu\", 0.0)) & (onwind_ds[\"p_nom_max\"] > onwind_config.get(\"min_p_nom_max\", 0.0)) ) ) if \"clip_p_max_pu\" in onwind_config: min_p_max_pu = onwind_config[\"clip_p_max_pu\"] onwind_ds[\"profile\"] = onwind_ds[\"profile\"].where(onwind_ds[\"profile\"] >= min_p_max_pu, 0) # shift back from UTC to network time onwind_ds[\"time\"] = ( pd.DatetimeIndex(onwind_ds[\"time\"], tz=\"UTC\").tz_convert(TIMEZONE).tz_localize(None).values ) onwind_ds.to_netcdf(outp_path) make_solar_profile(solar_config, cutout, outp_path) Make the solar geographical potentials and per unit availability time series for each raster cell ! Somewhat compute intensive ! Parameters: solar_config ( dict ) \u2013 the solar configuration (from the yaml config read by snakemake) cutout ( Cutout ) \u2013 the atlite cutout outp_path ( PathLike ) \u2013 the output path for the raster data Source code in workflow/scripts/build_renewable_potential.py def make_solar_profile( solar_config: dict, cutout: atlite.Cutout, outp_path: PathLike, ): \"\"\"Make the solar geographical potentials and per unit availability time series for each raster cell ! Somewhat compute intensive ! Args: solar_config (dict): the solar configuration (from the yaml config read by snakemake) cutout (atlite.Cutout): the atlite cutout outp_path (PathLike): the output path for the raster data \"\"\" logger.info(\"Making solar profile \") solar_config = snakemake.config[\"renewable\"][\"solar\"] solar_resource = solar_config[\"resource\"] solar_correction_factor = solar_config.get(\"correction_factor\", 1.0) solar_capacity_per_sqkm = solar_config[\"capacity_per_sqkm\"] if solar_correction_factor != 1.0: logger.info(f\"solar_correction_factor is set as {solar_correction_factor}\") # TODO not hardcoded res excluder_solar = ExclusionContainer(crs=3035, res=500) excluder_build_up = ExclusionContainer(crs=3035, res=500) build_up = snakemake.input[\"Build_up_raster\"] excluder_build_up.add_raster(build_up, invert=True, crs=CRS) excluder_solar.add_raster(grass, invert=True, crs=CRS) excluder_solar.add_raster(bare, invert=True, crs=CRS) excluder_solar.add_raster(shrubland, invert=True, crs=CRS) kwargs = dict(nprocesses=nprocesses, disable_progressbar=noprogress) # TODO remove if else? if noprogress: logger.info(\"Calculate solar landuse availabilities...\") start = time.time() solar_matrix = cutout.availabilitymatrix(provinces_shp, excluder_solar, **kwargs) buildup_matrix = cutout.availabilitymatrix(provinces_shp, excluder_build_up, **kwargs) duration = time.time() - start logger.info(f\"Completed solar availability calculation ({duration:2.2f}s)\") else: solar_matrix = cutout.availabilitymatrix( shapes=provinces_shp, excluder=excluder_solar, **kwargs ) buildup_matrix = cutout.availabilitymatrix(provinces_shp, excluder_build_up, **kwargs) solar_potential = ( solar_capacity_per_sqkm * solar_matrix.sum(\"bus\") * area + solar_capacity_per_sqkm * buildup_matrix.sum(\"bus\") * area ) solar_func = getattr(cutout, solar_resource.pop(\"method\")) solar_resource[\"dask_kwargs\"] = {\"num_workers\": nprocesses} # ? solar_capacity_factor = solar_correction_factor * solar_func( capacity_factor=True, **solar_resource ) solar_layout = solar_capacity_factor * area * solar_capacity_per_sqkm solar_profile, solar_capacities = solar_func( matrix=solar_matrix.stack(spatial=[\"y\", \"x\"]), layout=solar_layout, index=buses, per_unit=True, return_capacity=True, **solar_resource, ) logger.info(\"Calculating solar maximal capacity per bus (method 'simple')\") solar_p_nom_max = solar_capacity_per_sqkm * solar_matrix @ area solar_ds = xr.merge( [ (solar_correction_factor * solar_profile).rename(\"profile\"), solar_capacities.rename(\"weight\"), solar_p_nom_max.rename(\"p_nom_max\"), solar_potential.rename(\"potential\"), ] ) solar_ds = solar_ds.sel( bus=( (solar_ds[\"profile\"].mean(\"time\") > solar_config.get(\"min_p_max_pu\", 0.0)) & (solar_ds[\"p_nom_max\"] > solar_config.get(\"min_p_nom_max\", 0.0)) ) ) if \"clip_p_max_pu\" in solar_config: min_p_max_pu = solar_config[\"clip_p_max_pu\"] solar_ds[\"profile\"] = solar_ds[\"profile\"].where(solar_ds[\"profile\"] >= min_p_max_pu, 0) # shift back from UTC to network time solar_ds[\"time\"] = ( pd.DatetimeIndex(solar_ds[\"time\"], tz=\"UTC\").tz_convert(TIMEZONE).tz_localize(None).values ) solar_ds.to_netcdf(outp_path)","title":"build_renewable_potential"},{"location":"docs/reference/build_renewable_potential/#build_renewable_potential.make_offshore_wind_profile","text":"Make the offwind geographical potentials and per unit availability time series for each raster cell ! Somewhat compute intensive ! Parameters: offwind_config ( dict ) \u2013 the configuration for the offshore wind cutout ( Cutout ) \u2013 the atlite cutout outp_path ( PathLike ) \u2013 the output path for the raster date Source code in workflow/scripts/build_renewable_potential.py def make_offshore_wind_profile(offwind_config: dict, cutout: atlite.Cutout, outp_path: PathLike): \"\"\"Make the offwind geographical potentials and per unit availability time series for each raster cell ! Somewhat compute intensive ! Args: offwind_config (dict): the configuration for the offshore wind cutout (atlite.Cutout): the atlite cutout outp_path (PathLike): the output path for the raster date \"\"\" offwind_resource = offwind_config[\"resource\"] offwind_correction_factor = offwind_config.get( \"correction_factor\", DEFAULT_OFFSHORE_WIND_CORR_FACTOR ) offwind_capacity_per_sqkm = offwind_config[\"capacity_per_sqkm\"] if offwind_correction_factor != 1.0: logger.info(f\"offwind_correction_factor is set as {offwind_correction_factor}\") offwind_provinces = OFFSHORE_WIND_NODES EEZ_province_shp = gpd.read_file(snakemake.input[\"offshore_province_shapes\"]).set_index( \"province\" ) EEZ_province_shp = EEZ_province_shp.reindex(offwind_provinces).rename_axis(\"bus\") EEZ_country = gpd.GeoDataFrame( geometry=[EEZ_province_shp.unary_union], crs=EEZ_province_shp.crs, index=[\"country\"] ) excluder_offwind = ExclusionContainer(crs=3035, res=500) if \"max_depth\" in offwind_config: func = functools.partial(np.greater, -offwind_config[\"max_depth\"]) excluder_offwind.add_raster(snakemake.input.gebco, codes=func, crs=CRS, nodata=-1000) if offwind_config[\"natura\"]: protected_shp = gpd.read_file(snakemake.input[\"natura1\"]) protected_shp1 = gpd.read_file(snakemake.input[\"natura2\"]) protected_shp2 = gpd.read_file(snakemake.input[\"natura3\"]) protected_shp = pd.concat([protected_shp, protected_shp1], ignore_index=True) protected_shp = pd.concat([protected_shp, protected_shp2], ignore_index=True) protected_shp = protected_shp.geometry protected_shp = gpd.GeoDataFrame(protected_shp) protected_Marine_shp = gpd.tools.overlay(protected_shp, EEZ_country, how=\"intersection\") # this is to avoid atlite complaining about parallelisation if not os.path.isdir(os.path.dirname(TMP)): mkdir(os.path.dirname(TMP)) protected_Marine_shp.to_file(TMP) excluder_offwind.add_geometry(TMP) kwargs = dict(nprocesses=nprocesses, disable_progressbar=noprogress) if noprogress: logger.info(\"Calculate offwind landuse availabilities...\") start = time.time() offwind_matrix = cutout.availabilitymatrix(EEZ_province_shp, excluder_offwind, **kwargs) duration = time.time() - start logger.info(f\"Completed offwind availability calculation ({duration:2.2f}s)\") else: offwind_matrix = cutout.availabilitymatrix(EEZ_province_shp, excluder_offwind, **kwargs) offwind_potential = offwind_capacity_per_sqkm * offwind_matrix.sum(\"bus\") * area offwind_func = getattr(cutout, offwind_resource.pop(\"method\")) offwind_resource[\"dask_kwargs\"] = {\"num_workers\": nprocesses} # ? offwind_capacity_factor = offwind_correction_factor * offwind_func( capacity_factor=True, **offwind_resource ) offwind_layout = offwind_capacity_factor * area * offwind_capacity_per_sqkm offwind_profile, offwind_capacities = offwind_func( matrix=offwind_matrix.stack(spatial=[\"y\", \"x\"]), layout=offwind_layout, index=EEZ_province_shp.index, per_unit=True, return_capacity=True, **offwind_resource, ) logger.info(\"Calculating offwind maximal capacity per bus (method 'simple')\") offwind_p_nom_max = offwind_capacity_per_sqkm * offwind_matrix @ area offwind_ds = xr.merge( [ (offwind_correction_factor * offwind_profile).rename(\"profile\"), offwind_capacities.rename(\"weight\"), offwind_p_nom_max.rename(\"p_nom_max\"), offwind_potential.rename(\"potential\"), ] ) offwind_ds = offwind_ds.sel( bus=( (offwind_ds[\"profile\"].mean(\"time\") > offwind_config.get(\"min_p_max_pu\", 0.0)) & (offwind_ds[\"p_nom_max\"] > offwind_config.get(\"min_p_nom_max\", 0.0)) ) ) if \"clip_p_max_pu\" in offwind_config: min_p_max_pu = offwind_config[\"clip_p_max_pu\"] offwind_ds[\"profile\"] = offwind_ds[\"profile\"].where( offwind_ds[\"profile\"] >= min_p_max_pu, 0 ) # shift back from UTC to network time offwind_ds[\"time\"] = ( pd.DatetimeIndex(offwind_ds[\"time\"], tz=\"UTC\").tz_convert(TIMEZONE).tz_localize(None).values ) offwind_ds.to_netcdf(outp_path)","title":"make_offshore_wind_profile"},{"location":"docs/reference/build_renewable_potential/#build_renewable_potential.make_onshore_wind_profile","text":"Make the onwind geographical potentials and per unit availability time series for each raster cell ! Somewhat compute intensive ! Parameters: onwind_config ( dict ) \u2013 the onshore wind config (from the yaml config read by snakemake) cutout ( Cutout ) \u2013 the atlite cutout outp_path ( PathLike ) \u2013 the output path for the raster data Source code in workflow/scripts/build_renewable_potential.py def make_onshore_wind_profile(onwind_config: dict, cutout: atlite.Cutout, outp_path: PathLike): \"\"\"Make the onwind geographical potentials and per unit availability time series for each raster cell ! Somewhat compute intensive ! Args: onwind_config (dict): the onshore wind config (from the yaml config read by snakemake) cutout (atlite.Cutout): the atlite cutout outp_path (PathLike): the output path for the raster data \"\"\" logger.info(\"Making onshore wind profile \") onwind_resource = onwind_config[\"resource\"] onwind_correction_factor = onwind_config.get(\"correction_factor\", 1.0) onwind_capacity_per_sqkm = onwind_config[\"capacity_per_sqkm\"] if onwind_correction_factor != 1.0: logger.info(f\"onwind_correction_factor is set as {onwind_correction_factor}\") excluder_onwind = ExclusionContainer(crs=3035, res=500) excluder_onwind.add_raster(grass, invert=True, crs=4326) excluder_onwind.add_raster(bare, invert=True, crs=4326) excluder_onwind.add_raster(shrubland, invert=True, crs=4326) kwargs = dict(nprocesses=nprocesses, disable_progressbar=noprogress) if noprogress: logger.info(\"Calculate onwind landuse availabilities...\") start = time.time() onwind_matrix = cutout.availabilitymatrix(provinces_shp, excluder_onwind, **kwargs) duration = time.time() - start logger.info(f\"Completed onwind availability calculation ({duration:2.2f}s)\") else: onwind_matrix = cutout.availabilitymatrix(provinces_shp, excluder_onwind, **kwargs) onwind_potential = onwind_capacity_per_sqkm * onwind_matrix.sum(\"bus\") * area onwind_func = getattr(cutout, onwind_resource.pop(\"method\")) onwind_resource[\"dask_kwargs\"] = {\"num_workers\": nprocesses} # ? onwind_capacity_factor = onwind_correction_factor * onwind_func( capacity_factor=True, **onwind_resource ) onwind_layout = onwind_capacity_factor * area * onwind_capacity_per_sqkm onwind_profile, onwind_capacities = onwind_func( matrix=onwind_matrix.stack(spatial=[\"y\", \"x\"]), layout=onwind_layout, index=buses, per_unit=True, return_capacity=True, **onwind_resource, ) logger.info(\"Calculating onwind maximal capacity per bus (method 'simple')\") onwind_p_nom_max = onwind_capacity_per_sqkm * onwind_matrix @ area onwind_ds = xr.merge( [ (onwind_correction_factor * onwind_profile).rename(\"profile\"), onwind_capacities.rename(\"weight\"), onwind_p_nom_max.rename(\"p_nom_max\"), onwind_potential.rename(\"potential\"), ] ) onwind_ds = onwind_ds.sel( bus=( (onwind_ds[\"profile\"].mean(\"time\") > onwind_config.get(\"min_p_max_pu\", 0.0)) & (onwind_ds[\"p_nom_max\"] > onwind_config.get(\"min_p_nom_max\", 0.0)) ) ) if \"clip_p_max_pu\" in onwind_config: min_p_max_pu = onwind_config[\"clip_p_max_pu\"] onwind_ds[\"profile\"] = onwind_ds[\"profile\"].where(onwind_ds[\"profile\"] >= min_p_max_pu, 0) # shift back from UTC to network time onwind_ds[\"time\"] = ( pd.DatetimeIndex(onwind_ds[\"time\"], tz=\"UTC\").tz_convert(TIMEZONE).tz_localize(None).values ) onwind_ds.to_netcdf(outp_path)","title":"make_onshore_wind_profile"},{"location":"docs/reference/build_renewable_potential/#build_renewable_potential.make_solar_profile","text":"Make the solar geographical potentials and per unit availability time series for each raster cell ! Somewhat compute intensive ! Parameters: solar_config ( dict ) \u2013 the solar configuration (from the yaml config read by snakemake) cutout ( Cutout ) \u2013 the atlite cutout outp_path ( PathLike ) \u2013 the output path for the raster data Source code in workflow/scripts/build_renewable_potential.py def make_solar_profile( solar_config: dict, cutout: atlite.Cutout, outp_path: PathLike, ): \"\"\"Make the solar geographical potentials and per unit availability time series for each raster cell ! Somewhat compute intensive ! Args: solar_config (dict): the solar configuration (from the yaml config read by snakemake) cutout (atlite.Cutout): the atlite cutout outp_path (PathLike): the output path for the raster data \"\"\" logger.info(\"Making solar profile \") solar_config = snakemake.config[\"renewable\"][\"solar\"] solar_resource = solar_config[\"resource\"] solar_correction_factor = solar_config.get(\"correction_factor\", 1.0) solar_capacity_per_sqkm = solar_config[\"capacity_per_sqkm\"] if solar_correction_factor != 1.0: logger.info(f\"solar_correction_factor is set as {solar_correction_factor}\") # TODO not hardcoded res excluder_solar = ExclusionContainer(crs=3035, res=500) excluder_build_up = ExclusionContainer(crs=3035, res=500) build_up = snakemake.input[\"Build_up_raster\"] excluder_build_up.add_raster(build_up, invert=True, crs=CRS) excluder_solar.add_raster(grass, invert=True, crs=CRS) excluder_solar.add_raster(bare, invert=True, crs=CRS) excluder_solar.add_raster(shrubland, invert=True, crs=CRS) kwargs = dict(nprocesses=nprocesses, disable_progressbar=noprogress) # TODO remove if else? if noprogress: logger.info(\"Calculate solar landuse availabilities...\") start = time.time() solar_matrix = cutout.availabilitymatrix(provinces_shp, excluder_solar, **kwargs) buildup_matrix = cutout.availabilitymatrix(provinces_shp, excluder_build_up, **kwargs) duration = time.time() - start logger.info(f\"Completed solar availability calculation ({duration:2.2f}s)\") else: solar_matrix = cutout.availabilitymatrix( shapes=provinces_shp, excluder=excluder_solar, **kwargs ) buildup_matrix = cutout.availabilitymatrix(provinces_shp, excluder_build_up, **kwargs) solar_potential = ( solar_capacity_per_sqkm * solar_matrix.sum(\"bus\") * area + solar_capacity_per_sqkm * buildup_matrix.sum(\"bus\") * area ) solar_func = getattr(cutout, solar_resource.pop(\"method\")) solar_resource[\"dask_kwargs\"] = {\"num_workers\": nprocesses} # ? solar_capacity_factor = solar_correction_factor * solar_func( capacity_factor=True, **solar_resource ) solar_layout = solar_capacity_factor * area * solar_capacity_per_sqkm solar_profile, solar_capacities = solar_func( matrix=solar_matrix.stack(spatial=[\"y\", \"x\"]), layout=solar_layout, index=buses, per_unit=True, return_capacity=True, **solar_resource, ) logger.info(\"Calculating solar maximal capacity per bus (method 'simple')\") solar_p_nom_max = solar_capacity_per_sqkm * solar_matrix @ area solar_ds = xr.merge( [ (solar_correction_factor * solar_profile).rename(\"profile\"), solar_capacities.rename(\"weight\"), solar_p_nom_max.rename(\"p_nom_max\"), solar_potential.rename(\"potential\"), ] ) solar_ds = solar_ds.sel( bus=( (solar_ds[\"profile\"].mean(\"time\") > solar_config.get(\"min_p_max_pu\", 0.0)) & (solar_ds[\"p_nom_max\"] > solar_config.get(\"min_p_nom_max\", 0.0)) ) ) if \"clip_p_max_pu\" in solar_config: min_p_max_pu = solar_config[\"clip_p_max_pu\"] solar_ds[\"profile\"] = solar_ds[\"profile\"].where(solar_ds[\"profile\"] >= min_p_max_pu, 0) # shift back from UTC to network time solar_ds[\"time\"] = ( pd.DatetimeIndex(solar_ds[\"time\"], tz=\"UTC\").tz_convert(TIMEZONE).tz_localize(None).values ) solar_ds.to_netcdf(outp_path)","title":"make_solar_profile"},{"location":"docs/reference/build_solar_thermal_profiles/","text":"","title":"build_solar_thermal_profiles"},{"location":"docs/reference/build_temperature_profiles/","text":"Functions associated with the build_temperature_profiles rule. build_temp_profiles(pop_map_path, cutout_path, temperature_out) build the temperature profiles in the cutout, this converts the atlite temperature & weights the node building process by the population map Note that atlite only supports a single time zone shift Parameters: pop_map_path ( PathLike ) \u2013 the map to the pop density grid cell data (hdf5) cutout_path ( PathLike ) \u2013 the cutout path (atlite cutout) temperature_out ( PathLike ) \u2013 the output path (hdf5) Source code in workflow/scripts/build_temperature_profiles.py def build_temp_profiles(pop_map_path: PathLike, cutout_path: PathLike, temperature_out: PathLike): \"\"\"build the temperature profiles in the cutout, this converts the atlite temperature & weights the node building process by the population map Note that atlite only supports a single time zone shift Args: pop_map_path (PathLike): the map to the pop density grid cell data (hdf5) cutout_path (PathLike): the cutout path (atlite cutout) temperature_out (PathLike): the output path (hdf5) \"\"\" with pd.HDFStore(pop_map_path, mode=\"r\") as store: pop_map = store[\"population_gridcell_map\"] # this one includes soil temperature cutout = atlite.Cutout(cutout_path) # build a sparse matrix of BUSxCUTOUT_gridcells to weigh the cutout->bus aggregation process pop_matrix = sp.sparse.csr_matrix(pop_map.T) index = pop_map.columns index.name = \"provinces\" temperature = cutout.temperature(matrix=pop_matrix, index=index) # convert the cutout UTC time to local time temperature[\"time\"] = ( pd.DatetimeIndex(temperature[\"time\"], tz=\"UTC\") .tz_convert(TIMEZONE) .tz_localize(None) .values ) with pd.HDFStore(temperature_out, mode=\"w\", complevel=4) as store: store[\"temperature\"] = temperature.to_pandas().divide(pop_map.sum())","title":"build_temperature_profiles"},{"location":"docs/reference/build_temperature_profiles/#build_temperature_profiles.build_temp_profiles","text":"build the temperature profiles in the cutout, this converts the atlite temperature & weights the node building process by the population map Note that atlite only supports a single time zone shift Parameters: pop_map_path ( PathLike ) \u2013 the map to the pop density grid cell data (hdf5) cutout_path ( PathLike ) \u2013 the cutout path (atlite cutout) temperature_out ( PathLike ) \u2013 the output path (hdf5) Source code in workflow/scripts/build_temperature_profiles.py def build_temp_profiles(pop_map_path: PathLike, cutout_path: PathLike, temperature_out: PathLike): \"\"\"build the temperature profiles in the cutout, this converts the atlite temperature & weights the node building process by the population map Note that atlite only supports a single time zone shift Args: pop_map_path (PathLike): the map to the pop density grid cell data (hdf5) cutout_path (PathLike): the cutout path (atlite cutout) temperature_out (PathLike): the output path (hdf5) \"\"\" with pd.HDFStore(pop_map_path, mode=\"r\") as store: pop_map = store[\"population_gridcell_map\"] # this one includes soil temperature cutout = atlite.Cutout(cutout_path) # build a sparse matrix of BUSxCUTOUT_gridcells to weigh the cutout->bus aggregation process pop_matrix = sp.sparse.csr_matrix(pop_map.T) index = pop_map.columns index.name = \"provinces\" temperature = cutout.temperature(matrix=pop_matrix, index=index) # convert the cutout UTC time to local time temperature[\"time\"] = ( pd.DatetimeIndex(temperature[\"time\"], tz=\"UTC\") .tz_convert(TIMEZONE) .tz_localize(None) .values ) with pd.HDFStore(temperature_out, mode=\"w\", complevel=4) as store: store[\"temperature\"] = temperature.to_pandas().divide(pop_map.sum())","title":"build_temp_profiles"},{"location":"docs/reference/constants/","text":"Soft coded centalized constants get_province_names() HACK to make it possible for pytest to generate a smaller network Raises: ValueError \u2013 if the PROV_NAMES is not a list or str Returns: list ( list ) \u2013 the province node names to build the network Source code in workflow/scripts/constants.py def get_province_names() -> list: \"\"\"HACK to make it possible for pytest to generate a smaller network Raises: ValueError: if the PROV_NAMES is not a list or str Returns: list: the province node names to build the network \"\"\" default_prov_names = list(REGIONAL_GEO_TIMEZONES_DEFAULT) _provs = os.getenv(\"PROV_NAMES\", default_prov_names) if isinstance(_provs, str): _provs = re.findall(r\"[\\w']+\", _provs) if not _provs: xpected = '[\"region1\", ...]' err = f\"Environment var PROV_NAMES {_provs} for tests did not have expected format: \" raise ValueError(err + xpected) elif not isinstance(_provs, list): raise ValueError(\"PROV_NAMES must be a list or str\") return _provs","title":"constants"},{"location":"docs/reference/constants/#constants.get_province_names","text":"HACK to make it possible for pytest to generate a smaller network Raises: ValueError \u2013 if the PROV_NAMES is not a list or str Returns: list ( list ) \u2013 the province node names to build the network Source code in workflow/scripts/constants.py def get_province_names() -> list: \"\"\"HACK to make it possible for pytest to generate a smaller network Raises: ValueError: if the PROV_NAMES is not a list or str Returns: list: the province node names to build the network \"\"\" default_prov_names = list(REGIONAL_GEO_TIMEZONES_DEFAULT) _provs = os.getenv(\"PROV_NAMES\", default_prov_names) if isinstance(_provs, str): _provs = re.findall(r\"[\\w']+\", _provs) if not _provs: xpected = '[\"region1\", ...]' err = f\"Environment var PROV_NAMES {_provs} for tests did not have expected format: \" raise ValueError(err + xpected) elif not isinstance(_provs, list): raise ValueError(\"PROV_NAMES must be a list or str\") return _provs","title":"get_province_names"},{"location":"docs/reference/fetch_rasters/","text":"Methods to fetch raster data from copernicus archive. Requires sentinelHub API. EXAMPLE SCRIPT find_dataset_ids(name='global-dynamic-land-cover') find the catalogue ID of the dataset Parameters: name ( str , default: 'global-dynamic-land-cover' ) \u2013 dataset product name. Defaults to \"global-dynamic-land-cover\". Returns: dict ( dict ) \u2013 the results Source code in workflow/scripts/fetch_rasters.py def find_dataset_ids(name=\"global-dynamic-land-cover\") -> dict: \"\"\"find the catalogue ID of the dataset Args: name (str, optional): dataset product name. Defaults to \"global-dynamic-land-cover\". Returns: dict: the results \"\"\" bid = 0 # batches of 25 while True: batch = \"\" if bid == 0 else f\"b_start={bid}&\" search_req = requests.get( f\"https://land.copernicus.eu/api/@search?{batch}portal_type=DataSet&metadata_fields=UID&metadata_fields=dataset_full_format&&metadata_fields=dataset_download_information\", headers={\"Accept\": \"application/json\"}, ) if search_req.status_code != 200: print(search_req.text) exit(1) search_results = search_req.json() res = search_items(search_results, target_name=name) if res == []: bid += 25 else: break if search_results.status_code != 200: raise ValueError(search_results.text) res_list = [r for r in search_results[\"items\"] if r[\"@id\"].find(\"2019\") != -1] download_info_id = res_list[0][\"dataset_download_information\"][\"items\"][0][\"@id\"] download_id = res_list[0][\"UID\"] # dataset_url = res_list[0][\"@id\"] return download_info_id, download_id","title":"fetch_rasters"},{"location":"docs/reference/fetch_rasters/#fetch_rasters.find_dataset_ids","text":"find the catalogue ID of the dataset Parameters: name ( str , default: 'global-dynamic-land-cover' ) \u2013 dataset product name. Defaults to \"global-dynamic-land-cover\". Returns: dict ( dict ) \u2013 the results Source code in workflow/scripts/fetch_rasters.py def find_dataset_ids(name=\"global-dynamic-land-cover\") -> dict: \"\"\"find the catalogue ID of the dataset Args: name (str, optional): dataset product name. Defaults to \"global-dynamic-land-cover\". Returns: dict: the results \"\"\" bid = 0 # batches of 25 while True: batch = \"\" if bid == 0 else f\"b_start={bid}&\" search_req = requests.get( f\"https://land.copernicus.eu/api/@search?{batch}portal_type=DataSet&metadata_fields=UID&metadata_fields=dataset_full_format&&metadata_fields=dataset_download_information\", headers={\"Accept\": \"application/json\"}, ) if search_req.status_code != 200: print(search_req.text) exit(1) search_results = search_req.json() res = search_items(search_results, target_name=name) if res == []: bid += 25 else: break if search_results.status_code != 200: raise ValueError(search_results.text) res_list = [r for r in search_results[\"items\"] if r[\"@id\"].find(\"2019\") != -1] download_info_id = res_list[0][\"dataset_download_information\"][\"items\"][0][\"@id\"] download_id = res_list[0][\"UID\"] # dataset_url = res_list[0][\"@id\"] return download_info_id, download_id","title":"find_dataset_ids"},{"location":"docs/reference/fetch_shapes/","text":"Data fetch operation for region/province/country shapes eez_by_region(eez, province_shapes, prov_key='region') break up the eez by admin1 regions based on voronoi polygons of the centroids Parameters: eez ( GeoDataFrame ) \u2013 description province_shapes ( GeoDataFrame ) \u2013 description prov_key ( str , default: 'region' ) \u2013 name of the provinces col in province_shapes. Defaults to \"region\". Returns: GeoDataFrame \u2013 gpd.GeoDataFrame: description Source code in workflow/scripts/fetch_shapes.py def eez_by_region( eez: gpd.GeoDataFrame, province_shapes: gpd.GeoDataFrame, prov_key=\"region\" ) -> gpd.GeoDataFrame: \"\"\"break up the eez by admin1 regions based on voronoi polygons of the centroids Args: eez (gpd.GeoDataFrame): _description_ province_shapes (gpd.GeoDataFrame): _description_ prov_key (str, optional): name of the provinces col in province_shapes. Defaults to \"region\". Returns: gpd.GeoDataFrame: _description_ \"\"\" voronoi_cells = gpd.GeoDataFrame( geometry=province_shapes.centroid.voronoi_polygons(), crs=province_shapes.crs ) prov_centroids = province_shapes.copy() prov_centroids.geometry = prov_centroids.centroid # need to assign cells to province voronoi_cells = voronoi_cells.sjoin(prov_centroids, predicate=\"contains\").reset_index() if \"index_right\" in voronoi_cells.columns: voronoi_cells.drop(columns=[\"index_right\"], inplace=True) logger.debug(f\"Voronoi cells: {voronoi_cells}\") # check the below with ez.overlay(voronoi_cells, how=\"intersection\").boundary.plot() eez_by_region = voronoi_cells.overlay(eez, how=\"intersection\")[[prov_key, \"geometry\"]] return eez_by_region[eez_by_region[prov_key].isin(PROV_NAMES)].set_index(prov_key) fetch_country_shape(outp_path) fetch the country shape from natural earth and save it to the outpath Parameters: outp_path ( PathLike ) \u2013 the path to save the country shape (geojson) Source code in workflow/scripts/fetch_shapes.py def fetch_country_shape(outp_path: PathLike): \"\"\"fetch the country shape from natural earth and save it to the outpath Args: outp_path (PathLike): the path to save the country shape (geojson) \"\"\" country_shape = fetch_natural_earth_shape(\"admin_0_countries\", \"ADMIN\", COUNTRY_NAME) country_shape.set_index(\"region\", inplace=True) country_shape.to_file(outp_path, driver=\"GeoJSON\") fetch_maritime_eez(zone_name) fetch maritime data for a country from Maritime Gazette API# (Royal marine institute of Flanders data base) Parameters: zone_name ( str ) \u2013 the country's zone name, e.g \"Chinese\" for china Raises: HTTPError \u2013 if the request fails Returns: dict: the maritime data Source code in workflow/scripts/fetch_shapes.py def fetch_maritime_eez(zone_name: str) -> gpd.GeoDataFrame: \"\"\"fetch maritime data for a country from Maritime Gazette API# (Royal marine institute of Flanders data base) Args: zone_name (str): the country's zone name, e.g \"Chinese\" for china Raises: requests.HTTPError: if the request fails Returns: dict: the maritime data \"\"\" def find_record_id(zone_name: str) -> int: # get Maritime Gazette record ID for the country # the eez ID is 70: see https://www.marineregions.org/gazetteer.php?p=webservices&type=rest#/ url = f\"https://www.marineregions.org/rest/getGazetteerRecordsByName.json/{zone_name}/?like=true&fuzzy=false&typeID=70&offset=0&count=100\" response = requests.get(url) if response.status_code != 200: raise requests.HTTPError( f\"Failed to retrieve Maritime Gazette ID. Status code: {response.status_code}\" ) record_data = response.json() logger.debug(record_data) return [ data for data in record_data if (data[\"status\"] == \"standard\") and (data[\"preferredGazetteerName\"].lower().find(zone_name.lower()) != -1) ][0][\"MRGID\"] mgrid = find_record_id(zone_name) logger.debug(f\"Found Maritime Gazette ID for {zone_name}: {mgrid}\") # URL of the WFS service url = \"https://geo.vliz.be/geoserver/wfs\" # WFS request parameters + record ID filter params = dict( service=\"WFS\", version=\"1.1.0\", request=\"GetFeature\", typeName=\"MarineRegions:eez\", outputFormat=\"json\", filter=f\"<Filter><PropertyIsEqualTo><PropertyName>mrgid_eez</PropertyName><Literal>{mgrid}</Literal></PropertyIsEqualTo></Filter>\", ) # Fetch data from WFS using requests response_eez = requests.get(url, params=params) # Check for successful request if response_eez.status_code == 200: data = response_eez.json() else: logger.error(f\"Error: {response_eez.status_code}\") raise requests.HTTPError( f\"Failed to retrieve Maritime Gazette data. Status code: {response_eez.status_code}\" ) if data[\"totalFeatures\"] != 1: raise ValueError(f\"Expected 1 feature, got {data['totalFeatures']}\\n: {data}\") crs = data[\"crs\"][\"properties\"][\"name\"].split(\"EPSG::\")[-1] eez = gpd.GeoDataFrame.from_features(data[\"features\"]) return eez.set_crs(epsg=crs) fetch_natural_earth_shape(dataset_name, filter_key, filter_value='China', region_key=None) fetch region or country shape from natural earth dataset and filter Parameters: dataset_name ( str ) \u2013 the name of the natural earth dataset to fetch filter_key ( str ) \u2013 key to filter the records by filter_value ( str | list , default: 'China' ) \u2013 filter pass value. Defaults to \"China\". Example china country: build_natural_earth_shape(\"admin_0_countries\", \"ADMIN\", \"China\") china provinces: build_natural_earth_shape(\"admin_1_states_provinces\", \"iso_a2\", \"CN\", region_key = \"name_en\") Returns: GeoDataFrame \u2013 gpd.GeoDataFrame: the filtered records Source code in workflow/scripts/fetch_shapes.py def fetch_natural_earth_shape( dataset_name: str, filter_key: str, filter_value=\"China\", region_key=None ) -> gpd.GeoDataFrame: \"\"\"fetch region or country shape from natural earth dataset and filter Args: dataset_name (str): the name of the natural earth dataset to fetch filter_key (str): key to filter the records by filter_value (str|list, optional): filter pass value. Defaults to \"China\". Example: china country: build_natural_earth_shape(\"admin_0_countries\", \"ADMIN\", \"China\") china provinces: build_natural_earth_shape(\"admin_1_states_provinces\", \"iso_a2\", \"CN\", region_key = \"name_en\") Returns: gpd.GeoDataFrame: the filtered records \"\"\" shpfilename = shpreader.natural_earth( resolution=NATURAL_EARTH_RESOLUTION, category=\"cultural\", name=dataset_name ) reader = shpreader.Reader(shpfilename) records = list(reader.records()) if not region_key: region_key = filter_key if isinstance(filter_value, list): gdf = gpd.GeoDataFrame( [ {\"region\": c.attributes[region_key], \"geometry\": c.geometry} for c in records if c.attributes[filter_key] in filter_value ] ) else: gdf = gpd.GeoDataFrame( [ {\"region\": c.attributes[region_key], \"geometry\": c.geometry} for c in records if c.attributes[filter_key] == filter_value ] ) gdf.set_crs(epsg=CRS, inplace=True) return gdf fetch_province_shapes() fetch the province shapes from natural earth and save it to the outpath Returns: GeoDataFrame \u2013 gpd.GeoDataFrame: the province shapes Source code in workflow/scripts/fetch_shapes.py def fetch_province_shapes() -> gpd.GeoDataFrame: \"\"\"fetch the province shapes from natural earth and save it to the outpath Returns: gpd.GeoDataFrame: the province shapes \"\"\" province_shapes = fetch_natural_earth_shape( \"admin_1_states_provinces\", \"iso_a2\", COUNTRY_ISO, region_key=\"name_en\" ) province_shapes.rename(columns={\"region\": \"province\"}, inplace=True) province_shapes.province = province_shapes.province.str.replace(\" \", \"\") province_shapes.sort_values(\"province\", inplace=True) logger.debug(\"province shapes:\\n\", province_shapes) filtered = province_shapes[province_shapes[\"province\"].isin(PROV_NAMES)] if (filtered[\"province\"].unique() != sorted(PROV_NAMES)).all(): logger.warning( f\"Missing provinces: {set(PROV_NAMES) - set(province_shapes['province'].unique())}\" ) filtered.set_index(\"province\", inplace=True) return filtered.sort_index()","title":"fetch_shapes"},{"location":"docs/reference/fetch_shapes/#fetch_shapes.eez_by_region","text":"break up the eez by admin1 regions based on voronoi polygons of the centroids Parameters: eez ( GeoDataFrame ) \u2013 description province_shapes ( GeoDataFrame ) \u2013 description prov_key ( str , default: 'region' ) \u2013 name of the provinces col in province_shapes. Defaults to \"region\". Returns: GeoDataFrame \u2013 gpd.GeoDataFrame: description Source code in workflow/scripts/fetch_shapes.py def eez_by_region( eez: gpd.GeoDataFrame, province_shapes: gpd.GeoDataFrame, prov_key=\"region\" ) -> gpd.GeoDataFrame: \"\"\"break up the eez by admin1 regions based on voronoi polygons of the centroids Args: eez (gpd.GeoDataFrame): _description_ province_shapes (gpd.GeoDataFrame): _description_ prov_key (str, optional): name of the provinces col in province_shapes. Defaults to \"region\". Returns: gpd.GeoDataFrame: _description_ \"\"\" voronoi_cells = gpd.GeoDataFrame( geometry=province_shapes.centroid.voronoi_polygons(), crs=province_shapes.crs ) prov_centroids = province_shapes.copy() prov_centroids.geometry = prov_centroids.centroid # need to assign cells to province voronoi_cells = voronoi_cells.sjoin(prov_centroids, predicate=\"contains\").reset_index() if \"index_right\" in voronoi_cells.columns: voronoi_cells.drop(columns=[\"index_right\"], inplace=True) logger.debug(f\"Voronoi cells: {voronoi_cells}\") # check the below with ez.overlay(voronoi_cells, how=\"intersection\").boundary.plot() eez_by_region = voronoi_cells.overlay(eez, how=\"intersection\")[[prov_key, \"geometry\"]] return eez_by_region[eez_by_region[prov_key].isin(PROV_NAMES)].set_index(prov_key)","title":"eez_by_region"},{"location":"docs/reference/fetch_shapes/#fetch_shapes.fetch_country_shape","text":"fetch the country shape from natural earth and save it to the outpath Parameters: outp_path ( PathLike ) \u2013 the path to save the country shape (geojson) Source code in workflow/scripts/fetch_shapes.py def fetch_country_shape(outp_path: PathLike): \"\"\"fetch the country shape from natural earth and save it to the outpath Args: outp_path (PathLike): the path to save the country shape (geojson) \"\"\" country_shape = fetch_natural_earth_shape(\"admin_0_countries\", \"ADMIN\", COUNTRY_NAME) country_shape.set_index(\"region\", inplace=True) country_shape.to_file(outp_path, driver=\"GeoJSON\")","title":"fetch_country_shape"},{"location":"docs/reference/fetch_shapes/#fetch_shapes.fetch_maritime_eez","text":"fetch maritime data for a country from Maritime Gazette API# (Royal marine institute of Flanders data base) Parameters: zone_name ( str ) \u2013 the country's zone name, e.g \"Chinese\" for china Raises: HTTPError \u2013 if the request fails Returns: dict: the maritime data Source code in workflow/scripts/fetch_shapes.py def fetch_maritime_eez(zone_name: str) -> gpd.GeoDataFrame: \"\"\"fetch maritime data for a country from Maritime Gazette API# (Royal marine institute of Flanders data base) Args: zone_name (str): the country's zone name, e.g \"Chinese\" for china Raises: requests.HTTPError: if the request fails Returns: dict: the maritime data \"\"\" def find_record_id(zone_name: str) -> int: # get Maritime Gazette record ID for the country # the eez ID is 70: see https://www.marineregions.org/gazetteer.php?p=webservices&type=rest#/ url = f\"https://www.marineregions.org/rest/getGazetteerRecordsByName.json/{zone_name}/?like=true&fuzzy=false&typeID=70&offset=0&count=100\" response = requests.get(url) if response.status_code != 200: raise requests.HTTPError( f\"Failed to retrieve Maritime Gazette ID. Status code: {response.status_code}\" ) record_data = response.json() logger.debug(record_data) return [ data for data in record_data if (data[\"status\"] == \"standard\") and (data[\"preferredGazetteerName\"].lower().find(zone_name.lower()) != -1) ][0][\"MRGID\"] mgrid = find_record_id(zone_name) logger.debug(f\"Found Maritime Gazette ID for {zone_name}: {mgrid}\") # URL of the WFS service url = \"https://geo.vliz.be/geoserver/wfs\" # WFS request parameters + record ID filter params = dict( service=\"WFS\", version=\"1.1.0\", request=\"GetFeature\", typeName=\"MarineRegions:eez\", outputFormat=\"json\", filter=f\"<Filter><PropertyIsEqualTo><PropertyName>mrgid_eez</PropertyName><Literal>{mgrid}</Literal></PropertyIsEqualTo></Filter>\", ) # Fetch data from WFS using requests response_eez = requests.get(url, params=params) # Check for successful request if response_eez.status_code == 200: data = response_eez.json() else: logger.error(f\"Error: {response_eez.status_code}\") raise requests.HTTPError( f\"Failed to retrieve Maritime Gazette data. Status code: {response_eez.status_code}\" ) if data[\"totalFeatures\"] != 1: raise ValueError(f\"Expected 1 feature, got {data['totalFeatures']}\\n: {data}\") crs = data[\"crs\"][\"properties\"][\"name\"].split(\"EPSG::\")[-1] eez = gpd.GeoDataFrame.from_features(data[\"features\"]) return eez.set_crs(epsg=crs)","title":"fetch_maritime_eez"},{"location":"docs/reference/fetch_shapes/#fetch_shapes.fetch_natural_earth_shape","text":"fetch region or country shape from natural earth dataset and filter Parameters: dataset_name ( str ) \u2013 the name of the natural earth dataset to fetch filter_key ( str ) \u2013 key to filter the records by filter_value ( str | list , default: 'China' ) \u2013 filter pass value. Defaults to \"China\". Example china country: build_natural_earth_shape(\"admin_0_countries\", \"ADMIN\", \"China\") china provinces: build_natural_earth_shape(\"admin_1_states_provinces\", \"iso_a2\", \"CN\", region_key = \"name_en\") Returns: GeoDataFrame \u2013 gpd.GeoDataFrame: the filtered records Source code in workflow/scripts/fetch_shapes.py def fetch_natural_earth_shape( dataset_name: str, filter_key: str, filter_value=\"China\", region_key=None ) -> gpd.GeoDataFrame: \"\"\"fetch region or country shape from natural earth dataset and filter Args: dataset_name (str): the name of the natural earth dataset to fetch filter_key (str): key to filter the records by filter_value (str|list, optional): filter pass value. Defaults to \"China\". Example: china country: build_natural_earth_shape(\"admin_0_countries\", \"ADMIN\", \"China\") china provinces: build_natural_earth_shape(\"admin_1_states_provinces\", \"iso_a2\", \"CN\", region_key = \"name_en\") Returns: gpd.GeoDataFrame: the filtered records \"\"\" shpfilename = shpreader.natural_earth( resolution=NATURAL_EARTH_RESOLUTION, category=\"cultural\", name=dataset_name ) reader = shpreader.Reader(shpfilename) records = list(reader.records()) if not region_key: region_key = filter_key if isinstance(filter_value, list): gdf = gpd.GeoDataFrame( [ {\"region\": c.attributes[region_key], \"geometry\": c.geometry} for c in records if c.attributes[filter_key] in filter_value ] ) else: gdf = gpd.GeoDataFrame( [ {\"region\": c.attributes[region_key], \"geometry\": c.geometry} for c in records if c.attributes[filter_key] == filter_value ] ) gdf.set_crs(epsg=CRS, inplace=True) return gdf","title":"fetch_natural_earth_shape"},{"location":"docs/reference/fetch_shapes/#fetch_shapes.fetch_province_shapes","text":"fetch the province shapes from natural earth and save it to the outpath Returns: GeoDataFrame \u2013 gpd.GeoDataFrame: the province shapes Source code in workflow/scripts/fetch_shapes.py def fetch_province_shapes() -> gpd.GeoDataFrame: \"\"\"fetch the province shapes from natural earth and save it to the outpath Returns: gpd.GeoDataFrame: the province shapes \"\"\" province_shapes = fetch_natural_earth_shape( \"admin_1_states_provinces\", \"iso_a2\", COUNTRY_ISO, region_key=\"name_en\" ) province_shapes.rename(columns={\"region\": \"province\"}, inplace=True) province_shapes.province = province_shapes.province.str.replace(\" \", \"\") province_shapes.sort_values(\"province\", inplace=True) logger.debug(\"province shapes:\\n\", province_shapes) filtered = province_shapes[province_shapes[\"province\"].isin(PROV_NAMES)] if (filtered[\"province\"].unique() != sorted(PROV_NAMES)).all(): logger.warning( f\"Missing provinces: {set(PROV_NAMES) - set(province_shapes['province'].unique())}\" ) filtered.set_index(\"province\", inplace=True) return filtered.sort_index()","title":"fetch_province_shapes"},{"location":"docs/reference/functions/","text":"MAths calculations area_from_lon_lat_poly(geometry) For shapely geometry in lon-lat coordinates, returns area in km^2. Source code in workflow/scripts/functions.py def area_from_lon_lat_poly(geometry): \"\"\"For shapely geometry in lon-lat coordinates, returns area in km^2.\"\"\" project = partial( pyproj.transform, pyproj.Proj(init=\"epsg:4326\"), pyproj.Proj(proj=\"aea\") # Source: Lon-Lat ) # Target: Albers Equal Area Conical https://en.wikipedia.org/wiki/Albers_projection # TODO fix new_geometry = transform(project, geometry) # default area is in m^2 return new_geometry.area / 1e6 haversine(p1, p2) Calculate the great circle distance in km between two points on the earth (specified in decimal degrees) Parameters: p1 ( Point ) \u2013 location 1 in decimal deg p2 ( Point ) \u2013 location 2 in decimal deg Returns: float ( float ) \u2013 great circle distance in [km] Source code in workflow/scripts/functions.py def haversine(p1, p2) -> float: \"\"\"Calculate the great circle distance in km between two points on the earth (specified in decimal degrees) Args: p1 (shapely.Point): location 1 in decimal deg p2 (shapely.Point): location 2 in decimal deg Returns: float: great circle distance in [km] \"\"\" # convert decimal degrees to radians lon1, lat1, lon2, lat2 = map(radians, [p1[0], p1[1], p2[0], p2[1]]) # haversine formula dlon = lon2 - lon1 dlat = lat2 - lat1 a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2 c = 2 * asin(sqrt(a)) r = 6371 # Radius of earth in kilometers. Use 3956 for miles return c * r","title":"functions"},{"location":"docs/reference/functions/#functions.area_from_lon_lat_poly","text":"For shapely geometry in lon-lat coordinates, returns area in km^2. Source code in workflow/scripts/functions.py def area_from_lon_lat_poly(geometry): \"\"\"For shapely geometry in lon-lat coordinates, returns area in km^2.\"\"\" project = partial( pyproj.transform, pyproj.Proj(init=\"epsg:4326\"), pyproj.Proj(proj=\"aea\") # Source: Lon-Lat ) # Target: Albers Equal Area Conical https://en.wikipedia.org/wiki/Albers_projection # TODO fix new_geometry = transform(project, geometry) # default area is in m^2 return new_geometry.area / 1e6","title":"area_from_lon_lat_poly"},{"location":"docs/reference/functions/#functions.haversine","text":"Calculate the great circle distance in km between two points on the earth (specified in decimal degrees) Parameters: p1 ( Point ) \u2013 location 1 in decimal deg p2 ( Point ) \u2013 location 2 in decimal deg Returns: float ( float ) \u2013 great circle distance in [km] Source code in workflow/scripts/functions.py def haversine(p1, p2) -> float: \"\"\"Calculate the great circle distance in km between two points on the earth (specified in decimal degrees) Args: p1 (shapely.Point): location 1 in decimal deg p2 (shapely.Point): location 2 in decimal deg Returns: float: great circle distance in [km] \"\"\" # convert decimal degrees to radians lon1, lat1, lon2, lat2 = map(radians, [p1[0], p1[1], p2[0], p2[1]]) # haversine formula dlon = lon2 - lon1 dlat = lat2 - lat1 a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2 c = 2 * asin(sqrt(a)) r = 6371 # Radius of earth in kilometers. Use 3956 for miles return c * r","title":"haversine"},{"location":"docs/reference/make_summary/","text":"Create summary CSV files for all scenario runs including costs, capacities, capacity factors, curtailment, energy balances, prices and other metrics. calculate_co2_balance(n, label, co2_balance, withdrawal_stores=['CO2 capture']) calc the co2 balance [DOES NOT INCLUDE EMISSION GENERATING LINKSs] Args: n (pypsa.Network): the network object withdrawal_stores (list, optional): names of stores. Defaults to [\"CO2 capture\"]. label (str): the label for the column co2_balance (pd.DataFrame): the df to update Returns: DataFrame \u2013 tuple[float,float,float]: balance, Source code in workflow/scripts/make_summary.py def calculate_co2_balance( n: pypsa.Network, label: str, co2_balance: pd.DataFrame, withdrawal_stores=[\"CO2 capture\"] ) -> pd.DataFrame: \"\"\"calc the co2 balance [DOES NOT INCLUDE EMISSION GENERATING LINKSs] Args: n (pypsa.Network): the network object withdrawal_stores (list, optional): names of stores. Defaults to [\"CO2 capture\"]. label (str): the label for the column co2_balance (pd.DataFrame): the df to update Returns: tuple[float,float,float]: balance, \"\"\" # year *(assumes one planning year intended), year = int(np.round(n.snapshots.year.values.mean(), 0)) # emissions from generators (from fneumann course) emissions = ( n.generators_t.p / n.generators.efficiency * n.generators.carrier.map(n.carriers.co2_emissions) ) # t/h emissions_carrier = ( (n.snapshot_weightings.generators @ emissions).groupby(n.generators.carrier).sum() ) # format and drop 0 values emissions_carrier = emissions_carrier.where(emissions_carrier > 0).dropna() emissions_carrier.rename(year, inplace=True) emissions_carrier = emissions_carrier.to_frame() # CO2 withdrawal stores = n.stores_t.e.T.groupby(n.stores.carrier).sum() co2_stores = stores.index.intersection(withdrawal_stores) co2_withdrawal = stores.iloc[:, -1].loc[co2_stores] * -1 co2_withdrawal.rename(year, inplace=True) co2_withdrawal = co2_withdrawal.to_frame() year_balance = pd.concat([emissions_carrier, co2_withdrawal]) # combine with previous co2_balance = co2_balance.reindex(year_balance.index.union(co2_balance.index)) co2_balance.loc[year_balance.index, label] = year_balance[year] return co2_balance calculate_peak_dispatch(n, label, supply) Calculate the MAX dispatch of each component at the buses aggregated by carrier. Parameters: n ( Network ) \u2013 the network object label ( str ) \u2013 the labe representing the pathway supply ( DataFrame ) \u2013 supply energy balance (empty df) Returns: DataFrame \u2013 pd.DataFrame: updated supply DF Source code in workflow/scripts/make_summary.py def calculate_peak_dispatch(n: pypsa.Network, label: str, supply: pd.DataFrame) -> pd.DataFrame: \"\"\"Calculate the MAX dispatch of each component at the buses aggregated by carrier. Args: n (pypsa.Network): the network object label (str): the labe representing the pathway supply (pd.DataFrame): supply energy balance (empty df) Returns: pd.DataFrame: updated supply DF \"\"\" sup_ = n.statistics.supply( groupby=pypsa.statistics.get_carrier_and_bus_carrier, aggregate_time=\"max\" ) supply_reordered = sup_.reorder_levels([2, 0, 1]) supply_reordered.sort_index(inplace=True) supply[label] = supply_reordered return supply calculate_supply_energy(n, label, supply_energy) Calculate the total energy supply/consuption of each component at the buses aggregated by carrier. Parameters: n ( Network ) \u2013 the network object label ( str ) \u2013 the labe representing the pathway supply_energy ( DataFrame ) \u2013 supply energy balance (empty df) Returns: DataFrame \u2013 pd.DataFrame: updated supply energy balance Source code in workflow/scripts/make_summary.py def calculate_supply_energy( n: pypsa.Network, label: str, supply_energy: pd.DataFrame ) -> pd.DataFrame: \"\"\"Calculate the total energy supply/consuption of each component at the buses aggregated by carrier. Args: n (pypsa.Network): the network object label (str): the labe representing the pathway supply_energy (pd.DataFrame): supply energy balance (empty df) Returns: pd.DataFrame: updated supply energy balance \"\"\" eb = n.statistics.energy_balance(groupby=pypsa.statistics.get_carrier_and_bus_carrier) # fragile eb_reordered = eb.reorder_levels([2, 0, 1]) eb_reordered.sort_index(inplace=True) eb_reordered.rename(index={\"AC\": \"transmission losses\"}, level=2, inplace=True) supply_energy[label] = eb_reordered return supply_energy calculate_weighted_prices(n, label, weighted_prices) Demand-weighed prices for stores and loads. For stores if withdrawal is zero, use supply instead. Args: n (pypsa.Network): the network object label (str): the label representing the pathway (not needed, refactor) weighted_prices (pd.DataFrame): the dataframe to write to (not needed, refactor) Returns: DataFrame \u2013 pd.DataFrame: updated weighted_prices Source code in workflow/scripts/make_summary.py def calculate_weighted_prices( n: pypsa.Network, label: str, weighted_prices: pd.DataFrame ) -> pd.DataFrame: \"\"\"Demand-weighed prices for stores and loads. For stores if withdrawal is zero, use supply instead. Args: n (pypsa.Network): the network object label (str): the label representing the pathway (not needed, refactor) weighted_prices (pd.DataFrame): the dataframe to write to (not needed, refactor) Returns: pd.DataFrame: updated weighted_prices \"\"\" entries = pd.Index([\"electricity\", \"heat\", \"H2\", \"CO2 capture\", \"gas\", \"biomass\"]) weighted_prices = weighted_prices.reindex(entries) # loads loads = ( n.statistics.revenue(comps=\"Load\", groupby=pypsa.statistics.get_bus_carrier) / n.statistics.withdrawal(comps=\"Load\", groupby=pypsa.statistics.get_bus_carrier) * -1 ) loads.rename(index={\"AC\": \"electricity\"}, inplace=True) # stores w = n.statistics.withdrawal(comps=\"Store\") # biomass stores have no withdrawal for some reason w[w == 0] = n.statistics.supply(comps=\"Store\")[w == 0] weighted_prices[label] = pd.concat([loads, n.statistics.revenue(comps=\"Store\") / w]) return weighted_prices","title":"make_summary"},{"location":"docs/reference/make_summary/#make_summary.calculate_co2_balance","text":"calc the co2 balance [DOES NOT INCLUDE EMISSION GENERATING LINKSs] Args: n (pypsa.Network): the network object withdrawal_stores (list, optional): names of stores. Defaults to [\"CO2 capture\"]. label (str): the label for the column co2_balance (pd.DataFrame): the df to update Returns: DataFrame \u2013 tuple[float,float,float]: balance, Source code in workflow/scripts/make_summary.py def calculate_co2_balance( n: pypsa.Network, label: str, co2_balance: pd.DataFrame, withdrawal_stores=[\"CO2 capture\"] ) -> pd.DataFrame: \"\"\"calc the co2 balance [DOES NOT INCLUDE EMISSION GENERATING LINKSs] Args: n (pypsa.Network): the network object withdrawal_stores (list, optional): names of stores. Defaults to [\"CO2 capture\"]. label (str): the label for the column co2_balance (pd.DataFrame): the df to update Returns: tuple[float,float,float]: balance, \"\"\" # year *(assumes one planning year intended), year = int(np.round(n.snapshots.year.values.mean(), 0)) # emissions from generators (from fneumann course) emissions = ( n.generators_t.p / n.generators.efficiency * n.generators.carrier.map(n.carriers.co2_emissions) ) # t/h emissions_carrier = ( (n.snapshot_weightings.generators @ emissions).groupby(n.generators.carrier).sum() ) # format and drop 0 values emissions_carrier = emissions_carrier.where(emissions_carrier > 0).dropna() emissions_carrier.rename(year, inplace=True) emissions_carrier = emissions_carrier.to_frame() # CO2 withdrawal stores = n.stores_t.e.T.groupby(n.stores.carrier).sum() co2_stores = stores.index.intersection(withdrawal_stores) co2_withdrawal = stores.iloc[:, -1].loc[co2_stores] * -1 co2_withdrawal.rename(year, inplace=True) co2_withdrawal = co2_withdrawal.to_frame() year_balance = pd.concat([emissions_carrier, co2_withdrawal]) # combine with previous co2_balance = co2_balance.reindex(year_balance.index.union(co2_balance.index)) co2_balance.loc[year_balance.index, label] = year_balance[year] return co2_balance","title":"calculate_co2_balance"},{"location":"docs/reference/make_summary/#make_summary.calculate_peak_dispatch","text":"Calculate the MAX dispatch of each component at the buses aggregated by carrier. Parameters: n ( Network ) \u2013 the network object label ( str ) \u2013 the labe representing the pathway supply ( DataFrame ) \u2013 supply energy balance (empty df) Returns: DataFrame \u2013 pd.DataFrame: updated supply DF Source code in workflow/scripts/make_summary.py def calculate_peak_dispatch(n: pypsa.Network, label: str, supply: pd.DataFrame) -> pd.DataFrame: \"\"\"Calculate the MAX dispatch of each component at the buses aggregated by carrier. Args: n (pypsa.Network): the network object label (str): the labe representing the pathway supply (pd.DataFrame): supply energy balance (empty df) Returns: pd.DataFrame: updated supply DF \"\"\" sup_ = n.statistics.supply( groupby=pypsa.statistics.get_carrier_and_bus_carrier, aggregate_time=\"max\" ) supply_reordered = sup_.reorder_levels([2, 0, 1]) supply_reordered.sort_index(inplace=True) supply[label] = supply_reordered return supply","title":"calculate_peak_dispatch"},{"location":"docs/reference/make_summary/#make_summary.calculate_supply_energy","text":"Calculate the total energy supply/consuption of each component at the buses aggregated by carrier. Parameters: n ( Network ) \u2013 the network object label ( str ) \u2013 the labe representing the pathway supply_energy ( DataFrame ) \u2013 supply energy balance (empty df) Returns: DataFrame \u2013 pd.DataFrame: updated supply energy balance Source code in workflow/scripts/make_summary.py def calculate_supply_energy( n: pypsa.Network, label: str, supply_energy: pd.DataFrame ) -> pd.DataFrame: \"\"\"Calculate the total energy supply/consuption of each component at the buses aggregated by carrier. Args: n (pypsa.Network): the network object label (str): the labe representing the pathway supply_energy (pd.DataFrame): supply energy balance (empty df) Returns: pd.DataFrame: updated supply energy balance \"\"\" eb = n.statistics.energy_balance(groupby=pypsa.statistics.get_carrier_and_bus_carrier) # fragile eb_reordered = eb.reorder_levels([2, 0, 1]) eb_reordered.sort_index(inplace=True) eb_reordered.rename(index={\"AC\": \"transmission losses\"}, level=2, inplace=True) supply_energy[label] = eb_reordered return supply_energy","title":"calculate_supply_energy"},{"location":"docs/reference/make_summary/#make_summary.calculate_weighted_prices","text":"Demand-weighed prices for stores and loads. For stores if withdrawal is zero, use supply instead. Args: n (pypsa.Network): the network object label (str): the label representing the pathway (not needed, refactor) weighted_prices (pd.DataFrame): the dataframe to write to (not needed, refactor) Returns: DataFrame \u2013 pd.DataFrame: updated weighted_prices Source code in workflow/scripts/make_summary.py def calculate_weighted_prices( n: pypsa.Network, label: str, weighted_prices: pd.DataFrame ) -> pd.DataFrame: \"\"\"Demand-weighed prices for stores and loads. For stores if withdrawal is zero, use supply instead. Args: n (pypsa.Network): the network object label (str): the label representing the pathway (not needed, refactor) weighted_prices (pd.DataFrame): the dataframe to write to (not needed, refactor) Returns: pd.DataFrame: updated weighted_prices \"\"\" entries = pd.Index([\"electricity\", \"heat\", \"H2\", \"CO2 capture\", \"gas\", \"biomass\"]) weighted_prices = weighted_prices.reindex(entries) # loads loads = ( n.statistics.revenue(comps=\"Load\", groupby=pypsa.statistics.get_bus_carrier) / n.statistics.withdrawal(comps=\"Load\", groupby=pypsa.statistics.get_bus_carrier) * -1 ) loads.rename(index={\"AC\": \"electricity\"}, inplace=True) # stores w = n.statistics.withdrawal(comps=\"Store\") # biomass stores have no withdrawal for some reason w[w == 0] = n.statistics.supply(comps=\"Store\")[w == 0] weighted_prices[label] = pd.concat([loads, n.statistics.revenue(comps=\"Store\") / w]) return weighted_prices","title":"calculate_weighted_prices"},{"location":"docs/reference/plot_heatmap/","text":"","title":"plot_heatmap"},{"location":"docs/reference/plot_network/","text":"add_cost_pannel(df, fig, preferred_order, tech_colors, plot_additions, ax_loc=[-0.09, 0.28, 0.09, 0.45]) Add a cost pannel to the figure Parameters: df ( DataFrame ) \u2013 the cost data to plot fig ( Figure ) \u2013 the figure object to which the cost pannel will be added preferred_order ( Index ) \u2013 index, the order in whiich to plot tech_colors ( dict ) \u2013 the tech colors plot_additions ( bool ) \u2013 plot the additions ax_loc ( list , default: [-0.09, 0.28, 0.09, 0.45] ) \u2013 the location of the cost pannel. Defaults to [-0.09, 0.28, 0.09, 0.45]. Source code in workflow/scripts/plot_network.py def add_cost_pannel( df: pd.DataFrame, fig: plt.Figure, preferred_order: pd.Index, tech_colors: dict, plot_additions: bool, ax_loc=[-0.09, 0.28, 0.09, 0.45], ) -> None: \"\"\"Add a cost pannel to the figure Args: df (pd.DataFrame): the cost data to plot fig (plt.Figure): the figure object to which the cost pannel will be added preferred_order (pd.Index): index, the order in whiich to plot tech_colors (dict): the tech colors plot_additions (bool): plot the additions ax_loc (list, optional): the location of the cost pannel. Defaults to [-0.09, 0.28, 0.09, 0.45]. \"\"\" ax3 = fig.add_axes(ax_loc) reordered = preferred_order.intersection(df.index).append(df.index.difference(preferred_order)) colors = {k.lower(): v for k, v in tech_colors.items()} df.loc[reordered, df.columns].T.plot( kind=\"bar\", ax=ax3, stacked=True, color=[colors[k.lower()] for k in reordered], ) ax3.legend().remove() ax3.set_ylabel(\"annualized system cost bEUR/a\") ax3.set_xticklabels(ax3.get_xticklabels(), rotation=\"horizontal\") ax3.grid(axis=\"y\") ax3.set_ylim([0, df.sum().max() * 1.1]) if plot_additions: # add label percent = np.round((df.sum()[\"added\"] / df.sum()[\"total\"]) * 100) ax3.text(0.85, (df.sum()[\"added\"] + 15), str(percent) + \"%\", color=\"black\") fig.tight_layout() add_energy_pannel(df, fig, preferred_order, colors, ax_loc=[-0.09, 0.28, 0.09, 0.45]) Add a cost pannel to the figure Parameters: df ( DataFrame ) \u2013 the statistics supply output by carrier (from plot_energy map) fig ( Figure ) \u2013 the figure object to which the cost pannel will be added preferred_order ( Index ) \u2013 index, the order in whiich to plot colors ( Series ) \u2013 the colors for the techs, with the correct index and no extra techs ax_loc ( list , default: [-0.09, 0.28, 0.09, 0.45] ) \u2013 the pannel location. Defaults to [-0.09, 0.28, 0.09, 0.45]. Source code in workflow/scripts/plot_network.py def add_energy_pannel( df: pd.DataFrame, fig: plt.Figure, preferred_order: pd.Index, colors: pd.Series, ax_loc=[-0.09, 0.28, 0.09, 0.45], ) -> None: \"\"\"Add a cost pannel to the figure Args: df (pd.DataFrame): the statistics supply output by carrier (from plot_energy map) fig (plt.Figure): the figure object to which the cost pannel will be added preferred_order (pd.Index): index, the order in whiich to plot colors (pd.Series): the colors for the techs, with the correct index and no extra techs ax_loc (list, optional): the pannel location. Defaults to [-0.09, 0.28, 0.09, 0.45]. \"\"\" ax3 = fig.add_axes(ax_loc) reordered = preferred_order.intersection(df.index).append(df.index.difference(preferred_order)) df = df / PLOT_SUPPLY_UNITS # only works if colors has correct index df.loc[reordered, df.columns].T.plot( kind=\"bar\", ax=ax3, stacked=True, color=colors[reordered], ) ax3.legend().remove() ax3.set_ylabel(\"Electricity supply [TWh]\") ax3.set_xticklabels(ax3.get_xticklabels(), rotation=\"horizontal\") ax3.grid(axis=\"y\") ax3.set_ylim([0, df.sum().max() * 1.1]) fig.tight_layout() plot_cost_map(network, opts, base_year=2020, plot_additions=True, capex_only=False, cost_pannel=True, save_path=None) Plot the cost of each node on a map as well as the line capacities Parameters: network ( Network ) \u2013 the network object opts ( dict ) \u2013 the plotting config (snakemake.config[\"plotting\"]) base_year ( int , default: 2020 ) \u2013 the base year (for cost delta). Defaults to 2020. capex_only ( bool , default: False ) \u2013 do not plot VOM (FOM is in CAPEX). Defaults to False. plot_additions ( bool , default: True ) \u2013 plot a map of investments (p_nom_opt vs p_nom). Defaults to True. cost_pannel ( bool , default: True ) \u2013 add a bar graph with costs. Defaults to True. save_path ( PathLike , default: None ) \u2013 save figure to path (or not if None). Defaults to None. raises: ValueError: if plot_additions and not capex_only Source code in workflow/scripts/plot_network.py def plot_cost_map( network: pypsa.Network, opts: dict, base_year=2020, plot_additions=True, capex_only=False, cost_pannel=True, save_path: os.PathLike = None, ): \"\"\"Plot the cost of each node on a map as well as the line capacities Args: network (pypsa.Network): the network object opts (dict): the plotting config (snakemake.config[\"plotting\"]) base_year (int, optional): the base year (for cost delta). Defaults to 2020. capex_only (bool, optional): do not plot VOM (FOM is in CAPEX). Defaults to False. plot_additions (bool, optional): plot a map of investments (p_nom_opt vs p_nom). Defaults to True. cost_pannel (bool, optional): add a bar graph with costs. Defaults to True. save_path (os.PathLike, optional): save figure to path (or not if None). Defaults to None. raises: ValueError: if plot_additions and not capex_only \"\"\" if plot_additions and not capex_only: raise ValueError(\"Cannot plot additions without capex only\") tech_colors = make_nice_tech_colors(opts[\"tech_colors\"], opts[\"nice_names\"]) # TODO scale edges by cost from capex summary def calc_link_plot_width(row, carrier=\"AC\", additions=False): if row.length == 0 or row.carrier != carrier or not row.plottable: return 0 elif additions: return row.p_nom else: return row.p_nom_opt # ============ === Stats by bus === # calc costs & sum over component types to keep bus & carrier (remove no loc) costs = network.statistics.capex(groupby=[\"location\", \"carrier\"]) costs = costs.groupby(level=[1, 2]).sum().drop(\"\") # we miss some buses by grouping epr location, fill w 0s bus_idx = pd.MultiIndex.from_product([network.buses.index, [\"AC\"]]) costs = costs.reindex(bus_idx.union(costs.index), fill_value=0) # add marginal (excluding quasi fixed) to costs if desired if not capex_only: opex = network.statistics.opex(groupby=[\"location\", \"carrier\"]) opex = opex.groupby(level=[1, 2]).sum() cost_pies = costs + opex.reindex(costs.index, fill_value=0) # === make map components: pies and edges cost_pies = costs.fillna(0) cost_pies.index.names = [\"bus\", \"carrier\"] carriers = cost_pies.index.get_level_values(1).unique() # map edges link_plot_w = network.links.apply(lambda row: calc_link_plot_width(row, carrier=\"AC\"), axis=1) edges = pd.concat([network.lines.s_nom_opt, link_plot_w]).groupby(level=0).sum() line_lower_threshold = opts.get(\"min_edge_capacity\", 0) edge_widths = edges.clip(line_lower_threshold, edges.max()).replace(line_lower_threshold, 0) # === Additions === # for pathways sometimes interested in additions from last time step if plot_additions: installed = ( network.statistics.installed_capex(groupby=[\"location\", \"carrier\"]) .groupby(level=[1, 2]) .sum() ) costs_additional = costs - installed.reindex(costs.index, fill_value=0) cost_pies_additional = costs_additional.fillna(0) cost_pies_additional.index.names = [\"bus\", \"carrier\"] link_additions = network.links.apply( lambda row: calc_link_plot_width(row, carrier=\"AC\", additions=True), axis=1 ) added_links = link_plot_w - link_additions.reindex(link_plot_w.index, fill_value=0) added_lines = network.lines.s_nom_opt - network.lines.s_nom.reindex( network.lines.index, fill_value=0 ) edge_widths_added = pd.concat([added_links, added_lines]).groupby(level=0).sum() # add to carrier types carriers = carriers.union(cost_pies_additional.index.get_level_values(1).unique()) preferred_order = pd.Index(opts[\"preferred_order\"]) carriers = carriers.tolist() # Make figure with right number of pannels if plot_additions: fig, (ax1, ax2) = plt.subplots(1, 2, subplot_kw={\"projection\": ccrs.PlateCarree()}) fig.set_size_inches(opts[\"cost_map\"][\"figsize_w_additions\"]) else: fig, ax1 = plt.subplots(subplot_kw={\"projection\": ccrs.PlateCarree()}) fig.set_size_inches(opts[\"cost_map\"][\"figsize\"]) # Add the total costs bus_size_factor = opts[\"cost_map\"][\"bus_size_factor\"] linewidth_factor = opts[\"cost_map\"][\"linewidth_factor\"] plot_map( network, tech_colors=tech_colors, edge_widths=edge_widths / linewidth_factor, bus_colors=tech_colors, bus_sizes=cost_pies / bus_size_factor, edge_colors=opts[\"cost_map\"][\"edge_color\"], ax=ax1, add_legend=not plot_additions, bus_ref_title=f\"System costs{' (CAPEX)'if capex_only else ''}\", **opts[\"cost_map\"], ) # TODO check edges is working # Add the added pathway costs if plot_additions: plot_map( network, tech_colors=tech_colors, edge_widths=edge_widths_added / linewidth_factor, bus_colors=tech_colors, bus_sizes=cost_pies_additional / bus_size_factor, edge_colors=\"rosybrown\", ax=ax2, bus_ref_title=f\"Added costs{' (CAPEX)' if capex_only else ''}\", add_legend=True, **opts[\"cost_map\"], ) # Add the optional cost pannel if cost_pannel: df = pd.DataFrame(columns=[\"total\"]) df[\"total\"] = network.statistics.capex(nice_names=False).groupby(level=1).sum() if not capex_only: df[\"opex\"] = network.statistics.opex(nice_names=False).groupby(level=1).sum() df.rename(columns={\"total\": \"capex\"}) elif plot_additions: df[\"added\"] = ( df[\"total\"] - network.statistics.installed_capex(nice_names=False).groupby(level=1).sum() ) df.fillna(0, inplace=True) df = df / PLOT_COST_UNITS # TODO decide discount # df = df / (1 + discount_rate) ** (int(planning_horizon) - base_year) add_cost_pannel( df, fig, preferred_order, tech_colors, plot_additions, ax_loc=[-0.09, 0.28, 0.09, 0.45] ) fig.set_size_inches(opts[\"cost_map\"][f\"figsize{'_w_additions' if plot_additions else ''}\"]) fig.tight_layout() if save_path: fig.savefig(save_path, transparent=False, bbox_inches=\"tight\") plot_energy_map(network, opts, energy_pannel=True, save_path=None, carrier='AC', plot_ac_imports=False, components=['Generator', 'Link']) A map plot of energy, either AC or heat Parameters: network ( Network ) \u2013 the pyPSA network object opts ( dict ) \u2013 the plotting options (snakemake.config[\"plotting\"]) energy_pannel ( bool , default: True ) \u2013 add an anergy pie to the left. Defaults to True. save_path ( PathLike , default: None ) \u2013 Fig outp path. Defaults to None (no save). carrier ( str , default: 'AC' ) \u2013 the energy carrier. Defaults to \"AC\". plot_ac_imports ( bool , default: False ) \u2013 plot electricity imports. Defaults to False. components ( list , default: ['Generator', 'Link'] ) \u2013 the components to plot. Defaults to [\"Generator\", \"Link\"]. raises: ValueError: if carrier is not AC or heat Source code in workflow/scripts/plot_network.py def plot_energy_map( network: pypsa.Network, opts: dict, energy_pannel=True, save_path: os.PathLike = None, carrier=\"AC\", plot_ac_imports=False, components=[\"Generator\", \"Link\"], ): \"\"\"A map plot of energy, either AC or heat Args: network (pypsa.Network): the pyPSA network object opts (dict): the plotting options (snakemake.config[\"plotting\"]) energy_pannel (bool, optional): add an anergy pie to the left. Defaults to True. save_path (os.PathLike, optional): Fig outp path. Defaults to None (no save). carrier (str, optional): the energy carrier. Defaults to \"AC\". plot_ac_imports (bool, optional): plot electricity imports. Defaults to False. components (list, optional): the components to plot. Defaults to [\"Generator\", \"Link\"]. raises: ValueError: if carrier is not AC or heat \"\"\" if carrier not in [\"AC\", \"heat\"]: raise ValueError(\"Carrier must be either 'AC' or 'heat'\") # make the statistics. Buses not assigned to a region will be included # if they are linked to a region (e.g. turbine link w carrier = hydroelectricity) energy_supply = network.statistics.supply( groupby=get_location_and_carrier, bus_carrier=carrier, comps=components, ) # get rid of components supply_pies = energy_supply.groupby(level=[1, 2]).sum() # TODO fix this for heat # # calc costs & sum over component types to keep bus & carrier (remove no loc) # energy_supply = network.statistics.capex(groupby=[\"location\", \"carrier\"]) # energy_supply = energy_supply.groupby(level=[1, 2]).sum().drop(\"\") # # we miss some buses by grouping epr location, fill w 0s # bus_idx = pd.MultiIndex.from_product([network.buses.index, [\"AC\"]]) # supply_pies = energy_supply.reindex(bus_idx.union(energy_supply.index), fill_value=0) # remove imports from supply pies if carrier == \"AC\" and not plot_ac_imports: supply_pies = supply_pies.loc[supply_pies.index.get_level_values(1) != \"AC\"] # TODO aggregate costs below threshold into \"other\" -> requires messing with network # network.add(\"Carrier\", \"Other\") # get all carrier types carriers_list = supply_pies.index.get_level_values(1).unique() carriers_list = carriers_list.tolist() # TODO make line handling nicer line_lower_threshold = opts.get(\"min_edge_capacity\", 500) # Make figur fig, ax = plt.subplots(subplot_kw={\"projection\": ccrs.PlateCarree()}) fig.set_size_inches(opts[\"energy_map\"][\"figsize\"]) # get colors bus_colors = network.carriers.loc[network.carriers.nice_name.isin(carriers_list), \"color\"] bus_colors.rename(opts[\"nice_names\"], inplace=True) preferred_order = pd.Index(opts[\"preferred_order\"]) reordered = preferred_order.intersection(bus_colors.index).append( bus_colors.index.difference(preferred_order) ) # TODO there'sa problem with network colors when using heat, pies aren't grouped by location colors = network.carriers.color.copy() colors.index = colors.index.map(opts[\"nice_names\"]) tech_colors = make_nice_tech_colors(opts[\"tech_colors\"], opts[\"nice_names\"]) # make sure plot isnt overpopulated def calc_link_plot_width(row, carrier=\"AC\"): if row.length == 0 or row.carrier != carrier or not row.plottable: return 0 else: return row.p_nom_opt edge_carrier = \"H2 pipeline\" if carrier == \"heat\" else \"AC\" link_plot_w = network.links.apply(lambda row: calc_link_plot_width(row, edge_carrier), axis=1) edges = pd.concat([network.lines.s_nom_opt, link_plot_w]) edge_widths = edges.clip(line_lower_threshold, edges.max()).replace(line_lower_threshold, 0) opts_plot = opts[\"energy_map\"].copy() if carrier == \"heat\": opts_plot[\"ref_bus_sizes\"] = opts_plot[\"ref_bus_sizes_heat\"] opts_plot[\"ref_edge_sizes\"] = opts_plot[\"ref_edge_sizes_heat\"] opts_plot[\"linewidth_factor\"] = opts_plot[\"linewidth_factor_heat\"] opts_plot[\"bus_size_factor\"] = opts_plot[\"bus_size_factor_heat\"] plot_map( network, tech_colors=tech_colors, # colors.to_dict(), edge_widths=edge_widths / opts_plot[\"linewidth_factor\"], bus_colors=bus_colors.loc[reordered], bus_sizes=supply_pies / opts_plot[\"bus_size_factor\"], edge_colors=opts_plot[\"edge_color\"], ax=ax, edge_unit_conv=PLOT_CAP_UNITS, bus_unit_conv=PLOT_SUPPLY_UNITS, add_legend=True, **opts_plot, ) # # Add the optional cost pannel if energy_pannel: df = supply_pies.groupby(level=1).sum().to_frame() df = df.fillna(0) add_energy_pannel(df, fig, preferred_order, bus_colors, ax_loc=[-0.09, 0.28, 0.09, 0.45]) handles, labels = ax.get_legend_handles_labels() ax.legend(handles, labels, ncol=1, bbox_to_anchor=[1, 1], loc=\"upper left\") fig.tight_layout() if save_path: fig.savefig(save_path, transparent=True, bbox_inches=\"tight\") plot_map(network, tech_colors, edge_widths, bus_colors, bus_sizes, edge_colors='black', add_ref_edge_sizes=True, add_ref_bus_sizes=True, add_legend=True, bus_unit_conv=PLOT_COST_UNITS, edge_unit_conv=PLOT_CAP_UNITS, ax=None, **kwargs) Plot the network on a map Parameters: network ( Network ) \u2013 the pypsa network (filtered to contain only relevant buses & links) tech_colors ( dict ) \u2013 config mapping edge_colors ( Series | str , default: 'black' ) \u2013 the series of edge colors edge_widths ( Series ) \u2013 the edge widths bus_colors ( Series ) \u2013 the series of bus colors bus_sizes ( Series ) \u2013 the series of bus sizes add_ref_edge_sizes ( bool , default: True ) \u2013 add reference line sizes in legend (requires edge_colors=True). Defaults to True. add_ref_bus_sizes ( bool , default: True ) \u2013 add reference bus sizes in legend. Defaults to True. ax ( Axes , default: None ) \u2013 the plotting ax. Defaults to None (new figure). Source code in workflow/scripts/plot_network.py def plot_map( network: pypsa.Network, tech_colors: dict, edge_widths: pd.Series, bus_colors: pd.Series, bus_sizes: pd.Series, edge_colors: pd.Series | str = \"black\", add_ref_edge_sizes=True, add_ref_bus_sizes=True, add_legend=True, bus_unit_conv=PLOT_COST_UNITS, edge_unit_conv=PLOT_CAP_UNITS, ax=None, **kwargs, ) -> plt.Axes: \"\"\"Plot the network on a map Args: network (pypsa.Network): the pypsa network (filtered to contain only relevant buses & links) tech_colors (dict): config mapping edge_colors (pd.Series|str): the series of edge colors edge_widths (pd.Series): the edge widths bus_colors (pd.Series): the series of bus colors bus_sizes (pd.Series): the series of bus sizes add_ref_edge_sizes (bool, optional): add reference line sizes in legend (requires edge_colors=True). Defaults to True. add_ref_bus_sizes (bool, optional): add reference bus sizes in legend. Defaults to True. ax (plt.Axes, optional): the plotting ax. Defaults to None (new figure). \"\"\" if not ax: fig, ax = plt.subplots() network.plot( bus_sizes=bus_sizes, bus_colors=bus_colors, line_colors=edge_colors, link_colors=edge_colors, line_widths=edge_widths, link_widths=edge_widths, ax=ax, color_geomap=True, boundaries=kwargs.get(\"boundaries\", None), ) ax.add_feature(cfeature.BORDERS, linewidth=0.5, edgecolor=\"gray\") states_provinces = cfeature.NaturalEarthFeature( category=\"cultural\", name=\"admin_1_states_provinces_lines\", scale=\"50m\", facecolor=\"none\" ) # Add our states feature. ax.add_feature(states_provinces, edgecolor=\"lightgray\", alpha=0.7) if add_legend: carriers = bus_sizes.index.get_level_values(1).unique() colors = carriers.intersection(tech_colors).map(tech_colors).to_list() if isinstance(edge_colors, str): colors += [edge_colors] labels = carriers.to_list() + [\"HVDC or HVAC link\"] else: colors += edge_colors.values.to_list() labels = carriers.to_list() + edge_colors.index.to_list() leg_opt = {\"bbox_to_anchor\": (1.42, 1.04), \"frameon\": False} add_legend_patches(ax, colors, labels, legend_kw=leg_opt) if add_ref_edge_sizes & isinstance(edge_colors, str): ref_unit = kwargs.get(\"ref_edge_unit\", \"GW\") size_factor = float(kwargs.get(\"linewidth_factor\", 1e5)) ref_sizes = kwargs.get(\"ref_edge_sizes\", [1e5, 5e5]) labels = [f\"{float(s)/edge_unit_conv} {ref_unit}\" for s in ref_sizes] ref_sizes = list(map(lambda x: float(x) / size_factor, ref_sizes)) legend_kw = dict( loc=\"upper left\", bbox_to_anchor=(0.26, 1.0), frameon=False, labelspacing=0.8, handletextpad=2, title=kwargs.get(\"edge_ref_title\", \"Grid cap.\"), ) add_legend_lines( ax, ref_sizes, labels, patch_kw=dict(color=edge_colors), legend_kw=legend_kw ) # add reference bus sizes ferom the units if add_ref_bus_sizes: ref_unit = kwargs.get(\"ref_bus_unit\", \"bEUR/a\") size_factor = float(kwargs.get(\"bus_size_factor\", 1e10)) ref_sizes = kwargs.get(\"ref_bus_sizes\", [2e10, 1e10, 5e10]) labels = [f\"{float(s)/bus_unit_conv:.0f} {ref_unit}\" for s in ref_sizes] ref_sizes = list(map(lambda x: float(x) / size_factor, ref_sizes)) legend_kw = { \"loc\": \"upper left\", \"bbox_to_anchor\": (0.0, 1.0), \"labelspacing\": 0.8, \"frameon\": False, \"handletextpad\": 0, \"title\": kwargs.get(\"bus_ref_title\", \"UNDEFINED TITLE\"), } add_legend_circles( ax, ref_sizes, labels, srid=network.srid, patch_kw=dict(facecolor=\"lightgrey\"), legend_kw=legend_kw, ) return ax plot_nodal_prices(network, opts, carrier='AC', save_path=None) A map plot of energy, either AC or heat Parameters: network ( Network ) \u2013 the pyPSA network object opts ( dict ) \u2013 the plotting options (snakemake.config[\"plotting\"]) save_path ( PathLike , default: None ) \u2013 Fig outp path. Defaults to None (no save). carrier ( str , default: 'AC' ) \u2013 the energy carrier. Defaults to \"AC\". raises: ValueError: if carrier is not AC or heat Source code in workflow/scripts/plot_network.py def plot_nodal_prices( network: pypsa.Network, opts: dict, carrier=\"AC\", save_path: os.PathLike = None, ): \"\"\"A map plot of energy, either AC or heat Args: network (pypsa.Network): the pyPSA network object opts (dict): the plotting options (snakemake.config[\"plotting\"]) save_path (os.PathLike, optional): Fig outp path. Defaults to None (no save). carrier (str, optional): the energy carrier. Defaults to \"AC\". raises: ValueError: if carrier is not AC or heat \"\"\" if carrier not in [\"AC\", \"heat\"]: raise ValueError(\"Carrier must be either 'AC' or 'heat'\") # demand weighed prices per node nodal_prices = ( network.statistics.revenue( groupby=pypsa.statistics.get_bus_and_carrier_and_bus_carrier, comps=\"Load\", bus_carrier=carrier, ) / network.statistics.withdrawal( comps=\"Load\", groupby=pypsa.statistics.get_bus_and_carrier_and_bus_carrier, bus_carrier=carrier, ) * -1 ) # drop the carrier and bus_carrier, map to colors nodal_prices = nodal_prices.droplevel(1).droplevel(1) norm = plt.Normalize(vmin=nodal_prices.min(), vmax=nodal_prices.max()) cmap = plt.get_cmap(\"plasma\") bus_colors = nodal_prices.map(lambda x: cmap(norm(x))) energy_consum = network.statistics.withdrawal( groupby=pypsa.statistics.get_bus_and_carrier, bus_carrier=carrier, comps=[\"Load\"], ) consum_pies = energy_consum.groupby(level=1).sum() # Make figure fig, ax = plt.subplots(subplot_kw={\"projection\": ccrs.PlateCarree()}) fig.set_size_inches(opts[\"price_map\"][\"figsize\"]) # get colors # TODO make line handling nicer # make sure plot isnt overpopulated def calc_plot_width(row, carrier=\"AC\"): if row.length == 0: return 0 elif row.carrier != carrier: return 0 else: return row.p_nom_opt line_lower_threshold = opts.get(\"min_edge_capacity\", 500) edge_carrier = \"H2\" if carrier == \"heat\" else \"AC\" link_plot_w = network.links.apply(lambda row: calc_plot_width(row, edge_carrier), axis=1) edges = pd.concat([network.lines.s_nom_opt, link_plot_w]) edge_widths = edges.clip(line_lower_threshold, edges.max()).replace(line_lower_threshold, 0) bus_size_factor = opts[\"price_map\"][\"bus_size_factor\"] linewidth_factor = opts[\"price_map\"][f\"linewidth_factor{\"_heat\" if carrier == 'heat' else ''}\"] plot_map( network, tech_colors=None, edge_widths=edge_widths / linewidth_factor, bus_colors=bus_colors, bus_sizes=consum_pies / bus_size_factor, edge_colors=opts[\"price_map\"][\"edge_color\"], ax=ax, edge_unit_conv=PLOT_CAP_UNITS, bus_unit_conv=PLOT_SUPPLY_UNITS, add_legend=False, **opts[\"price_map\"], ) # Add colorbar based on bus_colors # fig.tight_layout() fig.subplots_adjust(right=0.85) cax = fig.add_axes([0.87, ax.get_position().y0, 0.02, ax.get_position().height]) sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm) sm.set_array([]) cbar = plt.colorbar(sm, cax=cax, orientation=\"vertical\") cbar.set_label(f\"Nodal Prices ${CURRENCY}/MWh\") if save_path: fig.savefig(save_path, transparent=True, bbox_inches=\"tight\")","title":"plot_network"},{"location":"docs/reference/plot_network/#plot_network.add_cost_pannel","text":"Add a cost pannel to the figure Parameters: df ( DataFrame ) \u2013 the cost data to plot fig ( Figure ) \u2013 the figure object to which the cost pannel will be added preferred_order ( Index ) \u2013 index, the order in whiich to plot tech_colors ( dict ) \u2013 the tech colors plot_additions ( bool ) \u2013 plot the additions ax_loc ( list , default: [-0.09, 0.28, 0.09, 0.45] ) \u2013 the location of the cost pannel. Defaults to [-0.09, 0.28, 0.09, 0.45]. Source code in workflow/scripts/plot_network.py def add_cost_pannel( df: pd.DataFrame, fig: plt.Figure, preferred_order: pd.Index, tech_colors: dict, plot_additions: bool, ax_loc=[-0.09, 0.28, 0.09, 0.45], ) -> None: \"\"\"Add a cost pannel to the figure Args: df (pd.DataFrame): the cost data to plot fig (plt.Figure): the figure object to which the cost pannel will be added preferred_order (pd.Index): index, the order in whiich to plot tech_colors (dict): the tech colors plot_additions (bool): plot the additions ax_loc (list, optional): the location of the cost pannel. Defaults to [-0.09, 0.28, 0.09, 0.45]. \"\"\" ax3 = fig.add_axes(ax_loc) reordered = preferred_order.intersection(df.index).append(df.index.difference(preferred_order)) colors = {k.lower(): v for k, v in tech_colors.items()} df.loc[reordered, df.columns].T.plot( kind=\"bar\", ax=ax3, stacked=True, color=[colors[k.lower()] for k in reordered], ) ax3.legend().remove() ax3.set_ylabel(\"annualized system cost bEUR/a\") ax3.set_xticklabels(ax3.get_xticklabels(), rotation=\"horizontal\") ax3.grid(axis=\"y\") ax3.set_ylim([0, df.sum().max() * 1.1]) if plot_additions: # add label percent = np.round((df.sum()[\"added\"] / df.sum()[\"total\"]) * 100) ax3.text(0.85, (df.sum()[\"added\"] + 15), str(percent) + \"%\", color=\"black\") fig.tight_layout()","title":"add_cost_pannel"},{"location":"docs/reference/plot_network/#plot_network.add_energy_pannel","text":"Add a cost pannel to the figure Parameters: df ( DataFrame ) \u2013 the statistics supply output by carrier (from plot_energy map) fig ( Figure ) \u2013 the figure object to which the cost pannel will be added preferred_order ( Index ) \u2013 index, the order in whiich to plot colors ( Series ) \u2013 the colors for the techs, with the correct index and no extra techs ax_loc ( list , default: [-0.09, 0.28, 0.09, 0.45] ) \u2013 the pannel location. Defaults to [-0.09, 0.28, 0.09, 0.45]. Source code in workflow/scripts/plot_network.py def add_energy_pannel( df: pd.DataFrame, fig: plt.Figure, preferred_order: pd.Index, colors: pd.Series, ax_loc=[-0.09, 0.28, 0.09, 0.45], ) -> None: \"\"\"Add a cost pannel to the figure Args: df (pd.DataFrame): the statistics supply output by carrier (from plot_energy map) fig (plt.Figure): the figure object to which the cost pannel will be added preferred_order (pd.Index): index, the order in whiich to plot colors (pd.Series): the colors for the techs, with the correct index and no extra techs ax_loc (list, optional): the pannel location. Defaults to [-0.09, 0.28, 0.09, 0.45]. \"\"\" ax3 = fig.add_axes(ax_loc) reordered = preferred_order.intersection(df.index).append(df.index.difference(preferred_order)) df = df / PLOT_SUPPLY_UNITS # only works if colors has correct index df.loc[reordered, df.columns].T.plot( kind=\"bar\", ax=ax3, stacked=True, color=colors[reordered], ) ax3.legend().remove() ax3.set_ylabel(\"Electricity supply [TWh]\") ax3.set_xticklabels(ax3.get_xticklabels(), rotation=\"horizontal\") ax3.grid(axis=\"y\") ax3.set_ylim([0, df.sum().max() * 1.1]) fig.tight_layout()","title":"add_energy_pannel"},{"location":"docs/reference/plot_network/#plot_network.plot_cost_map","text":"Plot the cost of each node on a map as well as the line capacities Parameters: network ( Network ) \u2013 the network object opts ( dict ) \u2013 the plotting config (snakemake.config[\"plotting\"]) base_year ( int , default: 2020 ) \u2013 the base year (for cost delta). Defaults to 2020. capex_only ( bool , default: False ) \u2013 do not plot VOM (FOM is in CAPEX). Defaults to False. plot_additions ( bool , default: True ) \u2013 plot a map of investments (p_nom_opt vs p_nom). Defaults to True. cost_pannel ( bool , default: True ) \u2013 add a bar graph with costs. Defaults to True. save_path ( PathLike , default: None ) \u2013 save figure to path (or not if None). Defaults to None. raises: ValueError: if plot_additions and not capex_only Source code in workflow/scripts/plot_network.py def plot_cost_map( network: pypsa.Network, opts: dict, base_year=2020, plot_additions=True, capex_only=False, cost_pannel=True, save_path: os.PathLike = None, ): \"\"\"Plot the cost of each node on a map as well as the line capacities Args: network (pypsa.Network): the network object opts (dict): the plotting config (snakemake.config[\"plotting\"]) base_year (int, optional): the base year (for cost delta). Defaults to 2020. capex_only (bool, optional): do not plot VOM (FOM is in CAPEX). Defaults to False. plot_additions (bool, optional): plot a map of investments (p_nom_opt vs p_nom). Defaults to True. cost_pannel (bool, optional): add a bar graph with costs. Defaults to True. save_path (os.PathLike, optional): save figure to path (or not if None). Defaults to None. raises: ValueError: if plot_additions and not capex_only \"\"\" if plot_additions and not capex_only: raise ValueError(\"Cannot plot additions without capex only\") tech_colors = make_nice_tech_colors(opts[\"tech_colors\"], opts[\"nice_names\"]) # TODO scale edges by cost from capex summary def calc_link_plot_width(row, carrier=\"AC\", additions=False): if row.length == 0 or row.carrier != carrier or not row.plottable: return 0 elif additions: return row.p_nom else: return row.p_nom_opt # ============ === Stats by bus === # calc costs & sum over component types to keep bus & carrier (remove no loc) costs = network.statistics.capex(groupby=[\"location\", \"carrier\"]) costs = costs.groupby(level=[1, 2]).sum().drop(\"\") # we miss some buses by grouping epr location, fill w 0s bus_idx = pd.MultiIndex.from_product([network.buses.index, [\"AC\"]]) costs = costs.reindex(bus_idx.union(costs.index), fill_value=0) # add marginal (excluding quasi fixed) to costs if desired if not capex_only: opex = network.statistics.opex(groupby=[\"location\", \"carrier\"]) opex = opex.groupby(level=[1, 2]).sum() cost_pies = costs + opex.reindex(costs.index, fill_value=0) # === make map components: pies and edges cost_pies = costs.fillna(0) cost_pies.index.names = [\"bus\", \"carrier\"] carriers = cost_pies.index.get_level_values(1).unique() # map edges link_plot_w = network.links.apply(lambda row: calc_link_plot_width(row, carrier=\"AC\"), axis=1) edges = pd.concat([network.lines.s_nom_opt, link_plot_w]).groupby(level=0).sum() line_lower_threshold = opts.get(\"min_edge_capacity\", 0) edge_widths = edges.clip(line_lower_threshold, edges.max()).replace(line_lower_threshold, 0) # === Additions === # for pathways sometimes interested in additions from last time step if plot_additions: installed = ( network.statistics.installed_capex(groupby=[\"location\", \"carrier\"]) .groupby(level=[1, 2]) .sum() ) costs_additional = costs - installed.reindex(costs.index, fill_value=0) cost_pies_additional = costs_additional.fillna(0) cost_pies_additional.index.names = [\"bus\", \"carrier\"] link_additions = network.links.apply( lambda row: calc_link_plot_width(row, carrier=\"AC\", additions=True), axis=1 ) added_links = link_plot_w - link_additions.reindex(link_plot_w.index, fill_value=0) added_lines = network.lines.s_nom_opt - network.lines.s_nom.reindex( network.lines.index, fill_value=0 ) edge_widths_added = pd.concat([added_links, added_lines]).groupby(level=0).sum() # add to carrier types carriers = carriers.union(cost_pies_additional.index.get_level_values(1).unique()) preferred_order = pd.Index(opts[\"preferred_order\"]) carriers = carriers.tolist() # Make figure with right number of pannels if plot_additions: fig, (ax1, ax2) = plt.subplots(1, 2, subplot_kw={\"projection\": ccrs.PlateCarree()}) fig.set_size_inches(opts[\"cost_map\"][\"figsize_w_additions\"]) else: fig, ax1 = plt.subplots(subplot_kw={\"projection\": ccrs.PlateCarree()}) fig.set_size_inches(opts[\"cost_map\"][\"figsize\"]) # Add the total costs bus_size_factor = opts[\"cost_map\"][\"bus_size_factor\"] linewidth_factor = opts[\"cost_map\"][\"linewidth_factor\"] plot_map( network, tech_colors=tech_colors, edge_widths=edge_widths / linewidth_factor, bus_colors=tech_colors, bus_sizes=cost_pies / bus_size_factor, edge_colors=opts[\"cost_map\"][\"edge_color\"], ax=ax1, add_legend=not plot_additions, bus_ref_title=f\"System costs{' (CAPEX)'if capex_only else ''}\", **opts[\"cost_map\"], ) # TODO check edges is working # Add the added pathway costs if plot_additions: plot_map( network, tech_colors=tech_colors, edge_widths=edge_widths_added / linewidth_factor, bus_colors=tech_colors, bus_sizes=cost_pies_additional / bus_size_factor, edge_colors=\"rosybrown\", ax=ax2, bus_ref_title=f\"Added costs{' (CAPEX)' if capex_only else ''}\", add_legend=True, **opts[\"cost_map\"], ) # Add the optional cost pannel if cost_pannel: df = pd.DataFrame(columns=[\"total\"]) df[\"total\"] = network.statistics.capex(nice_names=False).groupby(level=1).sum() if not capex_only: df[\"opex\"] = network.statistics.opex(nice_names=False).groupby(level=1).sum() df.rename(columns={\"total\": \"capex\"}) elif plot_additions: df[\"added\"] = ( df[\"total\"] - network.statistics.installed_capex(nice_names=False).groupby(level=1).sum() ) df.fillna(0, inplace=True) df = df / PLOT_COST_UNITS # TODO decide discount # df = df / (1 + discount_rate) ** (int(planning_horizon) - base_year) add_cost_pannel( df, fig, preferred_order, tech_colors, plot_additions, ax_loc=[-0.09, 0.28, 0.09, 0.45] ) fig.set_size_inches(opts[\"cost_map\"][f\"figsize{'_w_additions' if plot_additions else ''}\"]) fig.tight_layout() if save_path: fig.savefig(save_path, transparent=False, bbox_inches=\"tight\")","title":"plot_cost_map"},{"location":"docs/reference/plot_network/#plot_network.plot_energy_map","text":"A map plot of energy, either AC or heat Parameters: network ( Network ) \u2013 the pyPSA network object opts ( dict ) \u2013 the plotting options (snakemake.config[\"plotting\"]) energy_pannel ( bool , default: True ) \u2013 add an anergy pie to the left. Defaults to True. save_path ( PathLike , default: None ) \u2013 Fig outp path. Defaults to None (no save). carrier ( str , default: 'AC' ) \u2013 the energy carrier. Defaults to \"AC\". plot_ac_imports ( bool , default: False ) \u2013 plot electricity imports. Defaults to False. components ( list , default: ['Generator', 'Link'] ) \u2013 the components to plot. Defaults to [\"Generator\", \"Link\"]. raises: ValueError: if carrier is not AC or heat Source code in workflow/scripts/plot_network.py def plot_energy_map( network: pypsa.Network, opts: dict, energy_pannel=True, save_path: os.PathLike = None, carrier=\"AC\", plot_ac_imports=False, components=[\"Generator\", \"Link\"], ): \"\"\"A map plot of energy, either AC or heat Args: network (pypsa.Network): the pyPSA network object opts (dict): the plotting options (snakemake.config[\"plotting\"]) energy_pannel (bool, optional): add an anergy pie to the left. Defaults to True. save_path (os.PathLike, optional): Fig outp path. Defaults to None (no save). carrier (str, optional): the energy carrier. Defaults to \"AC\". plot_ac_imports (bool, optional): plot electricity imports. Defaults to False. components (list, optional): the components to plot. Defaults to [\"Generator\", \"Link\"]. raises: ValueError: if carrier is not AC or heat \"\"\" if carrier not in [\"AC\", \"heat\"]: raise ValueError(\"Carrier must be either 'AC' or 'heat'\") # make the statistics. Buses not assigned to a region will be included # if they are linked to a region (e.g. turbine link w carrier = hydroelectricity) energy_supply = network.statistics.supply( groupby=get_location_and_carrier, bus_carrier=carrier, comps=components, ) # get rid of components supply_pies = energy_supply.groupby(level=[1, 2]).sum() # TODO fix this for heat # # calc costs & sum over component types to keep bus & carrier (remove no loc) # energy_supply = network.statistics.capex(groupby=[\"location\", \"carrier\"]) # energy_supply = energy_supply.groupby(level=[1, 2]).sum().drop(\"\") # # we miss some buses by grouping epr location, fill w 0s # bus_idx = pd.MultiIndex.from_product([network.buses.index, [\"AC\"]]) # supply_pies = energy_supply.reindex(bus_idx.union(energy_supply.index), fill_value=0) # remove imports from supply pies if carrier == \"AC\" and not plot_ac_imports: supply_pies = supply_pies.loc[supply_pies.index.get_level_values(1) != \"AC\"] # TODO aggregate costs below threshold into \"other\" -> requires messing with network # network.add(\"Carrier\", \"Other\") # get all carrier types carriers_list = supply_pies.index.get_level_values(1).unique() carriers_list = carriers_list.tolist() # TODO make line handling nicer line_lower_threshold = opts.get(\"min_edge_capacity\", 500) # Make figur fig, ax = plt.subplots(subplot_kw={\"projection\": ccrs.PlateCarree()}) fig.set_size_inches(opts[\"energy_map\"][\"figsize\"]) # get colors bus_colors = network.carriers.loc[network.carriers.nice_name.isin(carriers_list), \"color\"] bus_colors.rename(opts[\"nice_names\"], inplace=True) preferred_order = pd.Index(opts[\"preferred_order\"]) reordered = preferred_order.intersection(bus_colors.index).append( bus_colors.index.difference(preferred_order) ) # TODO there'sa problem with network colors when using heat, pies aren't grouped by location colors = network.carriers.color.copy() colors.index = colors.index.map(opts[\"nice_names\"]) tech_colors = make_nice_tech_colors(opts[\"tech_colors\"], opts[\"nice_names\"]) # make sure plot isnt overpopulated def calc_link_plot_width(row, carrier=\"AC\"): if row.length == 0 or row.carrier != carrier or not row.plottable: return 0 else: return row.p_nom_opt edge_carrier = \"H2 pipeline\" if carrier == \"heat\" else \"AC\" link_plot_w = network.links.apply(lambda row: calc_link_plot_width(row, edge_carrier), axis=1) edges = pd.concat([network.lines.s_nom_opt, link_plot_w]) edge_widths = edges.clip(line_lower_threshold, edges.max()).replace(line_lower_threshold, 0) opts_plot = opts[\"energy_map\"].copy() if carrier == \"heat\": opts_plot[\"ref_bus_sizes\"] = opts_plot[\"ref_bus_sizes_heat\"] opts_plot[\"ref_edge_sizes\"] = opts_plot[\"ref_edge_sizes_heat\"] opts_plot[\"linewidth_factor\"] = opts_plot[\"linewidth_factor_heat\"] opts_plot[\"bus_size_factor\"] = opts_plot[\"bus_size_factor_heat\"] plot_map( network, tech_colors=tech_colors, # colors.to_dict(), edge_widths=edge_widths / opts_plot[\"linewidth_factor\"], bus_colors=bus_colors.loc[reordered], bus_sizes=supply_pies / opts_plot[\"bus_size_factor\"], edge_colors=opts_plot[\"edge_color\"], ax=ax, edge_unit_conv=PLOT_CAP_UNITS, bus_unit_conv=PLOT_SUPPLY_UNITS, add_legend=True, **opts_plot, ) # # Add the optional cost pannel if energy_pannel: df = supply_pies.groupby(level=1).sum().to_frame() df = df.fillna(0) add_energy_pannel(df, fig, preferred_order, bus_colors, ax_loc=[-0.09, 0.28, 0.09, 0.45]) handles, labels = ax.get_legend_handles_labels() ax.legend(handles, labels, ncol=1, bbox_to_anchor=[1, 1], loc=\"upper left\") fig.tight_layout() if save_path: fig.savefig(save_path, transparent=True, bbox_inches=\"tight\")","title":"plot_energy_map"},{"location":"docs/reference/plot_network/#plot_network.plot_map","text":"Plot the network on a map Parameters: network ( Network ) \u2013 the pypsa network (filtered to contain only relevant buses & links) tech_colors ( dict ) \u2013 config mapping edge_colors ( Series | str , default: 'black' ) \u2013 the series of edge colors edge_widths ( Series ) \u2013 the edge widths bus_colors ( Series ) \u2013 the series of bus colors bus_sizes ( Series ) \u2013 the series of bus sizes add_ref_edge_sizes ( bool , default: True ) \u2013 add reference line sizes in legend (requires edge_colors=True). Defaults to True. add_ref_bus_sizes ( bool , default: True ) \u2013 add reference bus sizes in legend. Defaults to True. ax ( Axes , default: None ) \u2013 the plotting ax. Defaults to None (new figure). Source code in workflow/scripts/plot_network.py def plot_map( network: pypsa.Network, tech_colors: dict, edge_widths: pd.Series, bus_colors: pd.Series, bus_sizes: pd.Series, edge_colors: pd.Series | str = \"black\", add_ref_edge_sizes=True, add_ref_bus_sizes=True, add_legend=True, bus_unit_conv=PLOT_COST_UNITS, edge_unit_conv=PLOT_CAP_UNITS, ax=None, **kwargs, ) -> plt.Axes: \"\"\"Plot the network on a map Args: network (pypsa.Network): the pypsa network (filtered to contain only relevant buses & links) tech_colors (dict): config mapping edge_colors (pd.Series|str): the series of edge colors edge_widths (pd.Series): the edge widths bus_colors (pd.Series): the series of bus colors bus_sizes (pd.Series): the series of bus sizes add_ref_edge_sizes (bool, optional): add reference line sizes in legend (requires edge_colors=True). Defaults to True. add_ref_bus_sizes (bool, optional): add reference bus sizes in legend. Defaults to True. ax (plt.Axes, optional): the plotting ax. Defaults to None (new figure). \"\"\" if not ax: fig, ax = plt.subplots() network.plot( bus_sizes=bus_sizes, bus_colors=bus_colors, line_colors=edge_colors, link_colors=edge_colors, line_widths=edge_widths, link_widths=edge_widths, ax=ax, color_geomap=True, boundaries=kwargs.get(\"boundaries\", None), ) ax.add_feature(cfeature.BORDERS, linewidth=0.5, edgecolor=\"gray\") states_provinces = cfeature.NaturalEarthFeature( category=\"cultural\", name=\"admin_1_states_provinces_lines\", scale=\"50m\", facecolor=\"none\" ) # Add our states feature. ax.add_feature(states_provinces, edgecolor=\"lightgray\", alpha=0.7) if add_legend: carriers = bus_sizes.index.get_level_values(1).unique() colors = carriers.intersection(tech_colors).map(tech_colors).to_list() if isinstance(edge_colors, str): colors += [edge_colors] labels = carriers.to_list() + [\"HVDC or HVAC link\"] else: colors += edge_colors.values.to_list() labels = carriers.to_list() + edge_colors.index.to_list() leg_opt = {\"bbox_to_anchor\": (1.42, 1.04), \"frameon\": False} add_legend_patches(ax, colors, labels, legend_kw=leg_opt) if add_ref_edge_sizes & isinstance(edge_colors, str): ref_unit = kwargs.get(\"ref_edge_unit\", \"GW\") size_factor = float(kwargs.get(\"linewidth_factor\", 1e5)) ref_sizes = kwargs.get(\"ref_edge_sizes\", [1e5, 5e5]) labels = [f\"{float(s)/edge_unit_conv} {ref_unit}\" for s in ref_sizes] ref_sizes = list(map(lambda x: float(x) / size_factor, ref_sizes)) legend_kw = dict( loc=\"upper left\", bbox_to_anchor=(0.26, 1.0), frameon=False, labelspacing=0.8, handletextpad=2, title=kwargs.get(\"edge_ref_title\", \"Grid cap.\"), ) add_legend_lines( ax, ref_sizes, labels, patch_kw=dict(color=edge_colors), legend_kw=legend_kw ) # add reference bus sizes ferom the units if add_ref_bus_sizes: ref_unit = kwargs.get(\"ref_bus_unit\", \"bEUR/a\") size_factor = float(kwargs.get(\"bus_size_factor\", 1e10)) ref_sizes = kwargs.get(\"ref_bus_sizes\", [2e10, 1e10, 5e10]) labels = [f\"{float(s)/bus_unit_conv:.0f} {ref_unit}\" for s in ref_sizes] ref_sizes = list(map(lambda x: float(x) / size_factor, ref_sizes)) legend_kw = { \"loc\": \"upper left\", \"bbox_to_anchor\": (0.0, 1.0), \"labelspacing\": 0.8, \"frameon\": False, \"handletextpad\": 0, \"title\": kwargs.get(\"bus_ref_title\", \"UNDEFINED TITLE\"), } add_legend_circles( ax, ref_sizes, labels, srid=network.srid, patch_kw=dict(facecolor=\"lightgrey\"), legend_kw=legend_kw, ) return ax","title":"plot_map"},{"location":"docs/reference/plot_network/#plot_network.plot_nodal_prices","text":"A map plot of energy, either AC or heat Parameters: network ( Network ) \u2013 the pyPSA network object opts ( dict ) \u2013 the plotting options (snakemake.config[\"plotting\"]) save_path ( PathLike , default: None ) \u2013 Fig outp path. Defaults to None (no save). carrier ( str , default: 'AC' ) \u2013 the energy carrier. Defaults to \"AC\". raises: ValueError: if carrier is not AC or heat Source code in workflow/scripts/plot_network.py def plot_nodal_prices( network: pypsa.Network, opts: dict, carrier=\"AC\", save_path: os.PathLike = None, ): \"\"\"A map plot of energy, either AC or heat Args: network (pypsa.Network): the pyPSA network object opts (dict): the plotting options (snakemake.config[\"plotting\"]) save_path (os.PathLike, optional): Fig outp path. Defaults to None (no save). carrier (str, optional): the energy carrier. Defaults to \"AC\". raises: ValueError: if carrier is not AC or heat \"\"\" if carrier not in [\"AC\", \"heat\"]: raise ValueError(\"Carrier must be either 'AC' or 'heat'\") # demand weighed prices per node nodal_prices = ( network.statistics.revenue( groupby=pypsa.statistics.get_bus_and_carrier_and_bus_carrier, comps=\"Load\", bus_carrier=carrier, ) / network.statistics.withdrawal( comps=\"Load\", groupby=pypsa.statistics.get_bus_and_carrier_and_bus_carrier, bus_carrier=carrier, ) * -1 ) # drop the carrier and bus_carrier, map to colors nodal_prices = nodal_prices.droplevel(1).droplevel(1) norm = plt.Normalize(vmin=nodal_prices.min(), vmax=nodal_prices.max()) cmap = plt.get_cmap(\"plasma\") bus_colors = nodal_prices.map(lambda x: cmap(norm(x))) energy_consum = network.statistics.withdrawal( groupby=pypsa.statistics.get_bus_and_carrier, bus_carrier=carrier, comps=[\"Load\"], ) consum_pies = energy_consum.groupby(level=1).sum() # Make figure fig, ax = plt.subplots(subplot_kw={\"projection\": ccrs.PlateCarree()}) fig.set_size_inches(opts[\"price_map\"][\"figsize\"]) # get colors # TODO make line handling nicer # make sure plot isnt overpopulated def calc_plot_width(row, carrier=\"AC\"): if row.length == 0: return 0 elif row.carrier != carrier: return 0 else: return row.p_nom_opt line_lower_threshold = opts.get(\"min_edge_capacity\", 500) edge_carrier = \"H2\" if carrier == \"heat\" else \"AC\" link_plot_w = network.links.apply(lambda row: calc_plot_width(row, edge_carrier), axis=1) edges = pd.concat([network.lines.s_nom_opt, link_plot_w]) edge_widths = edges.clip(line_lower_threshold, edges.max()).replace(line_lower_threshold, 0) bus_size_factor = opts[\"price_map\"][\"bus_size_factor\"] linewidth_factor = opts[\"price_map\"][f\"linewidth_factor{\"_heat\" if carrier == 'heat' else ''}\"] plot_map( network, tech_colors=None, edge_widths=edge_widths / linewidth_factor, bus_colors=bus_colors, bus_sizes=consum_pies / bus_size_factor, edge_colors=opts[\"price_map\"][\"edge_color\"], ax=ax, edge_unit_conv=PLOT_CAP_UNITS, bus_unit_conv=PLOT_SUPPLY_UNITS, add_legend=False, **opts[\"price_map\"], ) # Add colorbar based on bus_colors # fig.tight_layout() fig.subplots_adjust(right=0.85) cax = fig.add_axes([0.87, ax.get_position().y0, 0.02, ax.get_position().height]) sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm) sm.set_array([]) cbar = plt.colorbar(sm, cax=cax, orientation=\"vertical\") cbar.set_label(f\"Nodal Prices ${CURRENCY}/MWh\") if save_path: fig.savefig(save_path, transparent=True, bbox_inches=\"tight\")","title":"plot_nodal_prices"},{"location":"docs/reference/plot_network_heat/","text":"","title":"plot_network_heat"},{"location":"docs/reference/plot_statistics/","text":"plot_static_per_carrier(ds, ax, colors, drop_zero_vals=True) Generic function to plot different statics Parameters: ds ( DataFrame ) \u2013 the data to plot ax ( Axes ) \u2013 plotting axes colors ( DataFrame ) \u2013 colors for the carriers drop_zero_vals ( bool , default: True ) \u2013 Drop zeroes from data. Defaults to True. Source code in workflow/scripts/plot_statistics.py def plot_static_per_carrier(ds: DataFrame, ax: axes.Axes, colors: DataFrame, drop_zero_vals=True): \"\"\"Generic function to plot different statics Args: ds (DataFrame): the data to plot ax (matplotlib.axes.Axes): plotting axes colors (DataFrame): colors for the carriers drop_zero_vals (bool, optional): Drop zeroes from data. Defaults to True. \"\"\" if drop_zero_vals: ds = ds[ds != 0] ds = ds.dropna() logger.info(\"debuggin plot stat\") logger.info(colors) c = colors[ds.index.get_level_values(\"carrier\")] logger.info(c) logger.info(ds.index.get_level_values(\"carrier\")) logger.info(colors.loc[ds.index.get_level_values(\"carrier\")]) ds = ds.pipe(rename_index) label = f\"{ds.attrs['name']} [{ds.attrs['unit']}]\" ds.plot.barh(color=c.values, xlabel=label, ax=ax) ax.grid(axis=\"y\")","title":"plot_statistics"},{"location":"docs/reference/plot_statistics/#plot_statistics.plot_static_per_carrier","text":"Generic function to plot different statics Parameters: ds ( DataFrame ) \u2013 the data to plot ax ( Axes ) \u2013 plotting axes colors ( DataFrame ) \u2013 colors for the carriers drop_zero_vals ( bool , default: True ) \u2013 Drop zeroes from data. Defaults to True. Source code in workflow/scripts/plot_statistics.py def plot_static_per_carrier(ds: DataFrame, ax: axes.Axes, colors: DataFrame, drop_zero_vals=True): \"\"\"Generic function to plot different statics Args: ds (DataFrame): the data to plot ax (matplotlib.axes.Axes): plotting axes colors (DataFrame): colors for the carriers drop_zero_vals (bool, optional): Drop zeroes from data. Defaults to True. \"\"\" if drop_zero_vals: ds = ds[ds != 0] ds = ds.dropna() logger.info(\"debuggin plot stat\") logger.info(colors) c = colors[ds.index.get_level_values(\"carrier\")] logger.info(c) logger.info(ds.index.get_level_values(\"carrier\")) logger.info(colors.loc[ds.index.get_level_values(\"carrier\")]) ds = ds.pipe(rename_index) label = f\"{ds.attrs['name']} [{ds.attrs['unit']}]\" ds.plot.barh(color=c.values, xlabel=label, ax=ax) ax.grid(axis=\"y\")","title":"plot_static_per_carrier"},{"location":"docs/reference/plot_summary_all/","text":"Plots energy and cost summaries for solved networks. plot_co2_shadow_price(file_list, config, fig_name=None) plot the co2 price Parameters: file_list ( list ) \u2013 the input csvs from make_summaries config ( dict ) \u2013 the snakemake configuration fig_name ( PathLike , default: None ) \u2013 the figure name. Defaults to None. Source code in workflow/scripts/plot_summary_all.py def plot_co2_shadow_price(file_list: list, config: dict, fig_name=None): \"\"\"plot the co2 price Args: file_list (list): the input csvs from make_summaries config (dict): the snakemake configuration fig_name (os.PathLike, optional): the figure name. Defaults to None. \"\"\" co2_prices = {} co2_budget = {} for i, results_file in enumerate(file_list): df_metrics = pd.read_csv(results_file, index_col=list(range(1)), header=[1]) co2_prices.update(dict(df_metrics.loc[\"co2_shadow\"])) co2_budget.update(dict(df_metrics.loc[\"co2_budget\"])) fig, ax = plt.subplots() fig.set_size_inches((12, 8)) ax.plot(co2_prices.keys(), np.abs(list(co2_prices.values())), marker=\"o\", color=\"black\") ax.set_ylabel(\"CO2 Shadow price\") ax.set_xlabel(\"Year\") ax2 = ax.twinx() ax2.plot( co2_budget.keys(), [v / PLOT_CO2_UNITS for v in co2_budget.values()], marker=\"D\", color=\"blue\", ) ax2.set_ylabel(f\"CO2 Budget [{PLOT_CO2_LABEL}]\") fig.tight_layout() if fig_name is not None: fig.savefig(fig_name, transparent=True) plot_electricty_heat_balance(file_list, config, fig_dir=None) plot the energy production and consumption Parameters: file_list ( list ) \u2013 the input csvs from make_dirs([year/supply_energy.csv]) config ( dict ) \u2013 the configuration for plotting (snamkemake.config[\"plotting\"]) fig_dir ( PathLike , default: None ) \u2013 the figure name. Defaults to None. Source code in workflow/scripts/plot_summary_all.py def plot_electricty_heat_balance(file_list: list[os.PathLike], config: dict, fig_dir=None): \"\"\"plot the energy production and consumption Args: file_list (list): the input csvs from make_dirs([year/supply_energy.csv]) config (dict): the configuration for plotting (snamkemake.config[\"plotting\"]) fig_dir (os.PathLike, optional): the figure name. Defaults to None. \"\"\" elec_df = pd.DataFrame() heat_df = pd.DataFrame() for results_file in file_list: balance_df = pd.read_csv(results_file, index_col=list(range(2)), header=[1]) elec = balance_df.loc[\"AC\"].copy() elec.set_index(elec.columns[0], inplace=True) elec.rename(index={\"-\": \"electric load\"}, inplace=True) elec.index.rename(\"carrier\", inplace=True) # this groups subgroups of the same carrier. For example, baseyar hydro = link from dams # but new hydro is generator from province elec = elec.groupby(elec.index).sum() heat = balance_df.loc[\"heat\"].copy() heat.set_index(heat.columns[0], inplace=True) heat.rename(index={\"-\": \"heat load\"}, inplace=True) heat.index.rename(\"carrier\", inplace=True) heat = heat.groupby(heat.index).sum() to_drop = elec.index[ elec.max(axis=1).abs() < config[\"energy_threshold\"] / PLOT_SUPPLY_UNITS ] elec.loc[\"Other\"] = elec.loc[to_drop].sum(axis=0) elec.drop(to_drop, inplace=True) to_drop = heat.index[ heat.max(axis=1).abs() < config[\"energy_threshold\"] / PLOT_SUPPLY_UNITS ] heat.loc[\"Other\"] = heat.loc[to_drop].sum(axis=0) heat.drop(to_drop, inplace=True) elec_df = pd.concat([elec, elec_df], axis=1) heat_df = pd.concat([heat, heat_df], axis=1) elec_df.fillna(0, inplace=True) elec_df.sort_index(axis=1, inplace=True, ascending=True) elec_df = elec_df / PLOT_SUPPLY_UNITS heat_df.fillna(0, inplace=True) heat_df.sort_index(axis=1, inplace=True, ascending=True) heat_df = heat_df / PLOT_SUPPLY_UNITS # # split into consumption and generation el_gen = elec_df.where(elec_df >= 0).dropna(axis=0, how=\"all\").fillna(0) el_con = elec_df.where(elec_df < 0).dropna(axis=0, how=\"all\").fillna(0) heat_gen = heat_df.where(heat_df > 0).dropna(axis=0, how=\"all\").fillna(0) heat_con = heat_df.where(heat_df < 0).dropna(axis=0, how=\"all\").fillna(0) # group identical values el_con = el_con.groupby(el_con.index).sum() el_gen = el_gen.groupby(el_gen.index).sum() heat_con = heat_con.groupby(heat_con.index).sum() heat_gen = heat_gen.groupby(heat_gen.index).sum() logger.info(f\"Total energy of {round(elec_df.sum()[0])} TWh/a\") # =========== electricity ================= fig, ax = plt.subplots() fig.set_size_inches((12, 8)) for df in [el_gen, el_con]: preferred_order = pd.Index(config[\"preferred_order\"]) new_index = preferred_order.intersection(df.index).append( df.index.difference(preferred_order) ) colors = pd.DataFrame( new_index.map(config[\"tech_colors\"]), index=new_index, columns=[\"color\"] ) colors.fillna(NAN_COLOR, inplace=True) df.loc[new_index].T.plot( kind=\"bar\", ax=ax, stacked=True, color=colors[\"color\"], ) handles, labels = ax.get_legend_handles_labels() handles.reverse() labels.reverse() ax.set_ylim([el_con.sum(axis=0).min() * 1.1, el_gen.sum(axis=0).max() * 1.1]) ax.set_ylabel(\"Energy [TWh/a]\") ax.set_xlabel(\"\") ax.grid(axis=\"y\") ax.legend(handles, labels, ncol=1, bbox_to_anchor=[1, 1], loc=\"upper left\") fig.tight_layout() if fig_dir is not None: fig.savefig(os.path.join(fig_dir, \"elec_balance.png\"), transparent=True) # ================= heat ================= fig, ax = plt.subplots() fig.set_size_inches((12, 8)) for df in [heat_gen, heat_con]: preferred_order = pd.Index(config[\"preferred_order\"]) new_index = preferred_order.intersection(df.index).append( df.index.difference(preferred_order) ) colors = pd.DataFrame( new_index.map(config[\"tech_colors\"]), index=new_index, columns=[\"color\"] ) colors.fillna(NAN_COLOR, inplace=True) df.loc[new_index].T.plot( kind=\"bar\", ax=ax, stacked=True, color=colors[\"color\"], ) handles, labels = ax.get_legend_handles_labels() handles.reverse() labels.reverse() ax.set_ylim([heat_con.sum(axis=0).min() * 1.1, heat_gen.sum(axis=0).max() * 1.1]) ax.set_ylabel(\"Energy [TWh/a]\") ax.set_xlabel(\"\") ax.grid(axis=\"y\") ax.legend(handles, labels, ncol=1, bbox_to_anchor=[1, 1], loc=\"upper left\") fig.tight_layout() if fig_dir is not None: fig.savefig(os.path.join(fig_dir, \"heat_balance.png\"), transparent=True) plot_energy(file_list, config, fig_name=None) plot the energy production and consumption Parameters: file_list ( list ) \u2013 the input csvs config ( dict ) \u2013 the configuration for plotting (snamkemake.config[\"plotting\"]) fig_name ( PathLike , default: None ) \u2013 the figure name. Defaults to None. Source code in workflow/scripts/plot_summary_all.py def plot_energy(file_list: list, config: dict, fig_name=None): \"\"\"plot the energy production and consumption Args: file_list (list): the input csvs config (dict): the configuration for plotting (snamkemake.config[\"plotting\"]) fig_name (os.PathLike, optional): the figure name. Defaults to None. \"\"\" energy_df = pd.DataFrame() for results_file in file_list: en_df = pd.read_csv(results_file, index_col=list(range(2)), header=[1]) df_ = en_df.groupby(en_df.index.get_level_values(1)).sum() # do this here so aggregate costs of small items only for that year # convert MWh to TWh df_ = df_ / PLOT_SUPPLY_UNITS df_ = df_.groupby(df_.index.map(rename_techs)).sum() to_drop = df_.index[df_.max(axis=1) < config[\"energy_threshold\"] / PLOT_SUPPLY_UNITS] df_.loc[\"Other\"] = df_.loc[to_drop].sum(axis=0) df_ = df_.drop(to_drop) energy_df = pd.concat([df_, energy_df], axis=1) energy_df.fillna(0, inplace=True) energy_df.sort_index(axis=1, inplace=True) logger.info(f\"Total energy of {round(energy_df.sum()[0])} TWh/a\") preferred_order = pd.Index(config[\"preferred_order\"]) new_index = preferred_order.intersection(energy_df.index).append( energy_df.index.difference(preferred_order) ) new_columns = energy_df.columns.sort_values() fig, ax = plt.subplots() fig.set_size_inches((12, 8)) logger.debug(energy_df.loc[new_index, new_columns]) energy_df.loc[new_index, new_columns].T.plot( kind=\"bar\", ax=ax, stacked=True, color=[config[\"tech_colors\"][i] for i in new_index], ) handles, labels = ax.get_legend_handles_labels() handles.reverse() labels.reverse() ax.set_ylim([0, energy_df.sum(axis=0).max() * 1.1]) ax.set_ylabel(\"Energy [TWh/a]\") ax.set_xlabel(\"\") ax.grid(axis=\"y\") ax.legend(handles, labels, ncol=1, bbox_to_anchor=[1, 1], loc=\"upper left\") fig.tight_layout() if fig_name is not None: fig.savefig(fig_name, transparent=True) plot_pathway_co2(file_list, config, fig_name=None) Plot the CO2 pathway balance and totals Parameters: file_list ( list ) \u2013 the input csvs config ( dict ) \u2013 the plotting configuration fig_name ( _type_ , default: None ) \u2013 description . Defaults to None. Source code in workflow/scripts/plot_summary_all.py def plot_pathway_co2(file_list: list, config: dict, fig_name=None): \"\"\"Plot the CO2 pathway balance and totals Args: file_list (list): the input csvs config (dict): the plotting configuration fig_name (_type_, optional): _description_. Defaults to None. \"\"\" co2_balance_df = pd.DataFrame() for results_file in file_list: df_year = pd.read_csv(results_file, index_col=list(range(1)), header=[1]).T co2_balance_df = pd.concat([df_year, co2_balance_df]) co2_balance_df.sort_index(axis=0, inplace=True) fig, ax = plt.subplots() bar_width = 0.6 colors = co2_balance_df.T.index.map(config[\"plotting\"][\"tech_colors\"]).values co2_balance_df = co2_balance_df / PLOT_CO2_UNITS co2_balance_df.plot( kind=\"bar\", stacked=True, width=bar_width, color=pd.Series(colors).fillna(NAN_COLOR), ax=ax, ) bar_centers = np.unique([patch.get_x() + bar_width / 2 for patch in ax.patches]) ax.plot( bar_centers, co2_balance_df.sum(axis=1).values, color=\"black\", marker=\"D\", markersize=10, lw=3, label=\"Total\", ) ax.set_ylabel(PLOT_CO2_LABEL) handles, labels = ax.get_legend_handles_labels() ax.legend(handles, labels, ncol=1, bbox_to_anchor=[1, 1], loc=\"upper left\") fig.tight_layout() if fig_name is not None: fig.savefig(fig_name, transparent=True) plot_pathway_costs(file_list, config, social_discount_rate=0.0, fig_name=None) plot the costs Parameters: file_list ( list ) \u2013 the input csvs from make_summary config ( dict ) \u2013 the configuration for plotting (snakemake.config[\"plotting\"]) social_discount_rate ( float , default: 0.0 ) \u2013 the social discount rate (0.02). Defaults to 0.0. fig_name ( PathLike , default: None ) \u2013 the figure name. Defaults to None. Source code in workflow/scripts/plot_summary_all.py def plot_pathway_costs( file_list: list, config: dict, social_discount_rate=0.0, fig_name: os.PathLike = None ): \"\"\"plot the costs Args: file_list (list): the input csvs from make_summary config (dict): the configuration for plotting (snakemake.config[\"plotting\"]) social_discount_rate (float, optional): the social discount rate (0.02). Defaults to 0.0. fig_name (os.PathLike, optional): the figure name. Defaults to None. \"\"\" # all years in one df df = pd.DataFrame() for results_file in file_list: cost_df = pd.read_csv(results_file, index_col=list(range(3)), header=[1]) df_ = cost_df.groupby(cost_df.index.get_level_values(2)).sum() # do this here so aggregate costs of small items only for that year # TODO centralise unit df_ = df_ * COST_UNIT / PLOT_COST_UNITS df_ = df_.groupby(df_.index.map(rename_techs)).sum() to_drop = df_.index[df_.max(axis=1) < config[\"costs_threshold\"] / PLOT_COST_UNITS] df_.loc[\"Other\"] = df_.loc[to_drop].sum(axis=0) df_ = df_.drop(to_drop) df = pd.concat([df_, df], axis=1) df.fillna(0, inplace=True) df.rename(columns={int(y): y for y in df.columns}, inplace=True) df.sort_index(axis=1, inplace=True, ascending=True) # apply social discount rate if social_discount_rate > 0: base_year = min([int(y) for y in df.columns]) df = df.apply(lambda x: x / (1 + social_discount_rate) ** (int(x.name) - base_year), axis=0) elif social_discount_rate < 0: raise ValueError(\"Social discount rate must be positive\") preferred_order = pd.Index(config[\"preferred_order\"]) new_index = preferred_order.intersection(df.index).append(df.index.difference(preferred_order)) fig, ax = plt.subplots() fig.set_size_inches((12, 8)) df.loc[new_index].T.plot( kind=\"bar\", ax=ax, stacked=True, color=[config[\"tech_colors\"][i] for i in new_index], ) handles, labels = ax.get_legend_handles_labels() ax.set_ylim([0, df.sum(axis=0).max() * 1.1]) ax.set_ylabel(\"System Cost [EUR billion per year]\") ax.set_xlabel(\"\") ax.grid(axis=\"y\") # TODO fix this - doesnt work with non-constant interval ax.annotate( f\"Total cost in bn Eur: {df.sum().sum()*5:.2f}\", xy=(0.75, 0.9), color=\"darkgray\", xycoords=\"axes fraction\", ha=\"right\", va=\"top\", ) ax.legend(handles, labels, ncol=1, bbox_to_anchor=[1, 1], loc=\"upper left\") fig.tight_layout() if fig_name is not None: fig.savefig(fig_name, transparent=True) plot_prices(file_list, config, fig_name=None) plot the prices Parameters: file_list ( list ) \u2013 the input csvs from make_summary config ( dict ) \u2013 the configuration for plotting fig_name ( PathLike , default: None ) \u2013 the figure name. Defaults to None. Source code in workflow/scripts/plot_summary_all.py def plot_prices(file_list: list, config: dict, fig_name=None): \"\"\"plot the prices Args: file_list (list): the input csvs from make_summary config (dict): the configuration for plotting fig_name (os.PathLike, optional): the figure name. Defaults to None. \"\"\" prices_df = pd.DataFrame() for results_file in file_list: df_year = pd.read_csv(results_file, index_col=list(range(1)), header=[1]).T prices_df = pd.concat([df_year, prices_df]) prices_df.sort_index(axis=0, inplace=True) fig, ax = plt.subplots() fig.set_size_inches((12, 8)) colors = config[\"plotting\"][\"tech_colors\"] prices_df.plot( ax=ax, kind=\"line\", color=[colors[k] if k in colors else \"k\" for k in prices_df.columns], linewidth=3, ) ax.set_ylim([prices_df.min().min() * 1.1, prices_df.max().max() * 1.1]) ax.set_ylabel(\"prices [X/UNIT]\") ax.set_xlabel(\"\") ax.grid(axis=\"y\") handles, labels = ax.get_legend_handles_labels() handles.reverse() labels.reverse() ax.legend(handles, labels, ncol=1, bbox_to_anchor=[1, 1], loc=\"upper left\") fig.tight_layout() if fig_name is not None: fig.savefig(fig_name, transparent=False)","title":"plot_summary_all"},{"location":"docs/reference/plot_summary_all/#plot_summary_all.plot_co2_shadow_price","text":"plot the co2 price Parameters: file_list ( list ) \u2013 the input csvs from make_summaries config ( dict ) \u2013 the snakemake configuration fig_name ( PathLike , default: None ) \u2013 the figure name. Defaults to None. Source code in workflow/scripts/plot_summary_all.py def plot_co2_shadow_price(file_list: list, config: dict, fig_name=None): \"\"\"plot the co2 price Args: file_list (list): the input csvs from make_summaries config (dict): the snakemake configuration fig_name (os.PathLike, optional): the figure name. Defaults to None. \"\"\" co2_prices = {} co2_budget = {} for i, results_file in enumerate(file_list): df_metrics = pd.read_csv(results_file, index_col=list(range(1)), header=[1]) co2_prices.update(dict(df_metrics.loc[\"co2_shadow\"])) co2_budget.update(dict(df_metrics.loc[\"co2_budget\"])) fig, ax = plt.subplots() fig.set_size_inches((12, 8)) ax.plot(co2_prices.keys(), np.abs(list(co2_prices.values())), marker=\"o\", color=\"black\") ax.set_ylabel(\"CO2 Shadow price\") ax.set_xlabel(\"Year\") ax2 = ax.twinx() ax2.plot( co2_budget.keys(), [v / PLOT_CO2_UNITS for v in co2_budget.values()], marker=\"D\", color=\"blue\", ) ax2.set_ylabel(f\"CO2 Budget [{PLOT_CO2_LABEL}]\") fig.tight_layout() if fig_name is not None: fig.savefig(fig_name, transparent=True)","title":"plot_co2_shadow_price"},{"location":"docs/reference/plot_summary_all/#plot_summary_all.plot_electricty_heat_balance","text":"plot the energy production and consumption Parameters: file_list ( list ) \u2013 the input csvs from make_dirs([year/supply_energy.csv]) config ( dict ) \u2013 the configuration for plotting (snamkemake.config[\"plotting\"]) fig_dir ( PathLike , default: None ) \u2013 the figure name. Defaults to None. Source code in workflow/scripts/plot_summary_all.py def plot_electricty_heat_balance(file_list: list[os.PathLike], config: dict, fig_dir=None): \"\"\"plot the energy production and consumption Args: file_list (list): the input csvs from make_dirs([year/supply_energy.csv]) config (dict): the configuration for plotting (snamkemake.config[\"plotting\"]) fig_dir (os.PathLike, optional): the figure name. Defaults to None. \"\"\" elec_df = pd.DataFrame() heat_df = pd.DataFrame() for results_file in file_list: balance_df = pd.read_csv(results_file, index_col=list(range(2)), header=[1]) elec = balance_df.loc[\"AC\"].copy() elec.set_index(elec.columns[0], inplace=True) elec.rename(index={\"-\": \"electric load\"}, inplace=True) elec.index.rename(\"carrier\", inplace=True) # this groups subgroups of the same carrier. For example, baseyar hydro = link from dams # but new hydro is generator from province elec = elec.groupby(elec.index).sum() heat = balance_df.loc[\"heat\"].copy() heat.set_index(heat.columns[0], inplace=True) heat.rename(index={\"-\": \"heat load\"}, inplace=True) heat.index.rename(\"carrier\", inplace=True) heat = heat.groupby(heat.index).sum() to_drop = elec.index[ elec.max(axis=1).abs() < config[\"energy_threshold\"] / PLOT_SUPPLY_UNITS ] elec.loc[\"Other\"] = elec.loc[to_drop].sum(axis=0) elec.drop(to_drop, inplace=True) to_drop = heat.index[ heat.max(axis=1).abs() < config[\"energy_threshold\"] / PLOT_SUPPLY_UNITS ] heat.loc[\"Other\"] = heat.loc[to_drop].sum(axis=0) heat.drop(to_drop, inplace=True) elec_df = pd.concat([elec, elec_df], axis=1) heat_df = pd.concat([heat, heat_df], axis=1) elec_df.fillna(0, inplace=True) elec_df.sort_index(axis=1, inplace=True, ascending=True) elec_df = elec_df / PLOT_SUPPLY_UNITS heat_df.fillna(0, inplace=True) heat_df.sort_index(axis=1, inplace=True, ascending=True) heat_df = heat_df / PLOT_SUPPLY_UNITS # # split into consumption and generation el_gen = elec_df.where(elec_df >= 0).dropna(axis=0, how=\"all\").fillna(0) el_con = elec_df.where(elec_df < 0).dropna(axis=0, how=\"all\").fillna(0) heat_gen = heat_df.where(heat_df > 0).dropna(axis=0, how=\"all\").fillna(0) heat_con = heat_df.where(heat_df < 0).dropna(axis=0, how=\"all\").fillna(0) # group identical values el_con = el_con.groupby(el_con.index).sum() el_gen = el_gen.groupby(el_gen.index).sum() heat_con = heat_con.groupby(heat_con.index).sum() heat_gen = heat_gen.groupby(heat_gen.index).sum() logger.info(f\"Total energy of {round(elec_df.sum()[0])} TWh/a\") # =========== electricity ================= fig, ax = plt.subplots() fig.set_size_inches((12, 8)) for df in [el_gen, el_con]: preferred_order = pd.Index(config[\"preferred_order\"]) new_index = preferred_order.intersection(df.index).append( df.index.difference(preferred_order) ) colors = pd.DataFrame( new_index.map(config[\"tech_colors\"]), index=new_index, columns=[\"color\"] ) colors.fillna(NAN_COLOR, inplace=True) df.loc[new_index].T.plot( kind=\"bar\", ax=ax, stacked=True, color=colors[\"color\"], ) handles, labels = ax.get_legend_handles_labels() handles.reverse() labels.reverse() ax.set_ylim([el_con.sum(axis=0).min() * 1.1, el_gen.sum(axis=0).max() * 1.1]) ax.set_ylabel(\"Energy [TWh/a]\") ax.set_xlabel(\"\") ax.grid(axis=\"y\") ax.legend(handles, labels, ncol=1, bbox_to_anchor=[1, 1], loc=\"upper left\") fig.tight_layout() if fig_dir is not None: fig.savefig(os.path.join(fig_dir, \"elec_balance.png\"), transparent=True) # ================= heat ================= fig, ax = plt.subplots() fig.set_size_inches((12, 8)) for df in [heat_gen, heat_con]: preferred_order = pd.Index(config[\"preferred_order\"]) new_index = preferred_order.intersection(df.index).append( df.index.difference(preferred_order) ) colors = pd.DataFrame( new_index.map(config[\"tech_colors\"]), index=new_index, columns=[\"color\"] ) colors.fillna(NAN_COLOR, inplace=True) df.loc[new_index].T.plot( kind=\"bar\", ax=ax, stacked=True, color=colors[\"color\"], ) handles, labels = ax.get_legend_handles_labels() handles.reverse() labels.reverse() ax.set_ylim([heat_con.sum(axis=0).min() * 1.1, heat_gen.sum(axis=0).max() * 1.1]) ax.set_ylabel(\"Energy [TWh/a]\") ax.set_xlabel(\"\") ax.grid(axis=\"y\") ax.legend(handles, labels, ncol=1, bbox_to_anchor=[1, 1], loc=\"upper left\") fig.tight_layout() if fig_dir is not None: fig.savefig(os.path.join(fig_dir, \"heat_balance.png\"), transparent=True)","title":"plot_electricty_heat_balance"},{"location":"docs/reference/plot_summary_all/#plot_summary_all.plot_energy","text":"plot the energy production and consumption Parameters: file_list ( list ) \u2013 the input csvs config ( dict ) \u2013 the configuration for plotting (snamkemake.config[\"plotting\"]) fig_name ( PathLike , default: None ) \u2013 the figure name. Defaults to None. Source code in workflow/scripts/plot_summary_all.py def plot_energy(file_list: list, config: dict, fig_name=None): \"\"\"plot the energy production and consumption Args: file_list (list): the input csvs config (dict): the configuration for plotting (snamkemake.config[\"plotting\"]) fig_name (os.PathLike, optional): the figure name. Defaults to None. \"\"\" energy_df = pd.DataFrame() for results_file in file_list: en_df = pd.read_csv(results_file, index_col=list(range(2)), header=[1]) df_ = en_df.groupby(en_df.index.get_level_values(1)).sum() # do this here so aggregate costs of small items only for that year # convert MWh to TWh df_ = df_ / PLOT_SUPPLY_UNITS df_ = df_.groupby(df_.index.map(rename_techs)).sum() to_drop = df_.index[df_.max(axis=1) < config[\"energy_threshold\"] / PLOT_SUPPLY_UNITS] df_.loc[\"Other\"] = df_.loc[to_drop].sum(axis=0) df_ = df_.drop(to_drop) energy_df = pd.concat([df_, energy_df], axis=1) energy_df.fillna(0, inplace=True) energy_df.sort_index(axis=1, inplace=True) logger.info(f\"Total energy of {round(energy_df.sum()[0])} TWh/a\") preferred_order = pd.Index(config[\"preferred_order\"]) new_index = preferred_order.intersection(energy_df.index).append( energy_df.index.difference(preferred_order) ) new_columns = energy_df.columns.sort_values() fig, ax = plt.subplots() fig.set_size_inches((12, 8)) logger.debug(energy_df.loc[new_index, new_columns]) energy_df.loc[new_index, new_columns].T.plot( kind=\"bar\", ax=ax, stacked=True, color=[config[\"tech_colors\"][i] for i in new_index], ) handles, labels = ax.get_legend_handles_labels() handles.reverse() labels.reverse() ax.set_ylim([0, energy_df.sum(axis=0).max() * 1.1]) ax.set_ylabel(\"Energy [TWh/a]\") ax.set_xlabel(\"\") ax.grid(axis=\"y\") ax.legend(handles, labels, ncol=1, bbox_to_anchor=[1, 1], loc=\"upper left\") fig.tight_layout() if fig_name is not None: fig.savefig(fig_name, transparent=True)","title":"plot_energy"},{"location":"docs/reference/plot_summary_all/#plot_summary_all.plot_pathway_co2","text":"Plot the CO2 pathway balance and totals Parameters: file_list ( list ) \u2013 the input csvs config ( dict ) \u2013 the plotting configuration fig_name ( _type_ , default: None ) \u2013 description . Defaults to None. Source code in workflow/scripts/plot_summary_all.py def plot_pathway_co2(file_list: list, config: dict, fig_name=None): \"\"\"Plot the CO2 pathway balance and totals Args: file_list (list): the input csvs config (dict): the plotting configuration fig_name (_type_, optional): _description_. Defaults to None. \"\"\" co2_balance_df = pd.DataFrame() for results_file in file_list: df_year = pd.read_csv(results_file, index_col=list(range(1)), header=[1]).T co2_balance_df = pd.concat([df_year, co2_balance_df]) co2_balance_df.sort_index(axis=0, inplace=True) fig, ax = plt.subplots() bar_width = 0.6 colors = co2_balance_df.T.index.map(config[\"plotting\"][\"tech_colors\"]).values co2_balance_df = co2_balance_df / PLOT_CO2_UNITS co2_balance_df.plot( kind=\"bar\", stacked=True, width=bar_width, color=pd.Series(colors).fillna(NAN_COLOR), ax=ax, ) bar_centers = np.unique([patch.get_x() + bar_width / 2 for patch in ax.patches]) ax.plot( bar_centers, co2_balance_df.sum(axis=1).values, color=\"black\", marker=\"D\", markersize=10, lw=3, label=\"Total\", ) ax.set_ylabel(PLOT_CO2_LABEL) handles, labels = ax.get_legend_handles_labels() ax.legend(handles, labels, ncol=1, bbox_to_anchor=[1, 1], loc=\"upper left\") fig.tight_layout() if fig_name is not None: fig.savefig(fig_name, transparent=True)","title":"plot_pathway_co2"},{"location":"docs/reference/plot_summary_all/#plot_summary_all.plot_pathway_costs","text":"plot the costs Parameters: file_list ( list ) \u2013 the input csvs from make_summary config ( dict ) \u2013 the configuration for plotting (snakemake.config[\"plotting\"]) social_discount_rate ( float , default: 0.0 ) \u2013 the social discount rate (0.02). Defaults to 0.0. fig_name ( PathLike , default: None ) \u2013 the figure name. Defaults to None. Source code in workflow/scripts/plot_summary_all.py def plot_pathway_costs( file_list: list, config: dict, social_discount_rate=0.0, fig_name: os.PathLike = None ): \"\"\"plot the costs Args: file_list (list): the input csvs from make_summary config (dict): the configuration for plotting (snakemake.config[\"plotting\"]) social_discount_rate (float, optional): the social discount rate (0.02). Defaults to 0.0. fig_name (os.PathLike, optional): the figure name. Defaults to None. \"\"\" # all years in one df df = pd.DataFrame() for results_file in file_list: cost_df = pd.read_csv(results_file, index_col=list(range(3)), header=[1]) df_ = cost_df.groupby(cost_df.index.get_level_values(2)).sum() # do this here so aggregate costs of small items only for that year # TODO centralise unit df_ = df_ * COST_UNIT / PLOT_COST_UNITS df_ = df_.groupby(df_.index.map(rename_techs)).sum() to_drop = df_.index[df_.max(axis=1) < config[\"costs_threshold\"] / PLOT_COST_UNITS] df_.loc[\"Other\"] = df_.loc[to_drop].sum(axis=0) df_ = df_.drop(to_drop) df = pd.concat([df_, df], axis=1) df.fillna(0, inplace=True) df.rename(columns={int(y): y for y in df.columns}, inplace=True) df.sort_index(axis=1, inplace=True, ascending=True) # apply social discount rate if social_discount_rate > 0: base_year = min([int(y) for y in df.columns]) df = df.apply(lambda x: x / (1 + social_discount_rate) ** (int(x.name) - base_year), axis=0) elif social_discount_rate < 0: raise ValueError(\"Social discount rate must be positive\") preferred_order = pd.Index(config[\"preferred_order\"]) new_index = preferred_order.intersection(df.index).append(df.index.difference(preferred_order)) fig, ax = plt.subplots() fig.set_size_inches((12, 8)) df.loc[new_index].T.plot( kind=\"bar\", ax=ax, stacked=True, color=[config[\"tech_colors\"][i] for i in new_index], ) handles, labels = ax.get_legend_handles_labels() ax.set_ylim([0, df.sum(axis=0).max() * 1.1]) ax.set_ylabel(\"System Cost [EUR billion per year]\") ax.set_xlabel(\"\") ax.grid(axis=\"y\") # TODO fix this - doesnt work with non-constant interval ax.annotate( f\"Total cost in bn Eur: {df.sum().sum()*5:.2f}\", xy=(0.75, 0.9), color=\"darkgray\", xycoords=\"axes fraction\", ha=\"right\", va=\"top\", ) ax.legend(handles, labels, ncol=1, bbox_to_anchor=[1, 1], loc=\"upper left\") fig.tight_layout() if fig_name is not None: fig.savefig(fig_name, transparent=True)","title":"plot_pathway_costs"},{"location":"docs/reference/plot_summary_all/#plot_summary_all.plot_prices","text":"plot the prices Parameters: file_list ( list ) \u2013 the input csvs from make_summary config ( dict ) \u2013 the configuration for plotting fig_name ( PathLike , default: None ) \u2013 the figure name. Defaults to None. Source code in workflow/scripts/plot_summary_all.py def plot_prices(file_list: list, config: dict, fig_name=None): \"\"\"plot the prices Args: file_list (list): the input csvs from make_summary config (dict): the configuration for plotting fig_name (os.PathLike, optional): the figure name. Defaults to None. \"\"\" prices_df = pd.DataFrame() for results_file in file_list: df_year = pd.read_csv(results_file, index_col=list(range(1)), header=[1]).T prices_df = pd.concat([df_year, prices_df]) prices_df.sort_index(axis=0, inplace=True) fig, ax = plt.subplots() fig.set_size_inches((12, 8)) colors = config[\"plotting\"][\"tech_colors\"] prices_df.plot( ax=ax, kind=\"line\", color=[colors[k] if k in colors else \"k\" for k in prices_df.columns], linewidth=3, ) ax.set_ylim([prices_df.min().min() * 1.1, prices_df.max().max() * 1.1]) ax.set_ylabel(\"prices [X/UNIT]\") ax.set_xlabel(\"\") ax.grid(axis=\"y\") handles, labels = ax.get_legend_handles_labels() handles.reverse() labels.reverse() ax.legend(handles, labels, ncol=1, bbox_to_anchor=[1, 1], loc=\"upper left\") fig.tight_layout() if fig_name is not None: fig.savefig(fig_name, transparent=False)","title":"plot_prices"},{"location":"docs/reference/plot_time_series/","text":"plot_energy_balance(n, plot_config, bus_carrier='AC', start_date='2060-03-31 21:00', end_date='2060-04-06 12:00:00', add_load_line=True, ax=None) plot the electricity balance of the network for the given time range Parameters: n ( Network ) \u2013 the network plot_config ( dict ) \u2013 the plotting config (snakemake.config[\"plotting\"]) bus_carrier ( str , default: 'AC' ) \u2013 the carrier for the energy_balance op. Defaults to \"AC\". start_date ( str , default: '2060-03-31 21:00' ) \u2013 the range to plot. Defaults to \"2060-03-31 21:00\". end_date ( str , default: '2060-04-06 12:00:00' ) \u2013 the range to plot. Defaults to \"2060-04-06 12:00:00\". add_load_line ( bool , default: True ) \u2013 add a dashed line for the load. Defaults to True. Source code in workflow/scripts/plot_time_series.py def plot_energy_balance( n: pypsa.Network, plot_config: dict, bus_carrier=\"AC\", start_date=\"2060-03-31 21:00\", end_date=\"2060-04-06 12:00:00\", add_load_line=True, ax: plt.Axes = None, ): \"\"\"plot the electricity balance of the network for the given time range Args: n (pypsa.Network): the network plot_config (dict): the plotting config (snakemake.config[\"plotting\"]) bus_carrier (str, optional): the carrier for the energy_balance op. Defaults to \"AC\". start_date (str, optional): the range to plot. Defaults to \"2060-03-31 21:00\". end_date (str, optional): the range to plot. Defaults to \"2060-04-06 12:00:00\". add_load_line (bool, optional): add a dashed line for the load. Defaults to True. \"\"\" if not ax: fig, ax = plt.subplots(figsize=(16, 8)) p = ( n.statistics.energy_balance(aggregate_time=False, bus_carrier=bus_carrier) .dropna(how=\"all\") .groupby(\"carrier\") .sum() .div(PLOT_CAP_UNITS) # .drop(\"-\") .T ) p.rename(columns={\"-\": \"Load\", \"AC\": \"transmission losses\"}, inplace=True) p = p.loc[start_date:end_date] p[\"coal\"] = p[[c for c in p.columns if c.find(\"coal\") >= 0]].sum(axis=1) p[\"gas\"] = p[[c for c in p.columns if c.find(\"gas\") >= 0]].sum(axis=1) p.drop(columns=[c for c in p.columns if c.find(\"coal\") >= 0], inplace=True) p.drop(columns=[c for c in p.columns if c.find(\"gas\") >= 0], inplace=True) extra_c = { \"Load\": plot_config[\"tech_colors\"][\"electric load\"], \"transmission losses\": plot_config[\"tech_colors\"][\"transmission losses\"], } nice_tech_colors = make_nice_tech_colors(plot_config[\"tech_colors\"], plot_config[\"nice_names\"]) color_series = get_stat_colors(n, nice_tech_colors, extra_colors=extra_c) # colors & names part 1 p.rename(plot_config[\"nice_names\"], inplace=True) p.rename(columns={k: k.title() for k in p.columns}, inplace=True) color_series.index = color_series.index.str.strip() # split into supply and wothdrawal supply = p.where(p >= 0).dropna(axis=1, how=\"all\") charge = p.where(p < 0).dropna(how=\"all\", axis=1) # fix names and order charge.rename(columns={\"Battery Storage\": \"Battery\"}, inplace=True) supply.rename(columns={\"Battery Discharger\": \"Battery\"}, inplace=True) color_series.rename( {\"Battery Discharger\": \"Battery\", \"Battery Storage\": \"Battery\"}, inplace=True, ) preferred_order = plot_config[\"preferred_order\"] plot_order = ( supply.columns.intersection(preferred_order).to_list() + supply.columns.difference(preferred_order).to_list() ) plot_order_charge = [name for name in preferred_order if name in charge.columns] + [ name for name in charge.columns if name not in preferred_order ] supply = supply.reindex(columns=plot_order) charge = charge.reindex(columns=plot_order_charge) if not charge.empty: charge.plot.area(ax=ax, linewidth=0, color=color_series.loc[charge.columns]) supply.plot.area( ax=ax, linewidth=0, color=color_series.loc[supply.columns], ) if add_load_line: charge[\"load_pos\"] = charge[\"Load\"] * -1 charge[\"load_pos\"].plot(linewidth=2, color=\"black\", label=\"Load\", ax=ax, linestyle=\"--\") charge.drop(columns=\"load_pos\", inplace=True) ax.legend(ncol=1, loc=\"center left\", bbox_to_anchor=(1, 0.5), frameon=False, fontsize=16) ax.set_ylabel(PLOT_CAP_LABEL) ax.set_ylim(charge.sum(axis=1).min() * 1.07, supply.sum(axis=1).max() * 1.07) ax.grid(axis=\"y\") ax.set_xlim(supply.index.min(), supply.index.max()) return ax plot_load_duration_curve(network, carrier='AC', ax=None) plot the load duration curve for the given carrier Parameters: network ( Network ) \u2013 the pypasa network object carrier ( str , default: 'AC' ) \u2013 the load carrier, defaults to AC ax ( Axes , default: None ) \u2013 figure axes, if none fig will be created. Defaults to None. Returns: Axes \u2013 plt.Axes: the plotting axes Source code in workflow/scripts/plot_time_series.py def plot_load_duration_curve( network: pypsa.Network, carrier: str = \"AC\", ax: plt.Axes = None ) -> plt.Axes: \"\"\"plot the load duration curve for the given carrier Args: network (pypsa.Network): the pypasa network object carrier (str, optional): the load carrier, defaults to AC ax (plt.Axes, optional): figure axes, if none fig will be created. Defaults to None. Returns: plt.Axes: the plotting axes \"\"\" if not ax: fig, ax = plt.subplots(figsize=(16, 8)) load = network.statistics.withdrawal( groupby=get_location_and_carrier, aggregate_time=False, bus_carrier=carrier, comps=\"Load\", ).sum() load_curve = load.sort_values(ascending=False) / PLOT_CAP_LABEL load_curve.reset_index(drop=True).plot(ax=ax, lw=3) ax.set_ylabel(f\"Load [{PLOT_CAP_LABEL}]\") ax.set_xlabel(\"Hours\") return ax plot_regional_load_durations(network, carrier='AC', ax=None, cmap='plasma') plot the load duration curve for the given carrier stacked by region Parameters: network ( Network ) \u2013 the pypasa network object carrier ( str , default: 'AC' ) \u2013 the load carrier, defaults to AC ax ( Axes , default: None ) \u2013 axes to plot on, if none fig will be created. Defaults to None. Returns: Axes \u2013 plt.Axes: the plotting axes Source code in workflow/scripts/plot_time_series.py def plot_regional_load_durations( network: pypsa.Network, carrier=\"AC\", ax=None, cmap=\"plasma\" ) -> plt.Axes: \"\"\"plot the load duration curve for the given carrier stacked by region Args: network (pypsa.Network): the pypasa network object carrier (str, optional): the load carrier, defaults to AC ax (plt.Axes, optional): axes to plot on, if none fig will be created. Defaults to None. Returns: plt.Axes: the plotting axes \"\"\" if not ax: fig, ax = plt.subplots(figsize=(10, 8)) loads_all = network.statistics.withdrawal( groupby=get_location_and_carrier, aggregate_time=False, bus_carrier=carrier, comps=\"Load\" ).sum() load_curve_all = loads_all.sort_values(ascending=False) / PLOT_CAP_UNITS regio = network.statistics.withdrawal( groupby=get_location_and_carrier, aggregate_time=False, bus_carrier=carrier, comps=\"Load\" ) regio = regio.droplevel(1).T load_curve_regio = regio.loc[load_curve_all.index] / PLOT_CAP_UNITS fig, ax = plt.subplots() load_curve_regio.reset_index(drop=True).plot.area( ax=ax, stacked=True, cmap=cmap, legend=True, lw=3 ) ax.set_ylabel(f\"Load [{PLOT_CAP_LABEL}]\") ax.set_xlabel(\"Hours\") ax.legend( ncol=3, loc=\"upper center\", bbox_to_anchor=(0.5, -0.15), fontsize=\"small\", title_fontsize=\"small\", fancybox=True, shadow=True, ) return ax plot_residual_load_duration_curve(network, ax=None, vre_techs=['Onshore Wind', 'Offshore Wind', 'Solar']) plot the residual load duration curve for the given carrier Parameters: network ( Network ) \u2013 the pypasa network object ax ( Axes , default: None ) \u2013 Axes to plot on, if none fig will be created. Defaults to None. Returns: Axes \u2013 plt.Axes: the plotting axes Source code in workflow/scripts/plot_time_series.py def plot_residual_load_duration_curve( network, ax: plt.Axes = None, vre_techs=[\"Onshore Wind\", \"Offshore Wind\", \"Solar\"] ) -> plt.Axes: \"\"\"plot the residual load duration curve for the given carrier Args: network (pypsa.Network): the pypasa network object ax (plt.Axes, optional): Axes to plot on, if none fig will be created. Defaults to None. Returns: plt.Axes: the plotting axes \"\"\" CARRIER = \"AC\" if not ax: fig, ax = plt.subplots(figsize=(16, 8)) load = network.statistics.withdrawal( groupby=get_location_and_carrier, aggregate_time=False, bus_carrier=CARRIER, comps=\"Load\", ).sum() vre_supply = ( network.statistics.supply( groupby=get_location_and_carrier, aggregate_time=False, bus_carrier=CARRIER, comps=\"Generator\", ) .groupby(level=1) .sum() .loc[vre_techs] .sum() ) residual = (load - vre_supply).sort_values(ascending=False) / PLOT_CAP_UNITS residual.reset_index(drop=True).plot(ax=ax, lw=3) ax.set_ylabel(f\"Residual Load [{PLOT_CAP_LABEL}]\") ax.set_xlabel(\"Hours\") return ax","title":"plot_time_series"},{"location":"docs/reference/plot_time_series/#plot_time_series.plot_energy_balance","text":"plot the electricity balance of the network for the given time range Parameters: n ( Network ) \u2013 the network plot_config ( dict ) \u2013 the plotting config (snakemake.config[\"plotting\"]) bus_carrier ( str , default: 'AC' ) \u2013 the carrier for the energy_balance op. Defaults to \"AC\". start_date ( str , default: '2060-03-31 21:00' ) \u2013 the range to plot. Defaults to \"2060-03-31 21:00\". end_date ( str , default: '2060-04-06 12:00:00' ) \u2013 the range to plot. Defaults to \"2060-04-06 12:00:00\". add_load_line ( bool , default: True ) \u2013 add a dashed line for the load. Defaults to True. Source code in workflow/scripts/plot_time_series.py def plot_energy_balance( n: pypsa.Network, plot_config: dict, bus_carrier=\"AC\", start_date=\"2060-03-31 21:00\", end_date=\"2060-04-06 12:00:00\", add_load_line=True, ax: plt.Axes = None, ): \"\"\"plot the electricity balance of the network for the given time range Args: n (pypsa.Network): the network plot_config (dict): the plotting config (snakemake.config[\"plotting\"]) bus_carrier (str, optional): the carrier for the energy_balance op. Defaults to \"AC\". start_date (str, optional): the range to plot. Defaults to \"2060-03-31 21:00\". end_date (str, optional): the range to plot. Defaults to \"2060-04-06 12:00:00\". add_load_line (bool, optional): add a dashed line for the load. Defaults to True. \"\"\" if not ax: fig, ax = plt.subplots(figsize=(16, 8)) p = ( n.statistics.energy_balance(aggregate_time=False, bus_carrier=bus_carrier) .dropna(how=\"all\") .groupby(\"carrier\") .sum() .div(PLOT_CAP_UNITS) # .drop(\"-\") .T ) p.rename(columns={\"-\": \"Load\", \"AC\": \"transmission losses\"}, inplace=True) p = p.loc[start_date:end_date] p[\"coal\"] = p[[c for c in p.columns if c.find(\"coal\") >= 0]].sum(axis=1) p[\"gas\"] = p[[c for c in p.columns if c.find(\"gas\") >= 0]].sum(axis=1) p.drop(columns=[c for c in p.columns if c.find(\"coal\") >= 0], inplace=True) p.drop(columns=[c for c in p.columns if c.find(\"gas\") >= 0], inplace=True) extra_c = { \"Load\": plot_config[\"tech_colors\"][\"electric load\"], \"transmission losses\": plot_config[\"tech_colors\"][\"transmission losses\"], } nice_tech_colors = make_nice_tech_colors(plot_config[\"tech_colors\"], plot_config[\"nice_names\"]) color_series = get_stat_colors(n, nice_tech_colors, extra_colors=extra_c) # colors & names part 1 p.rename(plot_config[\"nice_names\"], inplace=True) p.rename(columns={k: k.title() for k in p.columns}, inplace=True) color_series.index = color_series.index.str.strip() # split into supply and wothdrawal supply = p.where(p >= 0).dropna(axis=1, how=\"all\") charge = p.where(p < 0).dropna(how=\"all\", axis=1) # fix names and order charge.rename(columns={\"Battery Storage\": \"Battery\"}, inplace=True) supply.rename(columns={\"Battery Discharger\": \"Battery\"}, inplace=True) color_series.rename( {\"Battery Discharger\": \"Battery\", \"Battery Storage\": \"Battery\"}, inplace=True, ) preferred_order = plot_config[\"preferred_order\"] plot_order = ( supply.columns.intersection(preferred_order).to_list() + supply.columns.difference(preferred_order).to_list() ) plot_order_charge = [name for name in preferred_order if name in charge.columns] + [ name for name in charge.columns if name not in preferred_order ] supply = supply.reindex(columns=plot_order) charge = charge.reindex(columns=plot_order_charge) if not charge.empty: charge.plot.area(ax=ax, linewidth=0, color=color_series.loc[charge.columns]) supply.plot.area( ax=ax, linewidth=0, color=color_series.loc[supply.columns], ) if add_load_line: charge[\"load_pos\"] = charge[\"Load\"] * -1 charge[\"load_pos\"].plot(linewidth=2, color=\"black\", label=\"Load\", ax=ax, linestyle=\"--\") charge.drop(columns=\"load_pos\", inplace=True) ax.legend(ncol=1, loc=\"center left\", bbox_to_anchor=(1, 0.5), frameon=False, fontsize=16) ax.set_ylabel(PLOT_CAP_LABEL) ax.set_ylim(charge.sum(axis=1).min() * 1.07, supply.sum(axis=1).max() * 1.07) ax.grid(axis=\"y\") ax.set_xlim(supply.index.min(), supply.index.max()) return ax","title":"plot_energy_balance"},{"location":"docs/reference/plot_time_series/#plot_time_series.plot_load_duration_curve","text":"plot the load duration curve for the given carrier Parameters: network ( Network ) \u2013 the pypasa network object carrier ( str , default: 'AC' ) \u2013 the load carrier, defaults to AC ax ( Axes , default: None ) \u2013 figure axes, if none fig will be created. Defaults to None. Returns: Axes \u2013 plt.Axes: the plotting axes Source code in workflow/scripts/plot_time_series.py def plot_load_duration_curve( network: pypsa.Network, carrier: str = \"AC\", ax: plt.Axes = None ) -> plt.Axes: \"\"\"plot the load duration curve for the given carrier Args: network (pypsa.Network): the pypasa network object carrier (str, optional): the load carrier, defaults to AC ax (plt.Axes, optional): figure axes, if none fig will be created. Defaults to None. Returns: plt.Axes: the plotting axes \"\"\" if not ax: fig, ax = plt.subplots(figsize=(16, 8)) load = network.statistics.withdrawal( groupby=get_location_and_carrier, aggregate_time=False, bus_carrier=carrier, comps=\"Load\", ).sum() load_curve = load.sort_values(ascending=False) / PLOT_CAP_LABEL load_curve.reset_index(drop=True).plot(ax=ax, lw=3) ax.set_ylabel(f\"Load [{PLOT_CAP_LABEL}]\") ax.set_xlabel(\"Hours\") return ax","title":"plot_load_duration_curve"},{"location":"docs/reference/plot_time_series/#plot_time_series.plot_regional_load_durations","text":"plot the load duration curve for the given carrier stacked by region Parameters: network ( Network ) \u2013 the pypasa network object carrier ( str , default: 'AC' ) \u2013 the load carrier, defaults to AC ax ( Axes , default: None ) \u2013 axes to plot on, if none fig will be created. Defaults to None. Returns: Axes \u2013 plt.Axes: the plotting axes Source code in workflow/scripts/plot_time_series.py def plot_regional_load_durations( network: pypsa.Network, carrier=\"AC\", ax=None, cmap=\"plasma\" ) -> plt.Axes: \"\"\"plot the load duration curve for the given carrier stacked by region Args: network (pypsa.Network): the pypasa network object carrier (str, optional): the load carrier, defaults to AC ax (plt.Axes, optional): axes to plot on, if none fig will be created. Defaults to None. Returns: plt.Axes: the plotting axes \"\"\" if not ax: fig, ax = plt.subplots(figsize=(10, 8)) loads_all = network.statistics.withdrawal( groupby=get_location_and_carrier, aggregate_time=False, bus_carrier=carrier, comps=\"Load\" ).sum() load_curve_all = loads_all.sort_values(ascending=False) / PLOT_CAP_UNITS regio = network.statistics.withdrawal( groupby=get_location_and_carrier, aggregate_time=False, bus_carrier=carrier, comps=\"Load\" ) regio = regio.droplevel(1).T load_curve_regio = regio.loc[load_curve_all.index] / PLOT_CAP_UNITS fig, ax = plt.subplots() load_curve_regio.reset_index(drop=True).plot.area( ax=ax, stacked=True, cmap=cmap, legend=True, lw=3 ) ax.set_ylabel(f\"Load [{PLOT_CAP_LABEL}]\") ax.set_xlabel(\"Hours\") ax.legend( ncol=3, loc=\"upper center\", bbox_to_anchor=(0.5, -0.15), fontsize=\"small\", title_fontsize=\"small\", fancybox=True, shadow=True, ) return ax","title":"plot_regional_load_durations"},{"location":"docs/reference/plot_time_series/#plot_time_series.plot_residual_load_duration_curve","text":"plot the residual load duration curve for the given carrier Parameters: network ( Network ) \u2013 the pypasa network object ax ( Axes , default: None ) \u2013 Axes to plot on, if none fig will be created. Defaults to None. Returns: Axes \u2013 plt.Axes: the plotting axes Source code in workflow/scripts/plot_time_series.py def plot_residual_load_duration_curve( network, ax: plt.Axes = None, vre_techs=[\"Onshore Wind\", \"Offshore Wind\", \"Solar\"] ) -> plt.Axes: \"\"\"plot the residual load duration curve for the given carrier Args: network (pypsa.Network): the pypasa network object ax (plt.Axes, optional): Axes to plot on, if none fig will be created. Defaults to None. Returns: plt.Axes: the plotting axes \"\"\" CARRIER = \"AC\" if not ax: fig, ax = plt.subplots(figsize=(16, 8)) load = network.statistics.withdrawal( groupby=get_location_and_carrier, aggregate_time=False, bus_carrier=CARRIER, comps=\"Load\", ).sum() vre_supply = ( network.statistics.supply( groupby=get_location_and_carrier, aggregate_time=False, bus_carrier=CARRIER, comps=\"Generator\", ) .groupby(level=1) .sum() .loc[vre_techs] .sum() ) residual = (load - vre_supply).sort_values(ascending=False) / PLOT_CAP_UNITS residual.reset_index(drop=True).plot(ax=ax, lw=3) ax.set_ylabel(f\"Residual Load [{PLOT_CAP_LABEL}]\") ax.set_xlabel(\"Hours\") return ax","title":"plot_residual_load_duration_curve"},{"location":"docs/reference/prepare_base_network/","text":"add_carriers(network, config, costs) ad the various carriers to the network based on the config file Parameters: network ( Network ) \u2013 the pypsa network config ( dict ) \u2013 the config file costs ( DataFrame ) \u2013 the costs dataframe Source code in workflow/scripts/prepare_base_network.py def add_carriers(network: pypsa.Network, config: dict, costs: pd.DataFrame): \"\"\"ad the various carriers to the network based on the config file Args: network (pypsa.Network): the pypsa network config (dict): the config file costs (pd.DataFrame): the costs dataframe \"\"\" network.add(\"Carrier\", \"AC\") if config[\"heat_coupling\"]: network.add(\"Carrier\", \"heat\") for carrier in config[\"Techs\"][\"vre_techs\"]: network.add(\"Carrier\", carrier) if carrier == \"hydroelectricity\": network.add(\"Carrier\", \"hydro_inflow\") for carrier in config[\"Techs\"][\"store_techs\"]: network.add(\"Carrier\", carrier) if carrier == \"battery\": network.add(\"Carrier\", \"battery discharger\") if \"coal power plant\" in config[\"Techs\"][\"conv_techs\"] and config[\"Techs\"][\"coal_cc\"]: network.add(\"Carrier\", \"coal cc\", co2_emissions=0.034)","title":"prepare_base_network"},{"location":"docs/reference/prepare_base_network/#prepare_base_network.add_carriers","text":"ad the various carriers to the network based on the config file Parameters: network ( Network ) \u2013 the pypsa network config ( dict ) \u2013 the config file costs ( DataFrame ) \u2013 the costs dataframe Source code in workflow/scripts/prepare_base_network.py def add_carriers(network: pypsa.Network, config: dict, costs: pd.DataFrame): \"\"\"ad the various carriers to the network based on the config file Args: network (pypsa.Network): the pypsa network config (dict): the config file costs (pd.DataFrame): the costs dataframe \"\"\" network.add(\"Carrier\", \"AC\") if config[\"heat_coupling\"]: network.add(\"Carrier\", \"heat\") for carrier in config[\"Techs\"][\"vre_techs\"]: network.add(\"Carrier\", carrier) if carrier == \"hydroelectricity\": network.add(\"Carrier\", \"hydro_inflow\") for carrier in config[\"Techs\"][\"store_techs\"]: network.add(\"Carrier\", carrier) if carrier == \"battery\": network.add(\"Carrier\", \"battery discharger\") if \"coal power plant\" in config[\"Techs\"][\"conv_techs\"] and config[\"Techs\"][\"coal_cc\"]: network.add(\"Carrier\", \"coal cc\", co2_emissions=0.034)","title":"add_carriers"},{"location":"docs/reference/prepare_base_network_2020/","text":"","title":"prepare_base_network_2020"},{"location":"docs/reference/prepare_network/","text":"add_H2(network, config, nodes, costs) add H2 generators, storage and links to the network - currently all or nothing Parameters: network ( Network ) \u2013 network object too which H2 comps will be added config ( dict ) \u2013 the config (snakemake config) nodes ( Index ) \u2013 the buses costs ( DataFrame ) \u2013 the cost database Source code in workflow/scripts/prepare_network.py def add_H2(network: pypsa.Network, config: dict, nodes: pd.Index, costs: pd.DataFrame): \"\"\"add H2 generators, storage and links to the network - currently all or nothing Args: network (pypsa.Network): network object too which H2 comps will be added config (dict): the config (snakemake config) nodes (pd.Index): the buses costs (pd.DataFrame): the cost database \"\"\" network.add( \"Link\", name=nodes + \" H2 Electrolysis\", bus0=nodes, bus1=nodes + \" H2\", bus2=nodes + \" central heat\", p_nom_extendable=True, carrier=\"H2 Electrolysis\", efficiency=costs.at[\"electrolysis\", \"efficiency\"], efficiency2=costs.at[\"electrolysis\", \"efficiency-heat\"], capital_cost=costs.at[\"electrolysis\", \"capital_cost\"], lifetime=costs.at[\"electrolysis\", \"lifetime\"], ) # TODO consider switching to turbines and making a switch for off # TODO understand MVs network.add( \"Link\", name=nodes + \" H2 Fuel Cell\", bus0=nodes + \" H2\", bus1=nodes, p_nom_extendable=True, efficiency=costs.at[\"fuel cell\", \"efficiency\"], capital_cost=costs.at[\"fuel cell\", \"efficiency\"] * costs.at[\"fuel cell\", \"capital_cost\"], lifetime=costs.at[\"fuel cell\", \"lifetime\"], carrier=\"H2 fuel cell\", ) H2_under_nodes_ = pd.Index(config[\"H2\"][\"geo_storage_nodes\"]) H2_type1_nodes_ = nodes.difference(H2_under_nodes_) H2_under_nodes = H2_under_nodes_.intersection(nodes) H2_type1_nodes = H2_type1_nodes_.intersection(nodes) if not ( H2_under_nodes_.shape == H2_under_nodes.shape and H2_type1_nodes_.shape == H2_type1_nodes.shape ): logger.warning(\"Some H2 storage nodes are not in the network buses\") network.add( \"Store\", H2_under_nodes + \" H2 Store\", bus=H2_under_nodes + \" H2\", e_nom_extendable=True, e_cyclic=True, capital_cost=costs.at[\"hydrogen storage underground\", \"capital_cost\"], lifetime=costs.at[\"hydrogen storage underground\", \"lifetime\"], ) network.add( \"Store\", H2_type1_nodes + \" H2 Store\", bus=H2_type1_nodes + \" H2\", e_nom_extendable=True, e_cyclic=True, capital_cost=costs.at[\"hydrogen storage tank type 1 including compressor\", \"capital_cost\"], lifetime=costs.at[\"hydrogen storage tank type 1 including compressor\", \"lifetime\"], ) if config[\"add_methanation\"]: cost_year = snakemake.wildcards[\"planning_horizons\"] network.add( \"Link\", nodes + \" Sabatier\", bus0=nodes + \" H2\", bus1=nodes + \" gas\", carrier=\"Sabatier\", p_nom_extendable=True, efficiency=costs.at[\"methanation\", \"efficiency\"], capital_cost=costs.at[\"methanation\", \"efficiency\"] * costs.at[\"methanation\", \"capital_cost\"] + costs.at[\"direct air capture\", \"capital_cost\"] * costs.at[\"gas\", \"co2_emissions\"] * costs.at[\"methanation\", \"efficiency\"], # TODO fix me lifetime=costs.at[\"methanation\", \"lifetime\"], marginal_cost=(400 - 5 * (int(cost_year) - 2020)) * costs.at[\"gas\", \"co2_emissions\"] * costs.at[\"methanation\", \"efficiency\"], ) if config[\"Techs\"][\"hydrogen_lines\"]: edge_path = config[\"edge_paths\"].get(config[\"scenario\"][\"topology\"], None) if edge_path is None: raise ValueError(f\"No grid found for topology {config['scenario']['topology']}\") else: edges_ = pd.read_csv( edge_path, sep=\",\", header=None, names=[\"bus0\", \"bus1\", \"p_nom\"] ).fillna(0) edges = edges_[edges_[\"bus0\"].isin(nodes) & edges_[\"bus1\"].isin(nodes)] if edges_.shape[0] != edges.shape[0]: logger.warning(\"Some edges are not in the network buses\") # fix this to use map with x.y lengths = NON_LIN_PATH_SCALING * np.array( [ haversine( [network.buses.at[bus0, \"x\"], network.buses.at[bus0, \"y\"]], [network.buses.at[bus1, \"x\"], network.buses.at[bus1, \"y\"]], ) for bus0, bus1 in edges[[\"bus0\", \"bus1\"]].values ] ) cc = costs.at[\"H2 (g) pipeline\", \"capital_cost\"] * lengths # === h2 pipeline with losses ==== # NB this only works if there is an equalising constraint, which is hidden in solve_ntwk network.add( \"Link\", edges[\"bus0\"] + \"-\" + edges[\"bus1\"] + \" H2 pipeline\", suffix=\" positive\", bus0=edges[\"bus0\"].values + \" H2\", bus1=edges[\"bus1\"].values + \" H2\", bus2=edges[\"bus0\"].values, carrier=\"H2 pipeline\", p_nom_extendable=True, p_nom=0, p_nom_min=0, p_min_pu=0, efficiency=config[\"transmission_efficiency\"][\"H2 pipeline\"][\"efficiency_static\"] * config[\"transmission_efficiency\"][\"H2 pipeline\"][\"efficiency_per_1000km\"] ** (lengths / 1000), efficiency2=-config[\"transmission_efficiency\"][\"H2 pipeline\"][\"compression_per_1000km\"] * lengths / 1e3, length=lengths, lifetime=costs.at[\"H2 (g) pipeline\", \"lifetime\"], capital_cost=cc, ) network.add( \"Link\", edges[\"bus0\"] + \"-\" + edges[\"bus1\"] + \" H2 pipeline\", suffix=\" reversed\", carrier=\"H2 pipeline\", bus0=edges[\"bus1\"].values + \" H2\", bus1=edges[\"bus0\"].values + \" H2\", bus2=edges[\"bus1\"].values, p_nom_extendable=True, p_nom=0, p_nom_min=0, p_min_pu=0, efficiency=config[\"transmission_efficiency\"][\"H2 pipeline\"][\"efficiency_static\"] * config[\"transmission_efficiency\"][\"H2 pipeline\"][\"efficiency_per_1000km\"] ** (lengths / 1000), efficiency2=-config[\"transmission_efficiency\"][\"H2 pipeline\"][\"compression_per_1000km\"] * lengths / 1e3, length=lengths, lifetime=costs.at[\"H2 (g) pipeline\", \"lifetime\"], capital_cost=0, ) add_carriers(network, config, costs) add the various carriers to the network based on the config file Parameters: network ( Network ) \u2013 the pypsa network config ( dict ) \u2013 the config file costs ( DataFrame ) \u2013 the costs dataframe Source code in workflow/scripts/prepare_network.py def add_carriers(network: pypsa.Network, config: dict, costs: pd.DataFrame): \"\"\"add the various carriers to the network based on the config file Args: network (pypsa.Network): the pypsa network config (dict): the config file costs (pd.DataFrame): the costs dataframe \"\"\" network.add(\"Carrier\", \"AC\") if config[\"heat_coupling\"]: network.add(\"Carrier\", \"heat\") for carrier in config[\"Techs\"][\"vre_techs\"]: network.add(\"Carrier\", carrier) if carrier == \"hydroelectricity\": network.add(\"Carrier\", \"hydro_inflow\") for carrier in config[\"Techs\"][\"store_techs\"]: network.add(\"Carrier\", carrier) if carrier == \"battery\": network.add(\"Carrier\", \"battery discharger\") # add fuel carriers, emissions in # in t_CO2/MWht if config[\"add_gas\"]: network.add(\"Carrier\", \"gas\", co2_emissions=costs.at[\"gas\", \"co2_emissions\"]) if config[\"add_coal\"]: network.add(\"Carrier\", \"coal\", co2_emissions=costs.at[\"coal\", \"co2_emissions\"]) add_conventional_generators(network, nodes, config, prov_centroids, costs) add conventional generators to the network Parameters: network ( Network ) \u2013 the pypsa network object nodes ( Index ) \u2013 the nodes config ( dict ) \u2013 the snakemake config prov_centroids ( GeoDataFrame ) \u2013 the x,y locations of the nodes costs ( DataFrame ) \u2013 the costs data base Source code in workflow/scripts/prepare_network.py def add_conventional_generators( network: pypsa.Network, nodes: pd.Index, config: dict, prov_centroids: gpd.GeoDataFrame, costs: pd.DataFrame, ): \"\"\"add conventional generators to the network Args: network (pypsa.Network): the pypsa network object nodes (pd.Index): the nodes config (dict): the snakemake config prov_centroids (gpd.GeoDataFrame): the x,y locations of the nodes costs (pd.DataFrame): the costs data base \"\"\" if config[\"add_gas\"]: # add converter from fuel source network.add( \"Bus\", nodes, suffix=\" gas\", x=prov_centroids.x, y=prov_centroids.y, carrier=\"gas\", location=nodes, ) network.add( \"Generator\", nodes, suffix=\" gas fuel\", bus=nodes + \" gas\", carrier=\"gas\", p_nom_extendable=True, p_nom=1e7, marginal_cost=costs.at[\"gas\", \"fuel\"], ) # TODO why not centralised? network.add( \"Store\", nodes + \" gas Store\", bus=nodes + \" gas\", e_nom_extendable=True, carrier=\"gas\", e_nom=1e7, e_cyclic=True, ) network.add( \"Link\", nodes, suffix=\" OCGT\", bus0=nodes + \" gas\", bus1=nodes, marginal_cost=costs.at[\"OCGT\", \"efficiency\"] * costs.at[\"OCGT\", \"VOM\"], # NB: VOM is per MWel capital_cost=costs.at[\"OCGT\", \"efficiency\"] * costs.at[\"OCGT\", \"capital_cost\"], # NB: capital cost is per MWel p_nom_extendable=True, efficiency=costs.at[\"OCGT\", \"efficiency\"], lifetime=costs.at[\"OCGT\", \"lifetime\"], carrier=\"gas\", ) if config[\"add_coal\"]: # this is the non sector-coupled approach # for industry may have an issue in that coal feeds to chem sector network.add( \"Generator\", nodes, suffix=\" coal power\", bus=nodes, carrier=\"coal\", p_nom_extendable=True, efficiency=costs.at[\"coal\", \"efficiency\"], marginal_cost=costs.at[\"coal\", \"marginal_cost\"], capital_cost=costs.at[\"coal\", \"efficiency\"] * costs.at[\"coal\", \"capital_cost\"], # NB: capital cost is per MWel lifetime=costs.at[\"coal\", \"lifetime\"], ) add_heat_coupling(network, config, nodes, prov_centroids, costs, planning_year) add the heat-coupling links and generators to the network Parameters: network ( Network ) \u2013 the network object config ( dict ) \u2013 the config nodes ( Index ) \u2013 the node names. Defaults to pd.Index. prov_centroids ( GeoDataFrame ) \u2013 the node locations. costs ( DataFrame ) \u2013 the costs dataframe for emissions Source code in workflow/scripts/prepare_network.py def add_heat_coupling( network: pypsa.Network, config: dict, nodes: pd.Index, prov_centroids: gpd.GeoDataFrame, costs: pd.DataFrame, planning_year: int, ): \"\"\"add the heat-coupling links and generators to the network Args: network (pypsa.Network): the network object config (dict): the config nodes (pd.Index): the node names. Defaults to pd.Index. prov_centroids (gpd.GeoDataFrame): the node locations. costs (pd.DataFrame): the costs dataframe for emissions \"\"\" central_fraction = pd.read_hdf(snakemake.input.central_fraction) with pd.HDFStore(snakemake.input.heat_demand_profile, mode=\"r\") as store: heat_demand = store[\"heat_demand_profiles\"] # TODO fix this if not working heat_demand.index = heat_demand.index.tz_localize(None) heat_demand = heat_demand.loc[network.snapshots] network.add( \"Bus\", nodes, suffix=\" decentral heat\", x=prov_centroids.x, y=prov_centroids.y, carrier=\"heat\", location=nodes, ) network.add( \"Bus\", nodes, suffix=\" central heat\", x=prov_centroids.x, y=prov_centroids.y, carrier=\"heat\", location=nodes, ) network.add( \"Load\", nodes, suffix=\" decentral heat\", bus=nodes + \" decentral heat\", p_set=heat_demand[nodes].multiply(1 - central_fraction[nodes]), ) network.add( \"Load\", nodes, suffix=\" central heat\", bus=nodes + \" central heat\", p_set=heat_demand[nodes].multiply(central_fraction[nodes]), ) if \"heat pump\" in config[\"Techs\"][\"vre_techs\"]: logger.info(f\"loading cop profiles from {snakemake.input.cop_name}\") with pd.HDFStore(snakemake.input.cop_name, mode=\"r\") as store: ashp_cop = store[\"ashp_cop_profiles\"] ashp_cop.index = ashp_cop.index.tz_localize(None) ashp_cop = shift_profile_to_planning_year( ashp_cop, snakemake.wildcards.planning_horizons ) gshp_cop = store[\"gshp_cop_profiles\"] gshp_cop.index = gshp_cop.index.tz_localize(None) gshp_cop = shift_profile_to_planning_year( gshp_cop, snakemake.wildcards.planning_horizons ) for cat in [\" decentral \", \" central \"]: network.add( \"Link\", nodes, suffix=cat + \"heat pump\", bus0=nodes, bus1=nodes + cat + \"heat\", carrier=\"heat pump\", efficiency=( ashp_cop.loc[network.snapshots, nodes] if config[\"time_dep_hp_cop\"] else costs.at[cat.lstrip() + \"air-sourced heat pump\", \"efficiency\"] ), capital_cost=costs.at[cat.lstrip() + \"air-sourced heat pump\", \"efficiency\"] * costs.at[cat.lstrip() + \"air-sourced heat pump\", \"capital_cost\"], marginal_cost=costs.at[cat.lstrip() + \"air-sourced heat pump\", \"efficiency\"] * costs.at[cat.lstrip() + \"air-sourced heat pump\", \"marginal_cost\"], p_nom_extendable=True, lifetime=costs.at[cat.lstrip() + \"air-sourced heat pump\", \"lifetime\"], ) network.add( \"Link\", nodes, suffix=\" ground heat pump\", bus0=nodes, bus1=nodes + \" decentral heat\", carrier=\"heat pump\", efficiency=( gshp_cop.loc[network.snapshots, nodes] if config[\"time_dep_hp_cop\"] else costs.at[\"decentral ground-sourced heat pump\", \"efficiency\"] ), marginal_cost=costs.at[cat.lstrip() + \"ground-sourced heat pump\", \"efficiency\"] * costs.at[cat.lstrip() + \"ground-sourced heat pump\", \"marginal_cost\"], capital_cost=costs.at[cat.lstrip() + \"ground-sourced heat pump\", \"efficiency\"] * costs.at[\"decentral ground-sourced heat pump\", \"capital_cost\"], p_nom_extendable=True, lifetime=costs.at[\"decentral ground-sourced heat pump\", \"lifetime\"], ) if \"water tanks\" in config[\"Techs\"][\"store_techs\"]: for cat in [\" decentral \", \" central \"]: network.add( \"Bus\", nodes, suffix=cat + \"water tanks\", x=prov_centroids.x, y=prov_centroids.y, carrier=\"water tanks\", location=nodes, ) network.add( \"Link\", nodes + cat + \"water tanks charger\", bus0=nodes + cat + \"heat\", bus1=nodes + cat + \"water tanks\", carrier=\"water tanks\", efficiency=costs.at[\"water tank charger\", \"efficiency\"], p_nom_extendable=True, ) network.add( \"Link\", nodes + cat + \"water tanks discharger\", bus0=nodes + cat + \"water tanks\", bus1=nodes + cat + \"heat\", carrier=\"water tanks\", efficiency=costs.at[\"water tank discharger\", \"efficiency\"], p_nom_extendable=True, ) # [HP] 180 day time constant for centralised, 3 day for decentralised tes_tau = config[\"water_tanks\"][\"tes_tau\"][cat.strip()] network.add( \"Store\", nodes + cat + \"water tank\", bus=nodes + cat + \"water tanks\", carrier=\"water tanks\", e_cyclic=True, e_nom_extendable=True, standing_loss=1 - np.exp(-1 / (24.0 * tes_tau)), capital_cost=costs.at[cat.lstrip() + \"water tank storage\", \"capital_cost\"], lifetime=costs.at[cat.lstrip() + \"water tank storage\", \"lifetime\"], ) if \"resistive heater\" in config[\"Techs\"][\"vre_techs\"]: for cat in [\" decentral \", \" central \"]: network.add( \"Link\", nodes + cat + \"resistive heater\", bus0=nodes, bus1=nodes + cat + \"heat\", carrier=\"resistive heater\", efficiency=costs.at[cat.lstrip() + \"resistive heater\", \"efficiency\"], capital_cost=costs.at[cat.lstrip() + \"resistive heater\", \"efficiency\"] * costs.at[cat.lstrip() + \"resistive heater\", \"capital_cost\"], marginal_cost=costs.at[cat.lstrip() + \"resistive heater\", \"efficiency\"] * costs.at[cat.lstrip() + \"resistive heater\", \"marginal_cost\"], p_nom_extendable=True, lifetime=costs.at[cat.lstrip() + \"resistive heater\", \"lifetime\"], ) if \"H2 CHP\" in config[\"Techs\"][\"vre_techs\"] and config[\"add_H2\"] and config[\"heat_coupling\"]: network.add( \"Bus\", nodes, suffix=\" central H2 CHP\", x=prov_centroids.x, y=prov_centroids.y, carrier=\"H2\", location=nodes, ) network.add( \"Link\", name=nodes + \" central H2 CHP\", bus0=nodes + \" H2\", bus1=nodes, bus2=nodes + \" central heat\", p_nom_extendable=True, carrier=\"H2 CHP\", efficiency=costs.at[\"central hydrogen CHP\", \"efficiency\"], efficiency2=costs.at[\"central hydrogen CHP\", \"efficiency\"] / costs.at[\"central hydrogen CHP\", \"c_b\"], capital_cost=costs.at[\"central hydrogen CHP\", \"efficiency\"] * costs.at[\"central hydrogen CHP\", \"capital_cost\"], lifetime=costs.at[\"central hydrogen CHP\", \"lifetime\"], ) if \"gas boiler\" in config[\"Techs\"][\"conv_techs\"]: for cat in [\" decentral \", \" central \"]: network.add( \"Link\", nodes + cat + \"gas boiler\", p_nom_extendable=True, bus0=nodes + \" gas\", bus1=nodes + cat + \"heat\", efficiency=costs.at[cat.lstrip() + \"gas boiler\", \"efficiency\"], marginal_cost=costs.at[cat.lstrip() + \"gas boiler\", \"VOM\"], capital_cost=costs.at[cat.lstrip() + \"gas boiler\", \"efficiency\"] * costs.at[cat.lstrip() + \"gas boiler\", \"capital_cost\"], lifetime=costs.at[cat.lstrip() + \"gas boiler\", \"lifetime\"], ) if \"CHP gas\" in config[\"Techs\"][\"conv_techs\"]: # TODO merge with gas ? network.add( \"Bus\", nodes, suffix=\" CHP gas\", x=prov_centroids.x, y=prov_centroids.y, carrier=\"gas\", location=nodes, ) network.add( \"Generator\", name=nodes + \" CHP gas\", bus=nodes + \" CHP gas\", carrier=\"gas\", p_nom_extendable=True, marginal_cost=costs.at[\"gas\", \"marginal_cost\"], ) # TODO why is not combined cycle? # TODO efficiency to be understood - DK doc not clear network.add( \"Link\", nodes, suffix=\" CHP gas\", bus0=nodes + \" gas\", bus1=nodes, bus2=nodes + \" central heat\", p_nom_extendable=True, marginal_cost=costs.at[\"central gas CHP\", \"efficiency\"] * costs.at[\"central gas CHP\", \"VOM\"], # NB: VOM is per MWel capital_cost=costs.at[\"central gas CHP\", \"efficiency\"] * costs.at[\"central gas CHP\", \"capital_cost\"], # NB: capital cost is per MWel efficiency=config[\"chp_parameters\"][\"eff_el\"], efficiency2=config[\"chp_parameters\"][\"eff_th\"], lifetime=costs.at[\"central gas CHP\", \"lifetime\"], ) if \"CHP coal\" in config[\"Techs\"][\"conv_techs\"]: # TODO merge with normal coal? network.add( \"Bus\", nodes, suffix=\" CHP coal\", x=prov_centroids.x, y=prov_centroids.y, carrier=\"coal\", location=nodes, ) network.add( \"Generator\", name=nodes + \" CHP coal\", bus=nodes + \" CHP coal\", carrier=\"coal\", p_nom_extendable=True, marginal_cost=costs.at[\"coal\", \"marginal_cost\"], ) network.add( \"Link\", name=nodes, suffix=\" CHP coal\", bus0=nodes + \" CHP coal\", bus1=nodes, bus2=nodes + \" central heat\", p_nom_extendable=True, marginal_cost=costs.at[\"central coal CHP\", \"efficiency\"] * costs.at[\"central coal CHP\", \"VOM\"], # NB: VOM is per MWel capital_cost=costs.at[\"central coal CHP\", \"efficiency\"] * costs.at[\"central coal CHP\", \"capital_cost\"], # NB: capital cost is per MWel efficiency=config[\"chp_parameters\"][\"eff_el\"], efficiency2=config[\"chp_parameters\"][\"eff_th\"], lifetime=costs.at[\"central coal CHP\", \"lifetime\"], ) if \"solar thermal\" in config[\"Techs\"][\"vre_techs\"]: # this is the amount of heat collected in W per m^2, accounting # for efficiency with pd.HDFStore(snakemake.input.solar_thermal_name, mode=\"r\") as store: # 1e3 converts from W/m^2 to MW/(1000m^2) = kW/m^2 solar_thermal = config[\"solar_cf_correction\"] * store[\"solar_thermal_profiles\"] / 1e3 solar_thermal.index = solar_thermal.index.tz_localize(None) solar_thermal = shift_profile_to_planning_year(solar_thermal, planning_year) solar_thermal = solar_thermal.loc[network.snapshots] for cat in [\" decentral \"]: network.add( \"Generator\", nodes, suffix=cat + \"solar thermal\", bus=nodes + cat + \"heat\", carrier=\"solar thermal\", p_nom_extendable=True, capital_cost=costs.at[cat.lstrip() + \"solar thermal\", \"capital_cost\"], p_max_pu=solar_thermal[nodes].clip(1.0e-4), lifetime=costs.at[cat.lstrip() + \"solar thermal\", \"lifetime\"], ) add_hydro(network, config, nodes, prov_shapes, costs, planning_horizons) Add the hydropower plants (dams) to the network. Due to the spillage/basin calculations these have real locations not just nodes. WARNING: the node is assigned based on the damn province name (turbine link) NOT future proof Parameters: network ( Network ) \u2013 the network object config ( dict ) \u2013 the yaml config nodes ( Index ) \u2013 the buses prov_shapes ( GeoDataFrame ) \u2013 the province shapes GDF costs ( DataFrame ) \u2013 the costs dataframe planning_horizons ( int ) \u2013 the year Source code in workflow/scripts/prepare_network.py def add_hydro( network: pypsa.Network, config: dict, nodes: pd.Index, prov_shapes: gpd.GeoDataFrame, costs: pd.DataFrame, planning_horizons: int, ): \"\"\"Add the hydropower plants (dams) to the network. Due to the spillage/basin calculations these have real locations not just nodes. WARNING: the node is assigned based on the damn province name (turbine link) NOT future proof Args: network (pypsa.Network): the network object config (dict): the yaml config nodes (pd.Index): the buses prov_shapes (gpd.GeoDataFrame): the province shapes GDF costs (pd.DataFrame): the costs dataframe planning_horizons (int): the year \"\"\" # load dams df = pd.read_csv(config[\"hydro_dams\"][\"dams_path\"], index_col=0) points = df.apply(lambda row: Point(row.Lon, row.Lat), axis=1) dams = gpd.GeoDataFrame(df, geometry=points, crs=CRS) hourly_rng = pd.date_range( config[\"hydro_dams\"][\"inflow_date_start\"], config[\"hydro_dams\"][\"inflow_date_end\"], freq=\"1h\", inclusive=\"left\", ) # TODO implement inflow calc, understand resolution (seems daily!) inflow = pd.read_pickle(config[\"hydro_dams\"][\"inflow_path\"]) # select inflow year hourly_rng = hourly_rng[hourly_rng.year == INFLOW_DATA_YR] inflow = inflow.loc[inflow.index.year == INFLOW_DATA_YR] inflow = inflow.reindex(hourly_rng, fill_value=0) inflow.columns = dams.index inflow = shift_profile_to_planning_year(inflow, planning_horizons) inflow = inflow.loc[network.snapshots] # m^3/KWh -> m^3/MWh water_consumption_factor = dams.loc[:, \"Water_consumption_factor_avg\"] * 1e3 ####### # ### Add hydro stations as buses network.add( \"Bus\", dams.index, suffix=\" station\", carrier=\"stations\", x=dams[\"geometry\"].to_crs(\"+proj=cea\").centroid.to_crs(prov_shapes.crs).x, y=dams[\"geometry\"].to_crs(\"+proj=cea\").centroid.to_crs(prov_shapes.crs).y, ) dam_buses = network.buses[network.buses.carrier == \"stations\"] # ===== add hydro reservoirs as stores ====== initial_capacity = pd.read_pickle(config[\"hydro_dams\"][\"reservoir_initial_capacity_path\"]) effective_capacity = pd.read_pickle(config[\"hydro_dams\"][\"reservoir_effective_capacity_path\"]) initial_capacity.index = dams.index effective_capacity.index = dams.index initial_capacity = initial_capacity / water_consumption_factor effective_capacity = effective_capacity / water_consumption_factor network.add( \"Store\", dams.index, suffix=\" reservoir\", bus=dam_buses.index, e_nom=effective_capacity, e_initial=initial_capacity, e_cyclic=True, # TODO fix all config[\"costs\"] marginal_cost=config[\"costs\"][\"marginal_cost\"][\"hydro\"], ) # add hydro turbines to link stations to provinces network.add( \"Link\", dams.index, suffix=\" turbines\", bus0=dam_buses.index, bus1=dams[\"Province\"], carrier=\"hydroelectricity\", p_nom=10 * dams[\"installed_capacity_10MW\"], capital_cost=( costs.at[\"hydro\", \"capital_cost\"] if config[\"hydro\"][\"hydro_capital_cost\"] else 0 ), efficiency=1, location=dams[\"Province\"], p_nom_extendable=False, ) # === add rivers to link station to station dam_edges = pd.read_csv(config[\"hydro_dams\"][\"damn_flows_path\"], delimiter=\",\") # === normal flow ==== for row in dam_edges.iterrows(): bus0 = row[1].bus0 + \" turbines\" bus2 = row[1].end_bus + \" station\" network.links.at[bus0, \"bus2\"] = bus2 network.links.at[bus0, \"efficiency2\"] = 1.0 # TODO WHY EXTENDABLE - weather year? for row in dam_edges.iterrows(): bus0 = row[1].bus0 + \" station\" bus1 = row[1].end_bus + \" station\" network.add( \"Link\", \"{}-{}\".format(bus0, bus1) + \" spillage\", bus0=bus0, bus1=bus1, p_nom_extendable=True, ) dam_ends = [ dam for dam in np.unique(dams.index.values) if dam not in dam_edges[\"bus0\"] or dam not in dam_edges[\"end_bus\"] or (dam in dam_edges[\"end_bus\"].values & dam not in dam_edges[\"bus0\"]) ] # need some kind of sink to absorb spillage (e,g ocean). # here hack by flowing to existing bus with 0 efficiency (lose) # TODO make more transparent -> generator with neg sign and 0 c0st for bus0 in dam_ends: network.add( \"Link\", bus0 + \" spillage\", bus0=bus0 + \" station\", bus1=\"Tibet\", p_nom_extendable=True, efficiency=0.0, ) # add inflow as generators # only feed into hydro stations which are the first of a cascade inflow_stations = [ dam for dam in np.unique(dams.index.values) if dam not in dam_edges[\"end_bus\"].values ] for inflow_station in inflow_stations: # p_nom = 1 and p_max_pu & p_min_pu = p_pu, compulsory inflow p_nom = (inflow / water_consumption_factor)[inflow_station].max() p_pu = (inflow / water_consumption_factor)[inflow_station] / p_nom p_pu.index = network.snapshots network.add( \"Generator\", inflow_station + \" inflow\", bus=inflow_station + \" station\", carrier=\"hydro_inflow\", p_max_pu=p_pu.clip(1.0e-6), # p_min_pu=p_pu.clip(1.0e-6), p_nom=p_nom, ) add_voltage_links(network, config) add HVDC/AC links (no KVL) Parameters: network ( Network ) \u2013 the network object config ( dict ) \u2013 the snakemake config Raises: ValueError \u2013 Invalid Edge path in config options Source code in workflow/scripts/prepare_network.py def add_voltage_links(network: pypsa.Network, config: dict): \"\"\"add HVDC/AC links (no KVL) Args: network (pypsa.Network): the network object config (dict): the snakemake config Raises: ValueError: Invalid Edge path in config options \"\"\" represented_hours = network.snapshot_weightings.sum()[0] n_years = represented_hours / 8760.0 # determine topology edge_path = config[\"edge_paths\"].get(config[\"scenario\"][\"topology\"], None) if edge_path is None: raise ValueError(f\"No grid found for topology {config['scenario']['topology']}\") else: edges_ = pd.read_csv( edge_path, sep=\",\", header=None, names=[\"bus0\", \"bus1\", \"p_nom\"] ).fillna(0) edges = edges_[edges_[\"bus0\"].isin(PROV_NAMES) & edges_[\"bus1\"].isin(PROV_NAMES)] if edges_.shape[0] != edges.shape[0]: logger.warning(\"Some edges are not in the network\") # fix this to use map with x.y lengths = NON_LIN_PATH_SCALING * np.array( [ haversine( [network.buses.at[bus0, \"x\"], network.buses.at[bus0, \"y\"]], [network.buses.at[bus1, \"x\"], network.buses.at[bus1, \"y\"]], ) for bus0, bus1 in edges[[\"bus0\", \"bus1\"]].values ] ) cc = ( (config[\"line_cost_factor\"] * lengths * [HVAC_cost_curve(len_) for len_ in lengths]) * LINE_SECURITY_MARGIN * FOM_LINES * n_years * annuity(ECON_LIFETIME_LINES, config[\"costs\"][\"discountrate\"]) ) # ==== lossy transport model (split into 2) ==== # NB this only works if there is an equalising constraint, which is hidden in solve_ntwk if config[\"line_losses\"]: network.add( \"Link\", edges[\"bus0\"] + \"-\" + edges[\"bus1\"], bus0=edges[\"bus0\"].values, bus1=edges[\"bus1\"].values, suffix=\" positive\", p_nom_extendable=True, p_nom=edges[\"p_nom\"].values, p_nom_min=edges[\"p_nom\"].values, p_min_pu=0, efficiency=config[\"transmission_efficiency\"][\"DC\"][\"efficiency_static\"] * config[\"transmission_efficiency\"][\"DC\"][\"efficiency_per_1000km\"] ** (lengths / 1000), length=lengths, capital_cost=cc, ) # 0 len for reversed in case line limits are specified in km network.add( \"Link\", edges[\"bus0\"] + \"-\" + edges[\"bus1\"], bus0=edges[\"bus1\"].values, bus1=edges[\"bus0\"].values, suffix=\" reversed\", p_nom_extendable=True, p_nom=edges[\"p_nom\"].values, p_nom_min=edges[\"p_nom\"].values, p_min_pu=0, efficiency=config[\"transmission_efficiency\"][\"DC\"][\"efficiency_static\"] * config[\"transmission_efficiency\"][\"DC\"][\"efficiency_per_1000km\"] ** (lengths / 1000), length=0, capital_cost=0, ) # lossless transport model else: network.add( \"Link\", edges[\"bus0\"] + \"-\" + edges[\"bus1\"], p_nom=edges[\"p_nom\"].values, p_nom_min=edges[\"p_nom\"].values, bus0=edges[\"bus0\"].values, bus1=edges[\"bus1\"].values, p_nom_extendable=True, p_min_pu=-1, length=lengths, capital_cost=cc, ) generate_periodic_profiles(dt_index=None, col_tzs=pd.Series(index=PROV_NAMES, data=len(PROV_NAMES) * ['Shanghai']), weekly_profile=range(24 * 7)) Give a 24*7 long list of weekly hourly profiles, generate this for each country for the period dt_index, taking account of time zones and Summer Time. Source code in workflow/scripts/prepare_network.py def generate_periodic_profiles( dt_index=None, col_tzs=pd.Series(index=PROV_NAMES, data=len(PROV_NAMES) * [\"Shanghai\"]), weekly_profile=range(24 * 7), ): \"\"\"Give a 24*7 long list of weekly hourly profiles, generate this for each country for the period dt_index, taking account of time zones and Summer Time.\"\"\" weekly_profile = pd.Series(weekly_profile, range(24 * 7)) # TODO fix, no longer take into accoutn summer time # ALSO ADD A TODO in base_network week_df = pd.DataFrame(index=dt_index, columns=col_tzs.index) for ct in col_tzs.index: week_df[ct] = [24 * dt.weekday() + dt.hour for dt in dt_index.tz_localize(None)] week_df[ct] = week_df[ct].map(weekly_profile) return week_df prepare_network(config) Prepares/makes the network object for overnight mode according to config & at 1 node per region/province Parameters: config ( dict ) \u2013 the snakemake config Returns: Network \u2013 pypsa.Network: the pypsa network object Source code in workflow/scripts/prepare_network.py def prepare_network(config: dict) -> pypsa.Network: \"\"\"Prepares/makes the network object for overnight mode according to config & at 1 node per region/province Args: config (dict): the snakemake config Returns: pypsa.Network: the pypsa network object \"\"\" # determine whether gas/coal to be added depending on specified conv techs config[\"add_gas\"] = ( True if [tech for tech in config[\"Techs\"][\"conv_techs\"] if \"gas\" in tech] else False ) config[\"add_coal\"] = ( True if [tech for tech in config[\"Techs\"][\"conv_techs\"] if \"coal\" in tech] else False ) planning_horizons = snakemake.wildcards[\"planning_horizons\"] # Build the Network object, which stores all other objects network = pypsa.Network() # load graph nodes = pd.Index(PROV_NAMES) # make snapshots (drop leap days) -> possibly do all the unpacking in the function snapshot_cfg = config[\"snapshots\"] snapshots = make_periodic_snapshots( year=planning_horizons, freq=snapshot_cfg[\"freq\"], start_day_hour=snapshot_cfg[\"start\"], end_day_hour=snapshot_cfg[\"end\"], bounds=snapshot_cfg[\"bounds\"], # naive local timezone tz=None, end_year=(None if not snapshot_cfg[\"end_year_plus1\"] else planning_horizons + 1), ) network.set_snapshots(snapshots) network.snapshot_weightings[:] = config[\"snapshots\"][\"frequency\"] represented_hours = network.snapshot_weightings.sum()[0] n_years = represented_hours / 8760.0 # load costs tech_costs = snakemake.input.tech_costs cost_year = planning_horizons costs = load_costs(tech_costs, config[\"costs\"], config[\"electricity\"], cost_year, n_years) # TODO check crs projection correct # load provinces prov_shapes = read_province_shapes(snakemake.input.province_shape) prov_centroids = prov_shapes.to_crs(\"+proj=cea\").centroid.to_crs(CRS) # add AC buses network.add(\"Bus\", nodes, x=prov_centroids.x, y=prov_centroids.y, location=nodes) # add carriers add_carriers(network, config, costs) # load datasets calculated by build_renewable_profiles ds_solar = xr.open_dataset(snakemake.input.profile_solar) ds_onwind = xr.open_dataset(snakemake.input.profile_onwind) ds_offwind = xr.open_dataset(snakemake.input.profile_offwind) # == shift datasets from reference to planning year, sort columns to match network bus order == solar_p_max_pu = calc_renewable_pu_avail(ds_solar, planning_horizons, snapshots) onwind_p_max_pu = calc_renewable_pu_avail(ds_onwind, planning_horizons, snapshots) offwind_p_max_pu = calc_renewable_pu_avail(ds_offwind, planning_horizons, snapshots) # TODO SOFT CODE BASE YEAR if config[\"scenario\"][\"co2_reduction\"] is None: pass elif isinstance(config[\"scenario\"][\"co2_reduction\"], dict): logger.info(\"Adding CO2 constraint based on scenario\") pathway = snakemake.wildcards[\"pathway\"] reduction = float(config[\"scenario\"][\"co2_reduction\"][pathway][str(planning_horizons)]) co2_limit = (CO2_EL_2020 + CO2_HEATING_2020) * (1 - reduction) network.add( \"GlobalConstraint\", \"co2_limit\", type=\"primary_energy\", carrier_attribute=\"co2_emissions\", sense=\"<=\", constant=co2_limit, ) elif not isinstance(config[\"scenario\"][\"co2_reduction\"], tuple): logger.info(\"Adding CO2 constraint based on scenario\") # TODO fix hard coded co2_limit = (CO2_EL_2020 + CO2_HEATING_2020) * ( 1 - float(config[\"scenario\"][\"co2_reduction\"]) ) # Chinese 2020 CO2 emissions of electric and heating sector network.add( \"GlobalConstraint\", \"co2_limit\", type=\"primary_energy\", carrier_attribute=\"co2_emissions\", sense=\"<=\", constant=co2_limit, ) else: logger.error(f\"Unhandled CO2 config {config[\"scenario\"][\"co2_reduction\"]}.\") raise ValueError(f\"Unhandled CO2 config {config[\"scenario\"][\"co2_reduction\"]}\") # load electricity demand data demand_path = snakemake.input.elec_load.replace(\"{planning_horizons}\", f\"{cost_year}\") with pd.HDFStore(demand_path, mode=\"r\") as store: load = LOAD_CONVERSION_FACTOR * store[\"load\"] # TODO add unit load = load.loc[network.snapshots, PROV_NAMES] network.add(\"Load\", nodes, bus=nodes, p_set=load[nodes]) # add renewables network.add( \"Generator\", nodes, suffix=\" onwind\", bus=nodes, carrier=\"onwind\", p_nom_extendable=True, p_nom_max=ds_onwind[\"p_nom_max\"].to_pandas(), capital_cost=costs.at[\"onwind\", \"capital_cost\"], marginal_cost=costs.at[\"onwind\", \"marginal_cost\"], p_max_pu=onwind_p_max_pu, lifetime=costs.at[\"onwind\", \"lifetime\"], ) offwind_nodes = ds_offwind[\"bus\"].to_pandas().index network.add( \"Generator\", offwind_nodes, suffix=\" offwind\", bus=offwind_nodes, carrier=\"offwind\", p_nom_extendable=True, p_nom_max=ds_offwind[\"p_nom_max\"].to_pandas(), capital_cost=costs.at[\"offwind\", \"capital_cost\"], marginal_cost=costs.at[\"offwind\", \"marginal_cost\"], p_max_pu=offwind_p_max_pu, lifetime=costs.at[\"offwind\", \"lifetime\"], ) network.add( \"Generator\", nodes, suffix=\" solar\", bus=nodes, carrier=\"solar\", p_nom_extendable=True, p_nom_max=ds_solar[\"p_nom_max\"].to_pandas(), capital_cost=costs.at[\"solar\", \"capital_cost\"], marginal_cost=costs.at[\"solar\", \"marginal_cost\"], p_max_pu=solar_p_max_pu, lifetime=costs.at[\"solar\", \"lifetime\"], ) add_conventional_generators(network, nodes, config, prov_centroids, costs) # nuclear is brownfield if \"nuclear\" in config[\"Techs\"][\"vre_techs\"]: nuclear_p_nom = pd.read_csv(config[\"nuclear_reactors\"][\"pp_path\"], index_col=0) nuclear_p_nom = pd.Series(nuclear_p_nom.squeeze()) nuclear_nodes = pd.Index(NUCLEAR_EXTENDABLE) network.add( \"Generator\", nuclear_nodes, suffix=\" nuclear\", p_nom_extendable=True, p_min_pu=0.7, bus=nuclear_nodes, carrier=\"nuclear\", efficiency=costs.at[\"nuclear\", \"efficiency\"], capital_cost=costs.at[\"nuclear\", \"capital_cost\"], # NB: capital cost is per MWel marginal_cost=costs.at[\"nuclear\", \"marginal_cost\"], lifetime=costs.at[\"nuclear\", \"lifetime\"], ) # TODO add coal CC? no retrofit option if \"PHS\" in config[\"Techs\"][\"store_techs\"]: # pure pumped hydro storage, fixed, 6h energy by default, no inflow hydrocapa_df = pd.read_csv(\"resources/data/hydro/PHS_p_nom.csv\", index_col=0) phss = hydrocapa_df.index[hydrocapa_df[\"MW\"] > 0].intersection(nodes) if config[\"hydro\"][\"hydro_capital_cost\"]: cc = costs.at[\"PHS\", \"capital_cost\"] else: cc = 0.0 network.add( \"StorageUnit\", phss, suffix=\" PHS\", bus=phss, carrier=\"PHS\", p_nom_extendable=False, p_nom=hydrocapa_df.loc[phss][\"MW\"], p_nom_min=hydrocapa_df.loc[phss][\"MW\"], max_hours=config[\"hydro\"][\"PHS_max_hours\"], efficiency_store=np.sqrt(costs.at[\"PHS\", \"efficiency\"]), efficiency_dispatch=np.sqrt(costs.at[\"PHS\", \"efficiency\"]), cyclic_state_of_charge=True, capital_cost=cc, marginal_cost=0.0, ) if config[\"add_hydro\"]: add_hydro(network, config, nodes, prov_centroids, costs, planning_horizons) if config[\"add_H2\"]: # do beore heat coupling to avoid warning network.add( \"Bus\", nodes, suffix=\" H2\", x=prov_centroids.x, y=prov_centroids.y, carrier=\"H2\", location=nodes, ) if config[\"heat_coupling\"]: add_heat_coupling(network, config, nodes, prov_centroids, costs, planning_horizons) if config[\"add_H2\"]: add_H2(network, config, nodes, costs) if \"battery\" in config[\"Techs\"][\"store_techs\"]: network.add( \"Bus\", nodes, suffix=\" battery\", x=prov_centroids.x, y=prov_centroids.y, carrier=\"battery\", location=nodes, ) # TODO Why no standing loss? network.add( \"Store\", nodes + \" battery\", bus=nodes + \" battery\", e_cyclic=True, e_nom_extendable=True, capital_cost=costs.at[\"battery storage\", \"capital_cost\"], lifetime=costs.at[\"battery storage\", \"lifetime\"], ) # TODO understand/remove sources, data should not be in code # Sources: # [HP]: Henning, Palzer http://www.sciencedirect.com/science/article/pii/S1364032113006710 # [B]: Budischak et al. http://www.sciencedirect.com/science/article/pii/S0378775312014759 network.add( \"Link\", nodes + \" battery charger\", bus0=nodes, bus1=nodes + \" battery\", efficiency=costs.at[\"battery inverter\", \"efficiency\"] ** 0.5, capital_cost=costs.at[\"battery inverter\", \"efficiency\"] * costs.at[\"battery inverter\", \"capital_cost\"], p_nom_extendable=True, carrier=\"battery\", lifetime=costs.at[\"battery inverter\", \"lifetime\"], ) network.add( \"Link\", nodes + \" battery discharger\", bus0=nodes + \" battery\", bus1=nodes, efficiency=costs.at[\"battery inverter\", \"efficiency\"] ** 0.5, marginal_cost=0.0, p_nom_extendable=True, carrier=\"battery discharger\", ) # ============= add lines ========= # The lines are implemented according to the transport model (no KVL) and without losses. # see Neumann et al 10.1016/j.apenergy.2022.118859 # TODO make not lossless optional (? - increases computing cost) if not config[\"no_lines\"]: add_voltage_links(network, config) assign_locations(network) return network","title":"prepare_network"},{"location":"docs/reference/prepare_network/#prepare_network.add_H2","text":"add H2 generators, storage and links to the network - currently all or nothing Parameters: network ( Network ) \u2013 network object too which H2 comps will be added config ( dict ) \u2013 the config (snakemake config) nodes ( Index ) \u2013 the buses costs ( DataFrame ) \u2013 the cost database Source code in workflow/scripts/prepare_network.py def add_H2(network: pypsa.Network, config: dict, nodes: pd.Index, costs: pd.DataFrame): \"\"\"add H2 generators, storage and links to the network - currently all or nothing Args: network (pypsa.Network): network object too which H2 comps will be added config (dict): the config (snakemake config) nodes (pd.Index): the buses costs (pd.DataFrame): the cost database \"\"\" network.add( \"Link\", name=nodes + \" H2 Electrolysis\", bus0=nodes, bus1=nodes + \" H2\", bus2=nodes + \" central heat\", p_nom_extendable=True, carrier=\"H2 Electrolysis\", efficiency=costs.at[\"electrolysis\", \"efficiency\"], efficiency2=costs.at[\"electrolysis\", \"efficiency-heat\"], capital_cost=costs.at[\"electrolysis\", \"capital_cost\"], lifetime=costs.at[\"electrolysis\", \"lifetime\"], ) # TODO consider switching to turbines and making a switch for off # TODO understand MVs network.add( \"Link\", name=nodes + \" H2 Fuel Cell\", bus0=nodes + \" H2\", bus1=nodes, p_nom_extendable=True, efficiency=costs.at[\"fuel cell\", \"efficiency\"], capital_cost=costs.at[\"fuel cell\", \"efficiency\"] * costs.at[\"fuel cell\", \"capital_cost\"], lifetime=costs.at[\"fuel cell\", \"lifetime\"], carrier=\"H2 fuel cell\", ) H2_under_nodes_ = pd.Index(config[\"H2\"][\"geo_storage_nodes\"]) H2_type1_nodes_ = nodes.difference(H2_under_nodes_) H2_under_nodes = H2_under_nodes_.intersection(nodes) H2_type1_nodes = H2_type1_nodes_.intersection(nodes) if not ( H2_under_nodes_.shape == H2_under_nodes.shape and H2_type1_nodes_.shape == H2_type1_nodes.shape ): logger.warning(\"Some H2 storage nodes are not in the network buses\") network.add( \"Store\", H2_under_nodes + \" H2 Store\", bus=H2_under_nodes + \" H2\", e_nom_extendable=True, e_cyclic=True, capital_cost=costs.at[\"hydrogen storage underground\", \"capital_cost\"], lifetime=costs.at[\"hydrogen storage underground\", \"lifetime\"], ) network.add( \"Store\", H2_type1_nodes + \" H2 Store\", bus=H2_type1_nodes + \" H2\", e_nom_extendable=True, e_cyclic=True, capital_cost=costs.at[\"hydrogen storage tank type 1 including compressor\", \"capital_cost\"], lifetime=costs.at[\"hydrogen storage tank type 1 including compressor\", \"lifetime\"], ) if config[\"add_methanation\"]: cost_year = snakemake.wildcards[\"planning_horizons\"] network.add( \"Link\", nodes + \" Sabatier\", bus0=nodes + \" H2\", bus1=nodes + \" gas\", carrier=\"Sabatier\", p_nom_extendable=True, efficiency=costs.at[\"methanation\", \"efficiency\"], capital_cost=costs.at[\"methanation\", \"efficiency\"] * costs.at[\"methanation\", \"capital_cost\"] + costs.at[\"direct air capture\", \"capital_cost\"] * costs.at[\"gas\", \"co2_emissions\"] * costs.at[\"methanation\", \"efficiency\"], # TODO fix me lifetime=costs.at[\"methanation\", \"lifetime\"], marginal_cost=(400 - 5 * (int(cost_year) - 2020)) * costs.at[\"gas\", \"co2_emissions\"] * costs.at[\"methanation\", \"efficiency\"], ) if config[\"Techs\"][\"hydrogen_lines\"]: edge_path = config[\"edge_paths\"].get(config[\"scenario\"][\"topology\"], None) if edge_path is None: raise ValueError(f\"No grid found for topology {config['scenario']['topology']}\") else: edges_ = pd.read_csv( edge_path, sep=\",\", header=None, names=[\"bus0\", \"bus1\", \"p_nom\"] ).fillna(0) edges = edges_[edges_[\"bus0\"].isin(nodes) & edges_[\"bus1\"].isin(nodes)] if edges_.shape[0] != edges.shape[0]: logger.warning(\"Some edges are not in the network buses\") # fix this to use map with x.y lengths = NON_LIN_PATH_SCALING * np.array( [ haversine( [network.buses.at[bus0, \"x\"], network.buses.at[bus0, \"y\"]], [network.buses.at[bus1, \"x\"], network.buses.at[bus1, \"y\"]], ) for bus0, bus1 in edges[[\"bus0\", \"bus1\"]].values ] ) cc = costs.at[\"H2 (g) pipeline\", \"capital_cost\"] * lengths # === h2 pipeline with losses ==== # NB this only works if there is an equalising constraint, which is hidden in solve_ntwk network.add( \"Link\", edges[\"bus0\"] + \"-\" + edges[\"bus1\"] + \" H2 pipeline\", suffix=\" positive\", bus0=edges[\"bus0\"].values + \" H2\", bus1=edges[\"bus1\"].values + \" H2\", bus2=edges[\"bus0\"].values, carrier=\"H2 pipeline\", p_nom_extendable=True, p_nom=0, p_nom_min=0, p_min_pu=0, efficiency=config[\"transmission_efficiency\"][\"H2 pipeline\"][\"efficiency_static\"] * config[\"transmission_efficiency\"][\"H2 pipeline\"][\"efficiency_per_1000km\"] ** (lengths / 1000), efficiency2=-config[\"transmission_efficiency\"][\"H2 pipeline\"][\"compression_per_1000km\"] * lengths / 1e3, length=lengths, lifetime=costs.at[\"H2 (g) pipeline\", \"lifetime\"], capital_cost=cc, ) network.add( \"Link\", edges[\"bus0\"] + \"-\" + edges[\"bus1\"] + \" H2 pipeline\", suffix=\" reversed\", carrier=\"H2 pipeline\", bus0=edges[\"bus1\"].values + \" H2\", bus1=edges[\"bus0\"].values + \" H2\", bus2=edges[\"bus1\"].values, p_nom_extendable=True, p_nom=0, p_nom_min=0, p_min_pu=0, efficiency=config[\"transmission_efficiency\"][\"H2 pipeline\"][\"efficiency_static\"] * config[\"transmission_efficiency\"][\"H2 pipeline\"][\"efficiency_per_1000km\"] ** (lengths / 1000), efficiency2=-config[\"transmission_efficiency\"][\"H2 pipeline\"][\"compression_per_1000km\"] * lengths / 1e3, length=lengths, lifetime=costs.at[\"H2 (g) pipeline\", \"lifetime\"], capital_cost=0, )","title":"add_H2"},{"location":"docs/reference/prepare_network/#prepare_network.add_carriers","text":"add the various carriers to the network based on the config file Parameters: network ( Network ) \u2013 the pypsa network config ( dict ) \u2013 the config file costs ( DataFrame ) \u2013 the costs dataframe Source code in workflow/scripts/prepare_network.py def add_carriers(network: pypsa.Network, config: dict, costs: pd.DataFrame): \"\"\"add the various carriers to the network based on the config file Args: network (pypsa.Network): the pypsa network config (dict): the config file costs (pd.DataFrame): the costs dataframe \"\"\" network.add(\"Carrier\", \"AC\") if config[\"heat_coupling\"]: network.add(\"Carrier\", \"heat\") for carrier in config[\"Techs\"][\"vre_techs\"]: network.add(\"Carrier\", carrier) if carrier == \"hydroelectricity\": network.add(\"Carrier\", \"hydro_inflow\") for carrier in config[\"Techs\"][\"store_techs\"]: network.add(\"Carrier\", carrier) if carrier == \"battery\": network.add(\"Carrier\", \"battery discharger\") # add fuel carriers, emissions in # in t_CO2/MWht if config[\"add_gas\"]: network.add(\"Carrier\", \"gas\", co2_emissions=costs.at[\"gas\", \"co2_emissions\"]) if config[\"add_coal\"]: network.add(\"Carrier\", \"coal\", co2_emissions=costs.at[\"coal\", \"co2_emissions\"])","title":"add_carriers"},{"location":"docs/reference/prepare_network/#prepare_network.add_conventional_generators","text":"add conventional generators to the network Parameters: network ( Network ) \u2013 the pypsa network object nodes ( Index ) \u2013 the nodes config ( dict ) \u2013 the snakemake config prov_centroids ( GeoDataFrame ) \u2013 the x,y locations of the nodes costs ( DataFrame ) \u2013 the costs data base Source code in workflow/scripts/prepare_network.py def add_conventional_generators( network: pypsa.Network, nodes: pd.Index, config: dict, prov_centroids: gpd.GeoDataFrame, costs: pd.DataFrame, ): \"\"\"add conventional generators to the network Args: network (pypsa.Network): the pypsa network object nodes (pd.Index): the nodes config (dict): the snakemake config prov_centroids (gpd.GeoDataFrame): the x,y locations of the nodes costs (pd.DataFrame): the costs data base \"\"\" if config[\"add_gas\"]: # add converter from fuel source network.add( \"Bus\", nodes, suffix=\" gas\", x=prov_centroids.x, y=prov_centroids.y, carrier=\"gas\", location=nodes, ) network.add( \"Generator\", nodes, suffix=\" gas fuel\", bus=nodes + \" gas\", carrier=\"gas\", p_nom_extendable=True, p_nom=1e7, marginal_cost=costs.at[\"gas\", \"fuel\"], ) # TODO why not centralised? network.add( \"Store\", nodes + \" gas Store\", bus=nodes + \" gas\", e_nom_extendable=True, carrier=\"gas\", e_nom=1e7, e_cyclic=True, ) network.add( \"Link\", nodes, suffix=\" OCGT\", bus0=nodes + \" gas\", bus1=nodes, marginal_cost=costs.at[\"OCGT\", \"efficiency\"] * costs.at[\"OCGT\", \"VOM\"], # NB: VOM is per MWel capital_cost=costs.at[\"OCGT\", \"efficiency\"] * costs.at[\"OCGT\", \"capital_cost\"], # NB: capital cost is per MWel p_nom_extendable=True, efficiency=costs.at[\"OCGT\", \"efficiency\"], lifetime=costs.at[\"OCGT\", \"lifetime\"], carrier=\"gas\", ) if config[\"add_coal\"]: # this is the non sector-coupled approach # for industry may have an issue in that coal feeds to chem sector network.add( \"Generator\", nodes, suffix=\" coal power\", bus=nodes, carrier=\"coal\", p_nom_extendable=True, efficiency=costs.at[\"coal\", \"efficiency\"], marginal_cost=costs.at[\"coal\", \"marginal_cost\"], capital_cost=costs.at[\"coal\", \"efficiency\"] * costs.at[\"coal\", \"capital_cost\"], # NB: capital cost is per MWel lifetime=costs.at[\"coal\", \"lifetime\"], )","title":"add_conventional_generators"},{"location":"docs/reference/prepare_network/#prepare_network.add_heat_coupling","text":"add the heat-coupling links and generators to the network Parameters: network ( Network ) \u2013 the network object config ( dict ) \u2013 the config nodes ( Index ) \u2013 the node names. Defaults to pd.Index. prov_centroids ( GeoDataFrame ) \u2013 the node locations. costs ( DataFrame ) \u2013 the costs dataframe for emissions Source code in workflow/scripts/prepare_network.py def add_heat_coupling( network: pypsa.Network, config: dict, nodes: pd.Index, prov_centroids: gpd.GeoDataFrame, costs: pd.DataFrame, planning_year: int, ): \"\"\"add the heat-coupling links and generators to the network Args: network (pypsa.Network): the network object config (dict): the config nodes (pd.Index): the node names. Defaults to pd.Index. prov_centroids (gpd.GeoDataFrame): the node locations. costs (pd.DataFrame): the costs dataframe for emissions \"\"\" central_fraction = pd.read_hdf(snakemake.input.central_fraction) with pd.HDFStore(snakemake.input.heat_demand_profile, mode=\"r\") as store: heat_demand = store[\"heat_demand_profiles\"] # TODO fix this if not working heat_demand.index = heat_demand.index.tz_localize(None) heat_demand = heat_demand.loc[network.snapshots] network.add( \"Bus\", nodes, suffix=\" decentral heat\", x=prov_centroids.x, y=prov_centroids.y, carrier=\"heat\", location=nodes, ) network.add( \"Bus\", nodes, suffix=\" central heat\", x=prov_centroids.x, y=prov_centroids.y, carrier=\"heat\", location=nodes, ) network.add( \"Load\", nodes, suffix=\" decentral heat\", bus=nodes + \" decentral heat\", p_set=heat_demand[nodes].multiply(1 - central_fraction[nodes]), ) network.add( \"Load\", nodes, suffix=\" central heat\", bus=nodes + \" central heat\", p_set=heat_demand[nodes].multiply(central_fraction[nodes]), ) if \"heat pump\" in config[\"Techs\"][\"vre_techs\"]: logger.info(f\"loading cop profiles from {snakemake.input.cop_name}\") with pd.HDFStore(snakemake.input.cop_name, mode=\"r\") as store: ashp_cop = store[\"ashp_cop_profiles\"] ashp_cop.index = ashp_cop.index.tz_localize(None) ashp_cop = shift_profile_to_planning_year( ashp_cop, snakemake.wildcards.planning_horizons ) gshp_cop = store[\"gshp_cop_profiles\"] gshp_cop.index = gshp_cop.index.tz_localize(None) gshp_cop = shift_profile_to_planning_year( gshp_cop, snakemake.wildcards.planning_horizons ) for cat in [\" decentral \", \" central \"]: network.add( \"Link\", nodes, suffix=cat + \"heat pump\", bus0=nodes, bus1=nodes + cat + \"heat\", carrier=\"heat pump\", efficiency=( ashp_cop.loc[network.snapshots, nodes] if config[\"time_dep_hp_cop\"] else costs.at[cat.lstrip() + \"air-sourced heat pump\", \"efficiency\"] ), capital_cost=costs.at[cat.lstrip() + \"air-sourced heat pump\", \"efficiency\"] * costs.at[cat.lstrip() + \"air-sourced heat pump\", \"capital_cost\"], marginal_cost=costs.at[cat.lstrip() + \"air-sourced heat pump\", \"efficiency\"] * costs.at[cat.lstrip() + \"air-sourced heat pump\", \"marginal_cost\"], p_nom_extendable=True, lifetime=costs.at[cat.lstrip() + \"air-sourced heat pump\", \"lifetime\"], ) network.add( \"Link\", nodes, suffix=\" ground heat pump\", bus0=nodes, bus1=nodes + \" decentral heat\", carrier=\"heat pump\", efficiency=( gshp_cop.loc[network.snapshots, nodes] if config[\"time_dep_hp_cop\"] else costs.at[\"decentral ground-sourced heat pump\", \"efficiency\"] ), marginal_cost=costs.at[cat.lstrip() + \"ground-sourced heat pump\", \"efficiency\"] * costs.at[cat.lstrip() + \"ground-sourced heat pump\", \"marginal_cost\"], capital_cost=costs.at[cat.lstrip() + \"ground-sourced heat pump\", \"efficiency\"] * costs.at[\"decentral ground-sourced heat pump\", \"capital_cost\"], p_nom_extendable=True, lifetime=costs.at[\"decentral ground-sourced heat pump\", \"lifetime\"], ) if \"water tanks\" in config[\"Techs\"][\"store_techs\"]: for cat in [\" decentral \", \" central \"]: network.add( \"Bus\", nodes, suffix=cat + \"water tanks\", x=prov_centroids.x, y=prov_centroids.y, carrier=\"water tanks\", location=nodes, ) network.add( \"Link\", nodes + cat + \"water tanks charger\", bus0=nodes + cat + \"heat\", bus1=nodes + cat + \"water tanks\", carrier=\"water tanks\", efficiency=costs.at[\"water tank charger\", \"efficiency\"], p_nom_extendable=True, ) network.add( \"Link\", nodes + cat + \"water tanks discharger\", bus0=nodes + cat + \"water tanks\", bus1=nodes + cat + \"heat\", carrier=\"water tanks\", efficiency=costs.at[\"water tank discharger\", \"efficiency\"], p_nom_extendable=True, ) # [HP] 180 day time constant for centralised, 3 day for decentralised tes_tau = config[\"water_tanks\"][\"tes_tau\"][cat.strip()] network.add( \"Store\", nodes + cat + \"water tank\", bus=nodes + cat + \"water tanks\", carrier=\"water tanks\", e_cyclic=True, e_nom_extendable=True, standing_loss=1 - np.exp(-1 / (24.0 * tes_tau)), capital_cost=costs.at[cat.lstrip() + \"water tank storage\", \"capital_cost\"], lifetime=costs.at[cat.lstrip() + \"water tank storage\", \"lifetime\"], ) if \"resistive heater\" in config[\"Techs\"][\"vre_techs\"]: for cat in [\" decentral \", \" central \"]: network.add( \"Link\", nodes + cat + \"resistive heater\", bus0=nodes, bus1=nodes + cat + \"heat\", carrier=\"resistive heater\", efficiency=costs.at[cat.lstrip() + \"resistive heater\", \"efficiency\"], capital_cost=costs.at[cat.lstrip() + \"resistive heater\", \"efficiency\"] * costs.at[cat.lstrip() + \"resistive heater\", \"capital_cost\"], marginal_cost=costs.at[cat.lstrip() + \"resistive heater\", \"efficiency\"] * costs.at[cat.lstrip() + \"resistive heater\", \"marginal_cost\"], p_nom_extendable=True, lifetime=costs.at[cat.lstrip() + \"resistive heater\", \"lifetime\"], ) if \"H2 CHP\" in config[\"Techs\"][\"vre_techs\"] and config[\"add_H2\"] and config[\"heat_coupling\"]: network.add( \"Bus\", nodes, suffix=\" central H2 CHP\", x=prov_centroids.x, y=prov_centroids.y, carrier=\"H2\", location=nodes, ) network.add( \"Link\", name=nodes + \" central H2 CHP\", bus0=nodes + \" H2\", bus1=nodes, bus2=nodes + \" central heat\", p_nom_extendable=True, carrier=\"H2 CHP\", efficiency=costs.at[\"central hydrogen CHP\", \"efficiency\"], efficiency2=costs.at[\"central hydrogen CHP\", \"efficiency\"] / costs.at[\"central hydrogen CHP\", \"c_b\"], capital_cost=costs.at[\"central hydrogen CHP\", \"efficiency\"] * costs.at[\"central hydrogen CHP\", \"capital_cost\"], lifetime=costs.at[\"central hydrogen CHP\", \"lifetime\"], ) if \"gas boiler\" in config[\"Techs\"][\"conv_techs\"]: for cat in [\" decentral \", \" central \"]: network.add( \"Link\", nodes + cat + \"gas boiler\", p_nom_extendable=True, bus0=nodes + \" gas\", bus1=nodes + cat + \"heat\", efficiency=costs.at[cat.lstrip() + \"gas boiler\", \"efficiency\"], marginal_cost=costs.at[cat.lstrip() + \"gas boiler\", \"VOM\"], capital_cost=costs.at[cat.lstrip() + \"gas boiler\", \"efficiency\"] * costs.at[cat.lstrip() + \"gas boiler\", \"capital_cost\"], lifetime=costs.at[cat.lstrip() + \"gas boiler\", \"lifetime\"], ) if \"CHP gas\" in config[\"Techs\"][\"conv_techs\"]: # TODO merge with gas ? network.add( \"Bus\", nodes, suffix=\" CHP gas\", x=prov_centroids.x, y=prov_centroids.y, carrier=\"gas\", location=nodes, ) network.add( \"Generator\", name=nodes + \" CHP gas\", bus=nodes + \" CHP gas\", carrier=\"gas\", p_nom_extendable=True, marginal_cost=costs.at[\"gas\", \"marginal_cost\"], ) # TODO why is not combined cycle? # TODO efficiency to be understood - DK doc not clear network.add( \"Link\", nodes, suffix=\" CHP gas\", bus0=nodes + \" gas\", bus1=nodes, bus2=nodes + \" central heat\", p_nom_extendable=True, marginal_cost=costs.at[\"central gas CHP\", \"efficiency\"] * costs.at[\"central gas CHP\", \"VOM\"], # NB: VOM is per MWel capital_cost=costs.at[\"central gas CHP\", \"efficiency\"] * costs.at[\"central gas CHP\", \"capital_cost\"], # NB: capital cost is per MWel efficiency=config[\"chp_parameters\"][\"eff_el\"], efficiency2=config[\"chp_parameters\"][\"eff_th\"], lifetime=costs.at[\"central gas CHP\", \"lifetime\"], ) if \"CHP coal\" in config[\"Techs\"][\"conv_techs\"]: # TODO merge with normal coal? network.add( \"Bus\", nodes, suffix=\" CHP coal\", x=prov_centroids.x, y=prov_centroids.y, carrier=\"coal\", location=nodes, ) network.add( \"Generator\", name=nodes + \" CHP coal\", bus=nodes + \" CHP coal\", carrier=\"coal\", p_nom_extendable=True, marginal_cost=costs.at[\"coal\", \"marginal_cost\"], ) network.add( \"Link\", name=nodes, suffix=\" CHP coal\", bus0=nodes + \" CHP coal\", bus1=nodes, bus2=nodes + \" central heat\", p_nom_extendable=True, marginal_cost=costs.at[\"central coal CHP\", \"efficiency\"] * costs.at[\"central coal CHP\", \"VOM\"], # NB: VOM is per MWel capital_cost=costs.at[\"central coal CHP\", \"efficiency\"] * costs.at[\"central coal CHP\", \"capital_cost\"], # NB: capital cost is per MWel efficiency=config[\"chp_parameters\"][\"eff_el\"], efficiency2=config[\"chp_parameters\"][\"eff_th\"], lifetime=costs.at[\"central coal CHP\", \"lifetime\"], ) if \"solar thermal\" in config[\"Techs\"][\"vre_techs\"]: # this is the amount of heat collected in W per m^2, accounting # for efficiency with pd.HDFStore(snakemake.input.solar_thermal_name, mode=\"r\") as store: # 1e3 converts from W/m^2 to MW/(1000m^2) = kW/m^2 solar_thermal = config[\"solar_cf_correction\"] * store[\"solar_thermal_profiles\"] / 1e3 solar_thermal.index = solar_thermal.index.tz_localize(None) solar_thermal = shift_profile_to_planning_year(solar_thermal, planning_year) solar_thermal = solar_thermal.loc[network.snapshots] for cat in [\" decentral \"]: network.add( \"Generator\", nodes, suffix=cat + \"solar thermal\", bus=nodes + cat + \"heat\", carrier=\"solar thermal\", p_nom_extendable=True, capital_cost=costs.at[cat.lstrip() + \"solar thermal\", \"capital_cost\"], p_max_pu=solar_thermal[nodes].clip(1.0e-4), lifetime=costs.at[cat.lstrip() + \"solar thermal\", \"lifetime\"], )","title":"add_heat_coupling"},{"location":"docs/reference/prepare_network/#prepare_network.add_hydro","text":"Add the hydropower plants (dams) to the network. Due to the spillage/basin calculations these have real locations not just nodes. WARNING: the node is assigned based on the damn province name (turbine link) NOT future proof Parameters: network ( Network ) \u2013 the network object config ( dict ) \u2013 the yaml config nodes ( Index ) \u2013 the buses prov_shapes ( GeoDataFrame ) \u2013 the province shapes GDF costs ( DataFrame ) \u2013 the costs dataframe planning_horizons ( int ) \u2013 the year Source code in workflow/scripts/prepare_network.py def add_hydro( network: pypsa.Network, config: dict, nodes: pd.Index, prov_shapes: gpd.GeoDataFrame, costs: pd.DataFrame, planning_horizons: int, ): \"\"\"Add the hydropower plants (dams) to the network. Due to the spillage/basin calculations these have real locations not just nodes. WARNING: the node is assigned based on the damn province name (turbine link) NOT future proof Args: network (pypsa.Network): the network object config (dict): the yaml config nodes (pd.Index): the buses prov_shapes (gpd.GeoDataFrame): the province shapes GDF costs (pd.DataFrame): the costs dataframe planning_horizons (int): the year \"\"\" # load dams df = pd.read_csv(config[\"hydro_dams\"][\"dams_path\"], index_col=0) points = df.apply(lambda row: Point(row.Lon, row.Lat), axis=1) dams = gpd.GeoDataFrame(df, geometry=points, crs=CRS) hourly_rng = pd.date_range( config[\"hydro_dams\"][\"inflow_date_start\"], config[\"hydro_dams\"][\"inflow_date_end\"], freq=\"1h\", inclusive=\"left\", ) # TODO implement inflow calc, understand resolution (seems daily!) inflow = pd.read_pickle(config[\"hydro_dams\"][\"inflow_path\"]) # select inflow year hourly_rng = hourly_rng[hourly_rng.year == INFLOW_DATA_YR] inflow = inflow.loc[inflow.index.year == INFLOW_DATA_YR] inflow = inflow.reindex(hourly_rng, fill_value=0) inflow.columns = dams.index inflow = shift_profile_to_planning_year(inflow, planning_horizons) inflow = inflow.loc[network.snapshots] # m^3/KWh -> m^3/MWh water_consumption_factor = dams.loc[:, \"Water_consumption_factor_avg\"] * 1e3 ####### # ### Add hydro stations as buses network.add( \"Bus\", dams.index, suffix=\" station\", carrier=\"stations\", x=dams[\"geometry\"].to_crs(\"+proj=cea\").centroid.to_crs(prov_shapes.crs).x, y=dams[\"geometry\"].to_crs(\"+proj=cea\").centroid.to_crs(prov_shapes.crs).y, ) dam_buses = network.buses[network.buses.carrier == \"stations\"] # ===== add hydro reservoirs as stores ====== initial_capacity = pd.read_pickle(config[\"hydro_dams\"][\"reservoir_initial_capacity_path\"]) effective_capacity = pd.read_pickle(config[\"hydro_dams\"][\"reservoir_effective_capacity_path\"]) initial_capacity.index = dams.index effective_capacity.index = dams.index initial_capacity = initial_capacity / water_consumption_factor effective_capacity = effective_capacity / water_consumption_factor network.add( \"Store\", dams.index, suffix=\" reservoir\", bus=dam_buses.index, e_nom=effective_capacity, e_initial=initial_capacity, e_cyclic=True, # TODO fix all config[\"costs\"] marginal_cost=config[\"costs\"][\"marginal_cost\"][\"hydro\"], ) # add hydro turbines to link stations to provinces network.add( \"Link\", dams.index, suffix=\" turbines\", bus0=dam_buses.index, bus1=dams[\"Province\"], carrier=\"hydroelectricity\", p_nom=10 * dams[\"installed_capacity_10MW\"], capital_cost=( costs.at[\"hydro\", \"capital_cost\"] if config[\"hydro\"][\"hydro_capital_cost\"] else 0 ), efficiency=1, location=dams[\"Province\"], p_nom_extendable=False, ) # === add rivers to link station to station dam_edges = pd.read_csv(config[\"hydro_dams\"][\"damn_flows_path\"], delimiter=\",\") # === normal flow ==== for row in dam_edges.iterrows(): bus0 = row[1].bus0 + \" turbines\" bus2 = row[1].end_bus + \" station\" network.links.at[bus0, \"bus2\"] = bus2 network.links.at[bus0, \"efficiency2\"] = 1.0 # TODO WHY EXTENDABLE - weather year? for row in dam_edges.iterrows(): bus0 = row[1].bus0 + \" station\" bus1 = row[1].end_bus + \" station\" network.add( \"Link\", \"{}-{}\".format(bus0, bus1) + \" spillage\", bus0=bus0, bus1=bus1, p_nom_extendable=True, ) dam_ends = [ dam for dam in np.unique(dams.index.values) if dam not in dam_edges[\"bus0\"] or dam not in dam_edges[\"end_bus\"] or (dam in dam_edges[\"end_bus\"].values & dam not in dam_edges[\"bus0\"]) ] # need some kind of sink to absorb spillage (e,g ocean). # here hack by flowing to existing bus with 0 efficiency (lose) # TODO make more transparent -> generator with neg sign and 0 c0st for bus0 in dam_ends: network.add( \"Link\", bus0 + \" spillage\", bus0=bus0 + \" station\", bus1=\"Tibet\", p_nom_extendable=True, efficiency=0.0, ) # add inflow as generators # only feed into hydro stations which are the first of a cascade inflow_stations = [ dam for dam in np.unique(dams.index.values) if dam not in dam_edges[\"end_bus\"].values ] for inflow_station in inflow_stations: # p_nom = 1 and p_max_pu & p_min_pu = p_pu, compulsory inflow p_nom = (inflow / water_consumption_factor)[inflow_station].max() p_pu = (inflow / water_consumption_factor)[inflow_station] / p_nom p_pu.index = network.snapshots network.add( \"Generator\", inflow_station + \" inflow\", bus=inflow_station + \" station\", carrier=\"hydro_inflow\", p_max_pu=p_pu.clip(1.0e-6), # p_min_pu=p_pu.clip(1.0e-6), p_nom=p_nom, )","title":"add_hydro"},{"location":"docs/reference/prepare_network/#prepare_network.add_voltage_links","text":"add HVDC/AC links (no KVL) Parameters: network ( Network ) \u2013 the network object config ( dict ) \u2013 the snakemake config Raises: ValueError \u2013 Invalid Edge path in config options Source code in workflow/scripts/prepare_network.py def add_voltage_links(network: pypsa.Network, config: dict): \"\"\"add HVDC/AC links (no KVL) Args: network (pypsa.Network): the network object config (dict): the snakemake config Raises: ValueError: Invalid Edge path in config options \"\"\" represented_hours = network.snapshot_weightings.sum()[0] n_years = represented_hours / 8760.0 # determine topology edge_path = config[\"edge_paths\"].get(config[\"scenario\"][\"topology\"], None) if edge_path is None: raise ValueError(f\"No grid found for topology {config['scenario']['topology']}\") else: edges_ = pd.read_csv( edge_path, sep=\",\", header=None, names=[\"bus0\", \"bus1\", \"p_nom\"] ).fillna(0) edges = edges_[edges_[\"bus0\"].isin(PROV_NAMES) & edges_[\"bus1\"].isin(PROV_NAMES)] if edges_.shape[0] != edges.shape[0]: logger.warning(\"Some edges are not in the network\") # fix this to use map with x.y lengths = NON_LIN_PATH_SCALING * np.array( [ haversine( [network.buses.at[bus0, \"x\"], network.buses.at[bus0, \"y\"]], [network.buses.at[bus1, \"x\"], network.buses.at[bus1, \"y\"]], ) for bus0, bus1 in edges[[\"bus0\", \"bus1\"]].values ] ) cc = ( (config[\"line_cost_factor\"] * lengths * [HVAC_cost_curve(len_) for len_ in lengths]) * LINE_SECURITY_MARGIN * FOM_LINES * n_years * annuity(ECON_LIFETIME_LINES, config[\"costs\"][\"discountrate\"]) ) # ==== lossy transport model (split into 2) ==== # NB this only works if there is an equalising constraint, which is hidden in solve_ntwk if config[\"line_losses\"]: network.add( \"Link\", edges[\"bus0\"] + \"-\" + edges[\"bus1\"], bus0=edges[\"bus0\"].values, bus1=edges[\"bus1\"].values, suffix=\" positive\", p_nom_extendable=True, p_nom=edges[\"p_nom\"].values, p_nom_min=edges[\"p_nom\"].values, p_min_pu=0, efficiency=config[\"transmission_efficiency\"][\"DC\"][\"efficiency_static\"] * config[\"transmission_efficiency\"][\"DC\"][\"efficiency_per_1000km\"] ** (lengths / 1000), length=lengths, capital_cost=cc, ) # 0 len for reversed in case line limits are specified in km network.add( \"Link\", edges[\"bus0\"] + \"-\" + edges[\"bus1\"], bus0=edges[\"bus1\"].values, bus1=edges[\"bus0\"].values, suffix=\" reversed\", p_nom_extendable=True, p_nom=edges[\"p_nom\"].values, p_nom_min=edges[\"p_nom\"].values, p_min_pu=0, efficiency=config[\"transmission_efficiency\"][\"DC\"][\"efficiency_static\"] * config[\"transmission_efficiency\"][\"DC\"][\"efficiency_per_1000km\"] ** (lengths / 1000), length=0, capital_cost=0, ) # lossless transport model else: network.add( \"Link\", edges[\"bus0\"] + \"-\" + edges[\"bus1\"], p_nom=edges[\"p_nom\"].values, p_nom_min=edges[\"p_nom\"].values, bus0=edges[\"bus0\"].values, bus1=edges[\"bus1\"].values, p_nom_extendable=True, p_min_pu=-1, length=lengths, capital_cost=cc, )","title":"add_voltage_links"},{"location":"docs/reference/prepare_network/#prepare_network.generate_periodic_profiles","text":"Give a 24*7 long list of weekly hourly profiles, generate this for each country for the period dt_index, taking account of time zones and Summer Time. Source code in workflow/scripts/prepare_network.py def generate_periodic_profiles( dt_index=None, col_tzs=pd.Series(index=PROV_NAMES, data=len(PROV_NAMES) * [\"Shanghai\"]), weekly_profile=range(24 * 7), ): \"\"\"Give a 24*7 long list of weekly hourly profiles, generate this for each country for the period dt_index, taking account of time zones and Summer Time.\"\"\" weekly_profile = pd.Series(weekly_profile, range(24 * 7)) # TODO fix, no longer take into accoutn summer time # ALSO ADD A TODO in base_network week_df = pd.DataFrame(index=dt_index, columns=col_tzs.index) for ct in col_tzs.index: week_df[ct] = [24 * dt.weekday() + dt.hour for dt in dt_index.tz_localize(None)] week_df[ct] = week_df[ct].map(weekly_profile) return week_df","title":"generate_periodic_profiles"},{"location":"docs/reference/prepare_network/#prepare_network.prepare_network","text":"Prepares/makes the network object for overnight mode according to config & at 1 node per region/province Parameters: config ( dict ) \u2013 the snakemake config Returns: Network \u2013 pypsa.Network: the pypsa network object Source code in workflow/scripts/prepare_network.py def prepare_network(config: dict) -> pypsa.Network: \"\"\"Prepares/makes the network object for overnight mode according to config & at 1 node per region/province Args: config (dict): the snakemake config Returns: pypsa.Network: the pypsa network object \"\"\" # determine whether gas/coal to be added depending on specified conv techs config[\"add_gas\"] = ( True if [tech for tech in config[\"Techs\"][\"conv_techs\"] if \"gas\" in tech] else False ) config[\"add_coal\"] = ( True if [tech for tech in config[\"Techs\"][\"conv_techs\"] if \"coal\" in tech] else False ) planning_horizons = snakemake.wildcards[\"planning_horizons\"] # Build the Network object, which stores all other objects network = pypsa.Network() # load graph nodes = pd.Index(PROV_NAMES) # make snapshots (drop leap days) -> possibly do all the unpacking in the function snapshot_cfg = config[\"snapshots\"] snapshots = make_periodic_snapshots( year=planning_horizons, freq=snapshot_cfg[\"freq\"], start_day_hour=snapshot_cfg[\"start\"], end_day_hour=snapshot_cfg[\"end\"], bounds=snapshot_cfg[\"bounds\"], # naive local timezone tz=None, end_year=(None if not snapshot_cfg[\"end_year_plus1\"] else planning_horizons + 1), ) network.set_snapshots(snapshots) network.snapshot_weightings[:] = config[\"snapshots\"][\"frequency\"] represented_hours = network.snapshot_weightings.sum()[0] n_years = represented_hours / 8760.0 # load costs tech_costs = snakemake.input.tech_costs cost_year = planning_horizons costs = load_costs(tech_costs, config[\"costs\"], config[\"electricity\"], cost_year, n_years) # TODO check crs projection correct # load provinces prov_shapes = read_province_shapes(snakemake.input.province_shape) prov_centroids = prov_shapes.to_crs(\"+proj=cea\").centroid.to_crs(CRS) # add AC buses network.add(\"Bus\", nodes, x=prov_centroids.x, y=prov_centroids.y, location=nodes) # add carriers add_carriers(network, config, costs) # load datasets calculated by build_renewable_profiles ds_solar = xr.open_dataset(snakemake.input.profile_solar) ds_onwind = xr.open_dataset(snakemake.input.profile_onwind) ds_offwind = xr.open_dataset(snakemake.input.profile_offwind) # == shift datasets from reference to planning year, sort columns to match network bus order == solar_p_max_pu = calc_renewable_pu_avail(ds_solar, planning_horizons, snapshots) onwind_p_max_pu = calc_renewable_pu_avail(ds_onwind, planning_horizons, snapshots) offwind_p_max_pu = calc_renewable_pu_avail(ds_offwind, planning_horizons, snapshots) # TODO SOFT CODE BASE YEAR if config[\"scenario\"][\"co2_reduction\"] is None: pass elif isinstance(config[\"scenario\"][\"co2_reduction\"], dict): logger.info(\"Adding CO2 constraint based on scenario\") pathway = snakemake.wildcards[\"pathway\"] reduction = float(config[\"scenario\"][\"co2_reduction\"][pathway][str(planning_horizons)]) co2_limit = (CO2_EL_2020 + CO2_HEATING_2020) * (1 - reduction) network.add( \"GlobalConstraint\", \"co2_limit\", type=\"primary_energy\", carrier_attribute=\"co2_emissions\", sense=\"<=\", constant=co2_limit, ) elif not isinstance(config[\"scenario\"][\"co2_reduction\"], tuple): logger.info(\"Adding CO2 constraint based on scenario\") # TODO fix hard coded co2_limit = (CO2_EL_2020 + CO2_HEATING_2020) * ( 1 - float(config[\"scenario\"][\"co2_reduction\"]) ) # Chinese 2020 CO2 emissions of electric and heating sector network.add( \"GlobalConstraint\", \"co2_limit\", type=\"primary_energy\", carrier_attribute=\"co2_emissions\", sense=\"<=\", constant=co2_limit, ) else: logger.error(f\"Unhandled CO2 config {config[\"scenario\"][\"co2_reduction\"]}.\") raise ValueError(f\"Unhandled CO2 config {config[\"scenario\"][\"co2_reduction\"]}\") # load electricity demand data demand_path = snakemake.input.elec_load.replace(\"{planning_horizons}\", f\"{cost_year}\") with pd.HDFStore(demand_path, mode=\"r\") as store: load = LOAD_CONVERSION_FACTOR * store[\"load\"] # TODO add unit load = load.loc[network.snapshots, PROV_NAMES] network.add(\"Load\", nodes, bus=nodes, p_set=load[nodes]) # add renewables network.add( \"Generator\", nodes, suffix=\" onwind\", bus=nodes, carrier=\"onwind\", p_nom_extendable=True, p_nom_max=ds_onwind[\"p_nom_max\"].to_pandas(), capital_cost=costs.at[\"onwind\", \"capital_cost\"], marginal_cost=costs.at[\"onwind\", \"marginal_cost\"], p_max_pu=onwind_p_max_pu, lifetime=costs.at[\"onwind\", \"lifetime\"], ) offwind_nodes = ds_offwind[\"bus\"].to_pandas().index network.add( \"Generator\", offwind_nodes, suffix=\" offwind\", bus=offwind_nodes, carrier=\"offwind\", p_nom_extendable=True, p_nom_max=ds_offwind[\"p_nom_max\"].to_pandas(), capital_cost=costs.at[\"offwind\", \"capital_cost\"], marginal_cost=costs.at[\"offwind\", \"marginal_cost\"], p_max_pu=offwind_p_max_pu, lifetime=costs.at[\"offwind\", \"lifetime\"], ) network.add( \"Generator\", nodes, suffix=\" solar\", bus=nodes, carrier=\"solar\", p_nom_extendable=True, p_nom_max=ds_solar[\"p_nom_max\"].to_pandas(), capital_cost=costs.at[\"solar\", \"capital_cost\"], marginal_cost=costs.at[\"solar\", \"marginal_cost\"], p_max_pu=solar_p_max_pu, lifetime=costs.at[\"solar\", \"lifetime\"], ) add_conventional_generators(network, nodes, config, prov_centroids, costs) # nuclear is brownfield if \"nuclear\" in config[\"Techs\"][\"vre_techs\"]: nuclear_p_nom = pd.read_csv(config[\"nuclear_reactors\"][\"pp_path\"], index_col=0) nuclear_p_nom = pd.Series(nuclear_p_nom.squeeze()) nuclear_nodes = pd.Index(NUCLEAR_EXTENDABLE) network.add( \"Generator\", nuclear_nodes, suffix=\" nuclear\", p_nom_extendable=True, p_min_pu=0.7, bus=nuclear_nodes, carrier=\"nuclear\", efficiency=costs.at[\"nuclear\", \"efficiency\"], capital_cost=costs.at[\"nuclear\", \"capital_cost\"], # NB: capital cost is per MWel marginal_cost=costs.at[\"nuclear\", \"marginal_cost\"], lifetime=costs.at[\"nuclear\", \"lifetime\"], ) # TODO add coal CC? no retrofit option if \"PHS\" in config[\"Techs\"][\"store_techs\"]: # pure pumped hydro storage, fixed, 6h energy by default, no inflow hydrocapa_df = pd.read_csv(\"resources/data/hydro/PHS_p_nom.csv\", index_col=0) phss = hydrocapa_df.index[hydrocapa_df[\"MW\"] > 0].intersection(nodes) if config[\"hydro\"][\"hydro_capital_cost\"]: cc = costs.at[\"PHS\", \"capital_cost\"] else: cc = 0.0 network.add( \"StorageUnit\", phss, suffix=\" PHS\", bus=phss, carrier=\"PHS\", p_nom_extendable=False, p_nom=hydrocapa_df.loc[phss][\"MW\"], p_nom_min=hydrocapa_df.loc[phss][\"MW\"], max_hours=config[\"hydro\"][\"PHS_max_hours\"], efficiency_store=np.sqrt(costs.at[\"PHS\", \"efficiency\"]), efficiency_dispatch=np.sqrt(costs.at[\"PHS\", \"efficiency\"]), cyclic_state_of_charge=True, capital_cost=cc, marginal_cost=0.0, ) if config[\"add_hydro\"]: add_hydro(network, config, nodes, prov_centroids, costs, planning_horizons) if config[\"add_H2\"]: # do beore heat coupling to avoid warning network.add( \"Bus\", nodes, suffix=\" H2\", x=prov_centroids.x, y=prov_centroids.y, carrier=\"H2\", location=nodes, ) if config[\"heat_coupling\"]: add_heat_coupling(network, config, nodes, prov_centroids, costs, planning_horizons) if config[\"add_H2\"]: add_H2(network, config, nodes, costs) if \"battery\" in config[\"Techs\"][\"store_techs\"]: network.add( \"Bus\", nodes, suffix=\" battery\", x=prov_centroids.x, y=prov_centroids.y, carrier=\"battery\", location=nodes, ) # TODO Why no standing loss? network.add( \"Store\", nodes + \" battery\", bus=nodes + \" battery\", e_cyclic=True, e_nom_extendable=True, capital_cost=costs.at[\"battery storage\", \"capital_cost\"], lifetime=costs.at[\"battery storage\", \"lifetime\"], ) # TODO understand/remove sources, data should not be in code # Sources: # [HP]: Henning, Palzer http://www.sciencedirect.com/science/article/pii/S1364032113006710 # [B]: Budischak et al. http://www.sciencedirect.com/science/article/pii/S0378775312014759 network.add( \"Link\", nodes + \" battery charger\", bus0=nodes, bus1=nodes + \" battery\", efficiency=costs.at[\"battery inverter\", \"efficiency\"] ** 0.5, capital_cost=costs.at[\"battery inverter\", \"efficiency\"] * costs.at[\"battery inverter\", \"capital_cost\"], p_nom_extendable=True, carrier=\"battery\", lifetime=costs.at[\"battery inverter\", \"lifetime\"], ) network.add( \"Link\", nodes + \" battery discharger\", bus0=nodes + \" battery\", bus1=nodes, efficiency=costs.at[\"battery inverter\", \"efficiency\"] ** 0.5, marginal_cost=0.0, p_nom_extendable=True, carrier=\"battery discharger\", ) # ============= add lines ========= # The lines are implemented according to the transport model (no KVL) and without losses. # see Neumann et al 10.1016/j.apenergy.2022.118859 # TODO make not lossless optional (? - increases computing cost) if not config[\"no_lines\"]: add_voltage_links(network, config) assign_locations(network) return network","title":"prepare_network"},{"location":"docs/reference/prepare_network_common/","text":"add_HV_links(network, config, n_years) add high voltage connections as links in the lossy transport model (see Neumann et al) Parameters: network ( Network ) \u2013 description config ( dict ) \u2013 description n_years ( int ) \u2013 description Raises: ValueError \u2013 description Source code in workflow/scripts/prepare_network_common.py def add_HV_links(network: pypsa.Network, config: dict, n_years: int): \"\"\"add high voltage connections as links in the lossy transport model (see Neumann et al) Args: network (pypsa.Network): _description_ config (dict): _description_ n_years (int): _description_ Raises: ValueError: _description_ \"\"\" edge_path = config[\"edge_paths\"].get(config[\"scenario\"][\"topology\"], None) if edge_path is None: raise ValueError(f\"No grid found for topology {config['scenario']['topology']}\") else: edges = pd.read_csv( edge_path, sep=\",\", header=None, names=[\"bus0\", \"bus1\", \"p_nom\"] ).fillna(0) # fix this to use map with x.y lengths = NON_LIN_PATH_SCALING * np.array( [ haversine( [network.buses.at[bus0, \"x\"], network.buses.at[bus0, \"y\"]], [network.buses.at[bus1, \"x\"], network.buses.at[bus1, \"y\"]], ) for bus0, bus1 in edges[[\"bus0\", \"bus1\"]].values ] ) cc = ( (config[\"line_cost_factor\"] * lengths * [HVAC_cost_curve(len_) for len_ in lengths]) * LINE_SECURITY_MARGIN * FOM_LINES * n_years * annuity(ECON_LIFETIME_LINES, config[\"costs\"][\"discountrate\"]) ) network.add( \"Link\", edges[\"bus0\"] + \"-\" + edges[\"bus1\"], p_nom=edges[\"p_nom\"].values, p_nom_min=edges[\"p_nom\"].values, bus0=edges[\"bus0\"].values, bus1=edges[\"bus1\"].values, p_nom_extendable=True, p_min_pu=-1, length=lengths, capital_cost=cc, ) calc_renewable_pu_avail(renewable_ds, planning_year, snapshots) calaculate the renewable per unit availability Parameters: renewable_ds ( Dataset ) \u2013 the renewable dataset from build_renewable_potential planning_year ( int ) \u2013 the investment year snapshots ( Index ) \u2013 the network snapshots Source code in workflow/scripts/prepare_network_common.py def calc_renewable_pu_avail( renewable_ds: xr.Dataset, planning_year: int, snapshots: pd.Index ) -> pd.DataFrame: \"\"\"calaculate the renewable per unit availability Args: renewable_ds (xr.Dataset): the renewable dataset from build_renewable_potential planning_year (int): the investment year snapshots (pd.Index): the network snapshots \"\"\" rnwable_p_max_pu = renewable_ds[\"profile\"].transpose(\"time\", \"bus\").to_pandas() rnwable_p_max_pu = shift_profile_to_planning_year(rnwable_p_max_pu, planning_year) if not (snapshots.isin(rnwable_p_max_pu.index)).all(): err = \"Snapshots do not match renewable data profile data:\" err += f\"\\n\\tmissing {snapshots.difference(rnwable_p_max_pu.index)}.\\n\" tip = \"You may may need to regenerate your cutout or adapt the snapshots\" raise ValueError(err + tip) rnwable_p_max_pu = rnwable_p_max_pu.loc[snapshots] return rnwable_p_max_pu.sort_index(axis=1)","title":"prepare_network_common"},{"location":"docs/reference/prepare_network_common/#prepare_network_common.add_HV_links","text":"add high voltage connections as links in the lossy transport model (see Neumann et al) Parameters: network ( Network ) \u2013 description config ( dict ) \u2013 description n_years ( int ) \u2013 description Raises: ValueError \u2013 description Source code in workflow/scripts/prepare_network_common.py def add_HV_links(network: pypsa.Network, config: dict, n_years: int): \"\"\"add high voltage connections as links in the lossy transport model (see Neumann et al) Args: network (pypsa.Network): _description_ config (dict): _description_ n_years (int): _description_ Raises: ValueError: _description_ \"\"\" edge_path = config[\"edge_paths\"].get(config[\"scenario\"][\"topology\"], None) if edge_path is None: raise ValueError(f\"No grid found for topology {config['scenario']['topology']}\") else: edges = pd.read_csv( edge_path, sep=\",\", header=None, names=[\"bus0\", \"bus1\", \"p_nom\"] ).fillna(0) # fix this to use map with x.y lengths = NON_LIN_PATH_SCALING * np.array( [ haversine( [network.buses.at[bus0, \"x\"], network.buses.at[bus0, \"y\"]], [network.buses.at[bus1, \"x\"], network.buses.at[bus1, \"y\"]], ) for bus0, bus1 in edges[[\"bus0\", \"bus1\"]].values ] ) cc = ( (config[\"line_cost_factor\"] * lengths * [HVAC_cost_curve(len_) for len_ in lengths]) * LINE_SECURITY_MARGIN * FOM_LINES * n_years * annuity(ECON_LIFETIME_LINES, config[\"costs\"][\"discountrate\"]) ) network.add( \"Link\", edges[\"bus0\"] + \"-\" + edges[\"bus1\"], p_nom=edges[\"p_nom\"].values, p_nom_min=edges[\"p_nom\"].values, bus0=edges[\"bus0\"].values, bus1=edges[\"bus1\"].values, p_nom_extendable=True, p_min_pu=-1, length=lengths, capital_cost=cc, )","title":"add_HV_links"},{"location":"docs/reference/prepare_network_common/#prepare_network_common.calc_renewable_pu_avail","text":"calaculate the renewable per unit availability Parameters: renewable_ds ( Dataset ) \u2013 the renewable dataset from build_renewable_potential planning_year ( int ) \u2013 the investment year snapshots ( Index ) \u2013 the network snapshots Source code in workflow/scripts/prepare_network_common.py def calc_renewable_pu_avail( renewable_ds: xr.Dataset, planning_year: int, snapshots: pd.Index ) -> pd.DataFrame: \"\"\"calaculate the renewable per unit availability Args: renewable_ds (xr.Dataset): the renewable dataset from build_renewable_potential planning_year (int): the investment year snapshots (pd.Index): the network snapshots \"\"\" rnwable_p_max_pu = renewable_ds[\"profile\"].transpose(\"time\", \"bus\").to_pandas() rnwable_p_max_pu = shift_profile_to_planning_year(rnwable_p_max_pu, planning_year) if not (snapshots.isin(rnwable_p_max_pu.index)).all(): err = \"Snapshots do not match renewable data profile data:\" err += f\"\\n\\tmissing {snapshots.difference(rnwable_p_max_pu.index)}.\\n\" tip = \"You may may need to regenerate your cutout or adapt the snapshots\" raise ValueError(err + tip) rnwable_p_max_pu = rnwable_p_max_pu.loc[snapshots] return rnwable_p_max_pu.sort_index(axis=1)","title":"calc_renewable_pu_avail"},{"location":"docs/reference/readers/","text":"File reading support functions read_pop_density(path, clip_shape=None, crs=CRS, chunks=25, var_name='pop_density') read raster data, clip it to a clip_shape and convert it to a GeoDataFrame Parameters: path ( PathLike ) \u2013 the target path for the raster data (tif) clip_shape ( GeoSeries , default: None ) \u2013 the shape to clip the data. Defaults to None. crs ( int , default: CRS ) \u2013 the coordinate system. Defaults to 4326. var_name ( str , default: 'pop_density' ) \u2013 the variable name. Defaults to \"var\". chunks ( int , default: 25 ) \u2013 the chunk size for the raster data. Defaults to 25. Returns: GeoDataFrame \u2013 gpd.GeoDataFrame: the raster data for the aoi Source code in workflow/scripts/readers.py def read_pop_density( path: os.PathLike, clip_shape: gpd.GeoSeries = None, crs=CRS, chunks=25, var_name=\"pop_density\", ) -> gpd.GeoDataFrame: \"\"\"read raster data, clip it to a clip_shape and convert it to a GeoDataFrame Args: path (os.PathLike): the target path for the raster data (tif) clip_shape (gpd.GeoSeries, optional): the shape to clip the data. Defaults to None. crs (int, optional): the coordinate system. Defaults to 4326. var_name (str, optional): the variable name. Defaults to \"var\". chunks (int, optional): the chunk size for the raster data. Defaults to 25. Returns: gpd.GeoDataFrame: the raster data for the aoi \"\"\" ds = read_raster(path, clip_shape, var_name, plot=False) ds = ds.where(ds > 0) df = ds.to_dataframe(var_name) df.reset_index(inplace=True) # Convert the DataFrame to a GeoDataFrame return gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.x, df.y), crs=crs) read_province_shapes(shape_file) read the province shape files Parameters: shape_file ( PathLike ) \u2013 the path to the .shp file & co Returns: GeoDataFrame \u2013 gpd.GeoDataFrame: the province shapes as a GeoDataFrame Source code in workflow/scripts/readers.py def read_province_shapes(shape_file: os.PathLike) -> gpd.GeoDataFrame: \"\"\"read the province shape files Args: shape_file (os.PathLike): the path to the .shp file & co Returns: gpd.GeoDataFrame: the province shapes as a GeoDataFrame \"\"\" prov_shapes = gpd.GeoDataFrame.from_file(shape_file) prov_shapes = prov_shapes.to_crs(CRS) prov_shapes.set_index(\"province\", inplace=True) # TODO: does this make sense? reindex after? if not (prov_shapes.sort_index().index == sorted(PROV_NAMES)).all(): missing = f\"Missing provinces: {set(PROV_NAMES) - set(prov_shapes.index)}\" raise ValueError(f\"Province names do not match expected names: missing {missing}\") return prov_shapes read_raster(path, clip_shape=None, var_name='var', chunks=60, plot=False) Read raster data and optionally clip it to a given shape. Parameters: path ( PathLike ) \u2013 The path to the raster file. clip_shape ( GeoSeries , default: None ) \u2013 The shape to clip the raster data. Defaults to None. var_name ( str , default: 'var' ) \u2013 The variable name to assign to the raster data. Defaults to \"var\". chunks ( int , default: 60 ) \u2013 The chunk size for the raster data. Defaults to 60. plot ( bool , default: False ) \u2013 Whether to plot the raster data. Defaults to False. Returns: DataArray ( DataArray ) \u2013 The raster data as an xarray DataArray. Source code in workflow/scripts/readers.py def read_raster( path: os.PathLike, clip_shape: gpd.GeoSeries = None, var_name=\"var\", chunks=60, plot=False, ) -> DataArray: \"\"\"Read raster data and optionally clip it to a given shape. Args: path (os.PathLike): The path to the raster file. clip_shape (gpd.GeoSeries, optional): The shape to clip the raster data. Defaults to None. var_name (str, optional): The variable name to assign to the raster data. Defaults to \"var\". chunks (int, optional): The chunk size for the raster data. Defaults to 60. plot (bool, optional): Whether to plot the raster data. Defaults to False. Returns: DataArray: The raster data as an xarray DataArray. \"\"\" ds = rioxarray.open_rasterio(path, chunks=chunks, default_name=\"pop_density\") ds = ds.rename(var_name) if clip_shape is not None: ds = ds.rio.clip(clip_shape.geometry) if plot: ds.plot() return ds","title":"readers"},{"location":"docs/reference/readers/#readers.read_pop_density","text":"read raster data, clip it to a clip_shape and convert it to a GeoDataFrame Parameters: path ( PathLike ) \u2013 the target path for the raster data (tif) clip_shape ( GeoSeries , default: None ) \u2013 the shape to clip the data. Defaults to None. crs ( int , default: CRS ) \u2013 the coordinate system. Defaults to 4326. var_name ( str , default: 'pop_density' ) \u2013 the variable name. Defaults to \"var\". chunks ( int , default: 25 ) \u2013 the chunk size for the raster data. Defaults to 25. Returns: GeoDataFrame \u2013 gpd.GeoDataFrame: the raster data for the aoi Source code in workflow/scripts/readers.py def read_pop_density( path: os.PathLike, clip_shape: gpd.GeoSeries = None, crs=CRS, chunks=25, var_name=\"pop_density\", ) -> gpd.GeoDataFrame: \"\"\"read raster data, clip it to a clip_shape and convert it to a GeoDataFrame Args: path (os.PathLike): the target path for the raster data (tif) clip_shape (gpd.GeoSeries, optional): the shape to clip the data. Defaults to None. crs (int, optional): the coordinate system. Defaults to 4326. var_name (str, optional): the variable name. Defaults to \"var\". chunks (int, optional): the chunk size for the raster data. Defaults to 25. Returns: gpd.GeoDataFrame: the raster data for the aoi \"\"\" ds = read_raster(path, clip_shape, var_name, plot=False) ds = ds.where(ds > 0) df = ds.to_dataframe(var_name) df.reset_index(inplace=True) # Convert the DataFrame to a GeoDataFrame return gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.x, df.y), crs=crs)","title":"read_pop_density"},{"location":"docs/reference/readers/#readers.read_province_shapes","text":"read the province shape files Parameters: shape_file ( PathLike ) \u2013 the path to the .shp file & co Returns: GeoDataFrame \u2013 gpd.GeoDataFrame: the province shapes as a GeoDataFrame Source code in workflow/scripts/readers.py def read_province_shapes(shape_file: os.PathLike) -> gpd.GeoDataFrame: \"\"\"read the province shape files Args: shape_file (os.PathLike): the path to the .shp file & co Returns: gpd.GeoDataFrame: the province shapes as a GeoDataFrame \"\"\" prov_shapes = gpd.GeoDataFrame.from_file(shape_file) prov_shapes = prov_shapes.to_crs(CRS) prov_shapes.set_index(\"province\", inplace=True) # TODO: does this make sense? reindex after? if not (prov_shapes.sort_index().index == sorted(PROV_NAMES)).all(): missing = f\"Missing provinces: {set(PROV_NAMES) - set(prov_shapes.index)}\" raise ValueError(f\"Province names do not match expected names: missing {missing}\") return prov_shapes","title":"read_province_shapes"},{"location":"docs/reference/readers/#readers.read_raster","text":"Read raster data and optionally clip it to a given shape. Parameters: path ( PathLike ) \u2013 The path to the raster file. clip_shape ( GeoSeries , default: None ) \u2013 The shape to clip the raster data. Defaults to None. var_name ( str , default: 'var' ) \u2013 The variable name to assign to the raster data. Defaults to \"var\". chunks ( int , default: 60 ) \u2013 The chunk size for the raster data. Defaults to 60. plot ( bool , default: False ) \u2013 Whether to plot the raster data. Defaults to False. Returns: DataArray ( DataArray ) \u2013 The raster data as an xarray DataArray. Source code in workflow/scripts/readers.py def read_raster( path: os.PathLike, clip_shape: gpd.GeoSeries = None, var_name=\"var\", chunks=60, plot=False, ) -> DataArray: \"\"\"Read raster data and optionally clip it to a given shape. Args: path (os.PathLike): The path to the raster file. clip_shape (gpd.GeoSeries, optional): The shape to clip the raster data. Defaults to None. var_name (str, optional): The variable name to assign to the raster data. Defaults to \"var\". chunks (int, optional): The chunk size for the raster data. Defaults to 60. plot (bool, optional): Whether to plot the raster data. Defaults to False. Returns: DataArray: The raster data as an xarray DataArray. \"\"\" ds = rioxarray.open_rasterio(path, chunks=chunks, default_name=\"pop_density\") ds = ds.rename(var_name) if clip_shape is not None: ds = ds.rio.clip(clip_shape.geometry) if plot: ds.plot() return ds","title":"read_raster"},{"location":"docs/reference/solve_network/","text":"Functions to add constraints and prepare the network for the solver. Associated with the solve_networks rule in the Snakefile. add_battery_constraints(n) Add constraint ensuring that charger = discharger, i.e. 1 * charger_size - efficiency * discharger_size = 0 Source code in workflow/scripts/solve_network.py def add_battery_constraints(n: pypsa.Network): \"\"\" Add constraint ensuring that charger = discharger, i.e. 1 * charger_size - efficiency * discharger_size = 0 \"\"\" if not n.links.p_nom_extendable.any(): return discharger_bool = n.links.index.str.contains(\"battery discharger\") charger_bool = n.links.index.str.contains(\"battery charger\") dischargers_ext = n.links[discharger_bool].query(\"p_nom_extendable\").index chargers_ext = n.links[charger_bool].query(\"p_nom_extendable\").index eff = n.links.efficiency[dischargers_ext].values lhs = n.model[\"Link-p_nom\"].loc[chargers_ext] - n.model[\"Link-p_nom\"].loc[dischargers_ext] * eff n.model.add_constraints(lhs == 0, name=\"Link-charger_ratio\") add_transimission_constraints(n) Add constraint ensuring that transmission lines p_nom are the same for both directions, i.e. p_nom positive = p_nom negative Parameters: n ( Network ) \u2013 the network object to optimize Source code in workflow/scripts/solve_network.py def add_transimission_constraints(n: pypsa.Network): \"\"\" Add constraint ensuring that transmission lines p_nom are the same for both directions, i.e. p_nom positive = p_nom negative Args: n (pypsa.Network): the network object to optimize \"\"\" if not n.links.p_nom_extendable.any(): return positive_bool = n.links.index.str.contains(\"positive\") negative_bool = n.links.index.str.contains(\"reversed\") positive_ext = n.links[positive_bool].query(\"p_nom_extendable\").index negative_ext = n.links[negative_bool].query(\"p_nom_extendable\").index lhs = n.model[\"Link-p_nom\"].loc[positive_ext] rhs = n.model[\"Link-p_nom\"].loc[negative_ext] n.model.add_constraints(lhs == rhs, name=\"Link-transimission\") extra_functionality(n, snapshots) Collects supplementary constraints which will be passed to pypsa.linopf.network_lopf . If you want to enforce additional custom constraints, this is a good location to add them. The arguments opts and snakemake.config are expected to be attached to the network. Source code in workflow/scripts/solve_network.py def extra_functionality(n: pypsa.Network, snapshots: DatetimeIndex): \"\"\" Collects supplementary constraints which will be passed to ``pypsa.linopf.network_lopf``. If you want to enforce additional custom constraints, this is a good location to add them. The arguments ``opts`` and ``snakemake.config`` are expected to be attached to the network. \"\"\" add_battery_constraints(n) add_transimission_constraints(n) add_chp_constraints(n)","title":"solve_network"},{"location":"docs/reference/solve_network/#solve_network.add_battery_constraints","text":"Add constraint ensuring that charger = discharger, i.e. 1 * charger_size - efficiency * discharger_size = 0 Source code in workflow/scripts/solve_network.py def add_battery_constraints(n: pypsa.Network): \"\"\" Add constraint ensuring that charger = discharger, i.e. 1 * charger_size - efficiency * discharger_size = 0 \"\"\" if not n.links.p_nom_extendable.any(): return discharger_bool = n.links.index.str.contains(\"battery discharger\") charger_bool = n.links.index.str.contains(\"battery charger\") dischargers_ext = n.links[discharger_bool].query(\"p_nom_extendable\").index chargers_ext = n.links[charger_bool].query(\"p_nom_extendable\").index eff = n.links.efficiency[dischargers_ext].values lhs = n.model[\"Link-p_nom\"].loc[chargers_ext] - n.model[\"Link-p_nom\"].loc[dischargers_ext] * eff n.model.add_constraints(lhs == 0, name=\"Link-charger_ratio\")","title":"add_battery_constraints"},{"location":"docs/reference/solve_network/#solve_network.add_transimission_constraints","text":"Add constraint ensuring that transmission lines p_nom are the same for both directions, i.e. p_nom positive = p_nom negative Parameters: n ( Network ) \u2013 the network object to optimize Source code in workflow/scripts/solve_network.py def add_transimission_constraints(n: pypsa.Network): \"\"\" Add constraint ensuring that transmission lines p_nom are the same for both directions, i.e. p_nom positive = p_nom negative Args: n (pypsa.Network): the network object to optimize \"\"\" if not n.links.p_nom_extendable.any(): return positive_bool = n.links.index.str.contains(\"positive\") negative_bool = n.links.index.str.contains(\"reversed\") positive_ext = n.links[positive_bool].query(\"p_nom_extendable\").index negative_ext = n.links[negative_bool].query(\"p_nom_extendable\").index lhs = n.model[\"Link-p_nom\"].loc[positive_ext] rhs = n.model[\"Link-p_nom\"].loc[negative_ext] n.model.add_constraints(lhs == rhs, name=\"Link-transimission\")","title":"add_transimission_constraints"},{"location":"docs/reference/solve_network/#solve_network.extra_functionality","text":"Collects supplementary constraints which will be passed to pypsa.linopf.network_lopf . If you want to enforce additional custom constraints, this is a good location to add them. The arguments opts and snakemake.config are expected to be attached to the network. Source code in workflow/scripts/solve_network.py def extra_functionality(n: pypsa.Network, snapshots: DatetimeIndex): \"\"\" Collects supplementary constraints which will be passed to ``pypsa.linopf.network_lopf``. If you want to enforce additional custom constraints, this is a good location to add them. The arguments ``opts`` and ``snakemake.config`` are expected to be attached to the network. \"\"\" add_battery_constraints(n) add_transimission_constraints(n) add_chp_constraints(n)","title":"extra_functionality"},{"location":"docs/reference/solve_network_myopic/","text":"Functions to add constraints and prepare the network for the solver. Associated with the solve_network_myopic rule in the Snakefile. To be merged/consolidated with the solve_network script. add_battery_constraints(n) Add constraint ensuring that charger = discharger, i.e. 1 * charger_size - efficiency * discharger_size = 0 Source code in workflow/scripts/solve_network_myopic.py def add_battery_constraints(n): \"\"\" Add constraint ensuring that charger = discharger, i.e. 1 * charger_size - efficiency * discharger_size = 0 \"\"\" if not n.links.p_nom_extendable.any(): return discharger_bool = n.links.index.str.contains(\"battery discharger\") charger_bool = n.links.index.str.contains(\"battery charger\") dischargers_ext = n.links[discharger_bool].query(\"p_nom_extendable\").index chargers_ext = n.links[charger_bool].query(\"p_nom_extendable\").index eff = n.links.efficiency[dischargers_ext].values lhs = n.model[\"Link-p_nom\"].loc[chargers_ext] - n.model[\"Link-p_nom\"].loc[dischargers_ext] * eff n.model.add_constraints(lhs == 0, name=\"Link-charger_ratio\") add_transimission_constraints(n) Add constraint ensuring that transmission lines p_nom are the same for both directions, i.e. p_nom positive = p_nom negative Source code in workflow/scripts/solve_network_myopic.py def add_transimission_constraints(n): \"\"\" Add constraint ensuring that transmission lines p_nom are the same for both directions, i.e. p_nom positive = p_nom negative \"\"\" if not n.links.p_nom_extendable.any(): return positive_bool = n.links.index.str.contains(\"positive\") negative_bool = n.links.index.str.contains(\"reversed\") positive_ext = n.links[positive_bool].query(\"p_nom_extendable\").index negative_ext = n.links[negative_bool].query(\"p_nom_extendable\").index lhs = n.model[\"Link-p_nom\"].loc[positive_ext] - n.model[\"Link-p_nom\"].loc[negative_ext] n.model.add_constraints(lhs == 0, name=\"Link-transimission\") extra_functionality(n, snapshots) Collects supplementary constraints which will be passed to pypsa.linopf.network_lopf . If you want to enforce additional custom constraints, this is a good location to add them. The arguments opts and snakemake.config are expected to be attached to the network. Source code in workflow/scripts/solve_network_myopic.py def extra_functionality(n, snapshots): \"\"\" Collects supplementary constraints which will be passed to ``pypsa.linopf.network_lopf``. If you want to enforce additional custom constraints, this is a good location to add them. The arguments ``opts`` and ``snakemake.config`` are expected to be attached to the network. \"\"\" opts = n.opts config = n.config add_chp_constraints(n) add_battery_constraints(n) add_transimission_constraints(n) if snakemake.wildcards.planning_horizons != \"2020\": add_retrofit_constraints(n)","title":"solve_network_myopic"},{"location":"docs/reference/solve_network_myopic/#solve_network_myopic.add_battery_constraints","text":"Add constraint ensuring that charger = discharger, i.e. 1 * charger_size - efficiency * discharger_size = 0 Source code in workflow/scripts/solve_network_myopic.py def add_battery_constraints(n): \"\"\" Add constraint ensuring that charger = discharger, i.e. 1 * charger_size - efficiency * discharger_size = 0 \"\"\" if not n.links.p_nom_extendable.any(): return discharger_bool = n.links.index.str.contains(\"battery discharger\") charger_bool = n.links.index.str.contains(\"battery charger\") dischargers_ext = n.links[discharger_bool].query(\"p_nom_extendable\").index chargers_ext = n.links[charger_bool].query(\"p_nom_extendable\").index eff = n.links.efficiency[dischargers_ext].values lhs = n.model[\"Link-p_nom\"].loc[chargers_ext] - n.model[\"Link-p_nom\"].loc[dischargers_ext] * eff n.model.add_constraints(lhs == 0, name=\"Link-charger_ratio\")","title":"add_battery_constraints"},{"location":"docs/reference/solve_network_myopic/#solve_network_myopic.add_transimission_constraints","text":"Add constraint ensuring that transmission lines p_nom are the same for both directions, i.e. p_nom positive = p_nom negative Source code in workflow/scripts/solve_network_myopic.py def add_transimission_constraints(n): \"\"\" Add constraint ensuring that transmission lines p_nom are the same for both directions, i.e. p_nom positive = p_nom negative \"\"\" if not n.links.p_nom_extendable.any(): return positive_bool = n.links.index.str.contains(\"positive\") negative_bool = n.links.index.str.contains(\"reversed\") positive_ext = n.links[positive_bool].query(\"p_nom_extendable\").index negative_ext = n.links[negative_bool].query(\"p_nom_extendable\").index lhs = n.model[\"Link-p_nom\"].loc[positive_ext] - n.model[\"Link-p_nom\"].loc[negative_ext] n.model.add_constraints(lhs == 0, name=\"Link-transimission\")","title":"add_transimission_constraints"},{"location":"docs/reference/solve_network_myopic/#solve_network_myopic.extra_functionality","text":"Collects supplementary constraints which will be passed to pypsa.linopf.network_lopf . If you want to enforce additional custom constraints, this is a good location to add them. The arguments opts and snakemake.config are expected to be attached to the network. Source code in workflow/scripts/solve_network_myopic.py def extra_functionality(n, snapshots): \"\"\" Collects supplementary constraints which will be passed to ``pypsa.linopf.network_lopf``. If you want to enforce additional custom constraints, this is a good location to add them. The arguments ``opts`` and ``snakemake.config`` are expected to be attached to the network. \"\"\" opts = n.opts config = n.config add_chp_constraints(n) add_battery_constraints(n) add_transimission_constraints(n) if snakemake.wildcards.planning_horizons != \"2020\": add_retrofit_constraints(n)","title":"extra_functionality"}]}